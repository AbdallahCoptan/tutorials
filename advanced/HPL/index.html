<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <title>High Performance Linpack (HPL) - UL HPC Tutorials</title>
  

  <link rel="shortcut icon" href="../../img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">

  
  <script>
    // Current page data
    var mkdocs_page_name = "High Performance Linpack (HPL)";
    var mkdocs_page_input_path = "advanced/HPL/index.md";
    var mkdocs_page_url = "/advanced/HPL/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script>
  <script src="../../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> UL HPC Tutorials</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">Home</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Basic Tutorials</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../basic/getting_started/">Getting Started</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../basic/sequential_jobs/">HPC workflow with sequential jobs</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Debug/">Efficient Debugging</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Advanced Software Management</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../EasyBuild/">Easybuild</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../RESIF/">RESIF</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>MPI / Performance Evaluation</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../OSU_MicroBenchmarks/">OSU Micro-benchmarks</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">High Performance Linpack (HPL)</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#ul-hpc-mpi-tutorial-high-performance-linpack-hpl-benchmarking-on-ul-hpc-platform">UL HPC MPI Tutorial: High-Performance Linpack (HPL) benchmarking on UL HPC platform</a></li>
                
                    <li><a class="toctree-l4" href="#objectives">Objectives</a></li>
                
                    <li><a class="toctree-l4" href="#pre-requisites">Pre-requisites</a></li>
                
                    <li><a class="toctree-l4" href="#building-the-hpl-benchmark">Building the HPL benchmark</a></li>
                
            
            </ul>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../HPCG/">High Performance Conjugate Gradient (HPCG)</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Mathematics</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../MATLAB1/README/">MATLAB</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../R/">R - statistical computing</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Bioinformatics</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Bioinformatics/">Running bioinformatics software</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Galaxy/">Galaxy</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Parallel Debuggers</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Allinea/">Allinea</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../TotalView/">TotalView</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Virtualization</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Vagrant/">Vagrant</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../vm5k/">Grid5000 - Automatic VM deployment with VM5K</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>CFD/MD/Chemistry</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../MultiPhysics/">Advanced Parallel execution</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Big Data</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Spark/">Big Data Application Over Apache Spark</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Advanced topics</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../advanced_workflows/README/">Advanced workflows on sequential jobs management</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../advanced_scheduling/">Advanced scheduling with SLURM</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Python/">Prototyping with Python</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../ReproducibleResearch/">Reproducible Research</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../rtfd/">Documentation (RTFD)</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../contributing/">Contributing</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../contacts/">Contacts</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">UL HPC Tutorials</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>MPI / Performance Evaluation &raquo;</li>
        
      
    
    <li>High Performance Linpack (HPL)</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>-<em>- mode: markdown; mode: visual-line; fill-column: 80 -</em>-</p>
<p>Copyright (c) 2013-2017 UL HPC Team  <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#104;&#112;&#99;&#45;&#115;&#121;&#115;&#97;&#100;&#109;&#105;&#110;&#115;&#64;&#117;&#110;&#105;&#46;&#108;&#117;">&#104;&#112;&#99;&#45;&#115;&#121;&#115;&#97;&#100;&#109;&#105;&#110;&#115;&#64;&#117;&#110;&#105;&#46;&#108;&#117;</a></p>
<hr />
<h1 id="ul-hpc-mpi-tutorial-high-performance-linpack-hpl-benchmarking-on-ul-hpc-platform">UL HPC MPI Tutorial: High-Performance Linpack (HPL) benchmarking on UL HPC platform</h1>
<p><a href="https://hpc.uni.lu"><img alt="By ULHPC" src="https://img.shields.io/badge/by-ULHPC-blue.svg" /></a> <a href="http://www.gnu.org/licenses/gpl-3.0.html"><img alt="Licence" src="https://img.shields.io/badge/license-GPL--3.0-blue.svg" /></a> <a href="https://github.com/ULHPC/tutorials/issues/"><img alt="GitHub issues" src="https://img.shields.io/github/issues/ULHPC/tutorials.svg" /></a> <a href="https://github.com/ULHPC/tutorials/raw/devel/advanced/OSU_MicroBenchmarks/slides.pdf"><img alt="" src="https://img.shields.io/badge/slides-PDF-red.svg" /></a> <a href="https://github.com/ULHPC/tutorials/tree/devel/advanced/HPL/"><img alt="Github" src="https://img.shields.io/badge/sources-github-green.svg" /></a> <a href="http://ulhpc-tutorials.readthedocs.io/en/latest/advanced/HPL/"><img alt="Documentation Status" src="http://readthedocs.org/projects/ulhpc-tutorials/badge/?version=latest" /></a> <a href="https://github.com/ULHPC/tutorials"><img alt="GitHub forks" src="https://img.shields.io/github/stars/ULHPC/tutorials.svg?style=social&amp;label=Star" /></a></p>
<p><a href="https://github.com/ULHPC/tutorials/raw/devel/advanced/OSU_MicroBenchmarks/slides.pdf"><img alt="" src="https://github.com/ULHPC/tutorials/raw/devel/advanced/OSU_MicroBenchmarks/cover_slides.png" /></a></p>
<p>The objective of this tutorial is to compile and run on of the reference HPC benchmarks, <a href="http://www.netlib.org/benchmark/hpl/">HPL</a>, on top of the <a href="http://hpc.uni.lu">UL HPC</a> platform.</p>
<p>You can work in groups for this training, yet individual work is encouraged to ensure you understand and practice the usage of MPI programs on an HPC platform.
If not yet done, you should consider completing the <a href="./../OSU_MicroBenchmarks/">OSU Micro-benchmark</a> tutorial as it introduces the effective usage of the different MPI suits available on the UL HPC platform.</p>
<p>In all cases, ensure you are able to <a href="https://hpc.uni.lu/users/docs/access.html">connect to the UL HPC  clusters</a>.</p>
<pre><code class="bash"># /!\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A (at least half) COMPUTING NODE
# Have an interactive job
(access)$&gt; si -n 14                                      # iris
(access)$&gt; srun -p interactive --qos qos-iteractive -n 14 --pty bash  # iris (long version)
(access)$&gt; oarsub -I -l enclosure=1/nodes=1,walltime=4   # chaos / gaia
</code></pre>

<p><strong>Advanced users only</strong>: rely on <code>screen</code> (see  <a href="http://support.suso.com/supki/Screen_tutorial">tutorial</a> or the <a href="https://hpc.uni.lu/users/docs/ScreenSessions.html">UL HPC tutorial</a> on the  frontend prior to running any <code>oarsub</code> or <code>srun/sbatch</code> command to be more resilient to disconnection.</p>
<p>The latest version of this tutorial is available on <a href="https://github.com/ULHPC/tutorials/tree/devel/advanced/HPL">Github</a>
Finally, advanced MPI users might be interested to take a look at the <a href="https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor">Intel Math Kernel Library Link Line Advisor</a>.</p>
<h2 id="objectives">Objectives</h2>
<p><a href="http://www.netlib.org/benchmark/hpl/">HPL</a> is a  portable implementation of the High-Performance Linpack (HPL) Benchmark for Distributed-Memory Computers. It is used as reference benchmark to provide data for the <a href="http://top500.org">Top500</a> list and thus rank to supercomputers worldwide.
HPL rely on an efficient implementation of the Basic Linear Algebra Subprograms (BLAS). You have several choices at this level:</p>
<ul>
<li>Intel MKL</li>
<li><a href="http://math-atlas.sourceforge.net/atlas_install/">ATLAS</a></li>
<li><a href="https://www.tacc.utexas.edu/research-development/tacc-software/gotoblas2/">GotoBlas</a></li>
</ul>
<p>The idea is to compare the different MPI and BLAS implementations available on the <a href="http://hpc.uni.lu">UL HPC platform</a>:</p>
<ul>
<li><a href="http://software.intel.com/en-us/intel-mpi-library/">Intel MPI</a> and the Intel MKL</li>
<li><a href="http://www.open-mpi.org/">OpenMPI</a></li>
<li><a href="http://mvapich.cse.ohio-state.edu/overview/">MVAPICH2</a> (MPI-3 over OpenFabrics-IB, Omni-Path, OpenFabrics-iWARP, PSM, and TCP/IP)</li>
<li><a href="http://math-atlas.sourceforge.net/atlas_install/">ATLAS</a></li>
<li><a href="https://www.tacc.utexas.edu/research-development/tacc-software/gotoblas2/">GotoBlas</a></li>
</ul>
<p>For the sake of time and simplicity, we will focus on the combination expected to lead to the best performant runs, <em>i.e.</em> Intel MKL and Intel MPI suite.</p>
<h2 id="pre-requisites">Pre-requisites</h2>
<p>On the <strong>access</strong> and a <strong>computing</strong> node of the cluster you're working on, clone the <a href="https://github.com/ULHPC/tutorials">ULHPC/tutorials</a>  and <a href="https://github.com/ULHPC/launcher-scripts">ULHPC/launcher-scripts</a> repositories</p>
<pre><code class="bash">$&gt; cd
$&gt; mkdir -p git/ULHPC &amp;&amp; cd  git/ULHPC
$&gt; git clone https://github.com/ULHPC/launcher-scripts.git
$&gt; git clone https://github.com/ULHPC/tutorials.git         # If not yet done
</code></pre>

<p>Prepare your working directory</p>
<pre><code class="bash">$&gt; mkdir -p ~/tutorials/HPL
$&gt; cd ~/tutorials/HPL
$&gt; ln -s ~/git/ULHPC/tutorials/advanced/HPL ref.ulhpc.d   # Keep a symlink to the reference tutorial
$&gt; ln -s ref.ulhpc.d/Makefile .     # symlink to the root Makefile
</code></pre>

<p>Fetch and uncompress the latest version of the <a href="http://www.netlib.org/benchmark/hpl/">HPL</a> benchmark (<em>i.e.</em> <strong>version 2.2</strong> at the time of writing).</p>
<pre><code class="bash">$&gt; cd ~/tutorials/HPL
$&gt; mkdir src
$&gt; cd src
# Download the latest version
$&gt; export HPL_VERSION=2.2
$&gt; wget --no-check-certificate http://www.netlib.org/benchmark/hpl/hpl-${HPL_VERSION}.tar.gz
$&gt; tar xvzf hpl-${HPL_VERSION}.tar.gz
$&gt; cd  hpl-${HPL_VERSION}
</code></pre>

<h2 id="building-the-hpl-benchmark">Building the HPL benchmark</h2>
<p>We are first going to use the <a href="http://software.intel.com/en-us/intel-cluster-toolkit-compiler/">Intel Cluster Toolkit Compiler Edition</a>, which provides Intel C/C++ and Fortran compilers, Intel MPI.</p>
<pre><code class="bash">$&gt; cd ~/tutorials/HPL
# Load the appropriate module
$&gt; module spider MPI     # Search for available modules featuring MPI
$&gt; module load toolchain/intel   # On iris -- use 'module load toolchain/ictce' otherwise
$&gt; module list
Currently Loaded Modules:
  1) compiler/GCCcore/6.3.0                   4) compiler/ifort/2017.1.132-GCC-6.3.0-2.27                 7) toolchain/iimpi/2017a
  2) tools/binutils/2.27-GCCcore-6.3.0        5) toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27             8) numlib/imkl/2017.1.132-iimpi-2017a
  3) compiler/icc/2017.1.132-GCC-6.3.0-2.27   6) mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27   9) toolchain/intel/2017a
</code></pre>

<p>You notice that Intel MKL is now loaded.</p>
<p>Read the <code>INSTALL</code> file. In particular, you'll have to edit and adapt a new makefile <code>Make.intel64</code>
(inspired from <code>setup/Make.Linux_PII_CBLAS</code> typically)</p>
<pre><code class="bash">$&gt; cd ~/tutorials/HPL/src/hpl-2.2
$&gt; cp setup/Make.Linux_Intel64 Make.intel64
</code></pre>

<p>Once tweaked, run the compilation by:</p>
<pre><code class="bash">$&gt; make arch=intel64 clean_arch_all
$&gt; make arch=intel64
</code></pre>

<p>But <strong>first</strong>, you will need to configure correctly the file <code>Make.intel64</code>.
Take your favorite editor (<code>vim</code>, <code>nano</code>, etc.) to modify it. In particular, you should adapt:</p>
<ul>
<li><code>TOPdir</code> to point to the directory holding the HPL sources (<em>i.e.</em> where you uncompress them: <code>$(HOME)/tutorials/HPL/src/hpl-2.2</code>)</li>
<li>Adapt the <code>MP*</code> variables to point to the appropriate MPI libraries path.</li>
<li>(eventually) adapt the <code>CCFLAGS</code></li>
</ul>
<p>Here is for instance a suggested difference for intel MPI:</p>
<pre><code class="diff">--- setup/Make.Linux_Intel64    2016-02-24 02:10:50.000000000 +0100
+++ Make.intel64        2017-06-12 13:48:31.016524323 +0200
@@ -61,13 +61,13 @@
 # - Platform identifier ------------------------------------------------
 # ----------------------------------------------------------------------
 #
-ARCH         = Linux_Intel64
+ARCH         = $(arch)
 #
 # ----------------------------------------------------------------------
 # - HPL Directory Structure / HPL library ------------------------------
 # ----------------------------------------------------------------------
 #
-TOPdir       = $(HOME)/hpl
+TOPdir       = $(HOME)/tutorials/HPL/src/hpl-2.2
 INCdir       = $(TOPdir)/include
 BINdir       = $(TOPdir)/bin/$(ARCH)
 LIBdir       = $(TOPdir)/lib/$(ARCH)
@@ -81,9 +81,9 @@
 # header files,  MPlib  is defined  to be the name of  the library to be
 # used. The variable MPdir is only used for defining MPinc and MPlib.
 #
-# MPdir        = /opt/intel/mpi/4.1.0
-# MPinc        = -I$(MPdir)/include64
-# MPlib        = $(MPdir)/lib64/libmpi.a
+MPdir        = $(I_MPI_ROOT)/intel64
+MPinc        = -I$(MPdir)/include
+MPlib        = $(MPdir)/lib/libmpi.a
 #
 # ----------------------------------------------------------------------
 # - Linear Algebra library (BLAS or VSIPL) -----------------------------
@@ -178,7 +178,7 @@
 CC       = mpiicc
 CCNOOPT  = $(HPL_DEFS)
 OMP_DEFS = -openmp
-CCFLAGS  = $(HPL_DEFS) -O3 -w -ansi-alias -i-static -z noexecstack -z relro -z now -nocompchk -Wall
+CCFLAGS  = $(HPL_DEFS) -O3 -w -ansi-alias -i-static -z noexecstack -z relro -z now -nocompchk -Wall -xHost
 #
 # On some platforms,  it is necessary  to use the Fortran linker to find
 # the Fortran internals used in the BLAS library.
</code></pre>

<p>If you don't succeed by yourself, use the following <a href="https://raw.githubusercontent.com/ULHPC/tutorials/devel/advanced/HPL/src/hpl-2.2/Make.intel64">makefile</a>:</p>
<pre><code>$&gt; cd ~/tutorials/HPL
$&gt; cp ref.ulhpc.d/src/hpl-2.2/Make.intel64 src/hpl-2.2/Make.intel64
</code></pre>

<p>Once compiled, ensure you are able to run it:</p>
<pre><code class="bash">$&gt; cd ~/tutorials/HPL/src/hpl-2.2/bin/intel64
$&gt; cat HPL.dat      # Default (dummy) HPL.dat  input file

# On Slurm cluster (iris)
$&gt; srun -n $SLURM_NTASKS ./xhpl

# On OAR clusters (gaia, chaos)
$&gt; mpirun -hostfile $OAR_NODEFILE ./xhpl
</code></pre>

<h3 id="preparing-batch-runs">Preparing batch runs</h3>
<p>We are now going to prepare launcher scripts to permit passive runs (typically in the <code>{default | batch}</code> queue).
We will place them in a separate directory (<code>runs/</code>) as it will host the outcomes of the executions on the UL HPC platform .</p>
<pre><code class="bash">$&gt; cd ~/tutorials/HPL
$&gt; mkdir runs    # Prepare the specific run directory
</code></pre>

<p>Now you'll have to find the optimal set of parameters for using a single
node. You can use the following site:
<a href="http://hpl-calculator.sourceforge.net/">HPL Calculator</a> to find good parameters
and expected performances and adapt <code>bin/intel64/HPL.dat</code> accordingly.
Here we are going to use reasonable choices as outline from <a href="http://www.advancedclustering.com/act_kb/tune-hpl-dat-file/">this website</a></p>
<h3 id="slurm-launcher-intel-mpi">Slurm launcher (Intel MPI)</h3>
<p>Copy and adapt the <a href="https://github.com/ULHPC/launcher-scripts/blob/devel/slurm/launcher.default.sh">default SLURM launcher</a> you should have a copy in <code>~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh</code></p>
<pre><code class="bash">$&gt; cd ~/tutorials/HPL/runs
# Prepare a laucnher for intel suit
$&gt; cp ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh launcher-HPL.intel.sh
</code></pre>

<p>Take your favorite editor (<code>vim</code>, <code>nano</code>, etc.) to modify it according to your needs.</p>
<p>Here is for instance a suggested difference for intel MPI:</p>
<pre><code class="diff">--- ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh  2017-06-11 23:40:34.007152000 +0200
+++ launcher-HPL.intel.sh       2017-06-11 23:41:57.597055000 +0200
@@ -10,8 +10,8 @@
 #
 #          Set number of resources
 #
-#SBATCH -N 1
+#SBATCH -N 2
 #SBATCH --ntasks-per-node=28
 ### -c, --cpus-per-task=&lt;ncpus&gt;
 ###     (multithreading) Request that ncpus be allocated per process
 #SBATCH -c 1
@@ -64,15 +64,15 @@
 module load toolchain/intel

 # Directory holding your built applications
-APPDIR=&quot;$HOME&quot;
+APPDIR=&quot;$HOME/tutorials/HPL/src/hpl-2.2/bin/intel64&quot;
 # The task to be executed i.E. your favorite Java/C/C++/Ruby/Perl/Python/R/whatever program
 # to be invoked in parallel
-TASK=&quot;${APPDIR}/app.exe&quot;
+TASK=&quot;${APPDIR}/xhpl&quot;

 # The command to run
-CMD=&quot;${TASK}&quot;
+# CMD=&quot;${TASK}&quot;
 ### General MPI Case:
-# CMD=&quot;srun -n $SLURM_NTASKS ${TASK}&quot;
+CMD=&quot;srun -n $SLURM_NTASKS ${TASK}&quot;
 ### OpenMPI case if you wish to specialize the MCA parameters
 #CMD=&quot;mpirun -np $SLURM_NTASKS --mca btl openib,self,sm ${TASK}&quot;
</code></pre>

<p>Now you should create an input <code>HPL.dat</code> file within the <code>runs/</code>.</p>
<pre><code class="bash">$&gt; cd ~/tutorials/HPL/runs
$&gt; cp ../ref.ulhpc.d/HPL.dat .
$&gt; ll
total 0
-rw-r--r--. 1 svarrette clusterusers 1.5K Jun 12 15:38 HPL.dat
-rwxr-xr-x. 1 svarrette clusterusers 2.7K Jun 12 15:25 launcher-HPL.intel.sh
</code></pre>

<p>You are ready for testing a batch job:</p>
<pre><code class="bash">$&gt; cd ~/tutorials/HPL/runs
$&gt; sbatch ./launcher-HPL.intel.sh
$&gt; sq     # OR (long version) squeue -u $USER
</code></pre>

<p><strong>(bonus)</strong> Connect to one of the allocated nodes and run <code>htop</code> (followed by <code>u</code> to select process run under your username, and <code>F5</code> to enable the tree-view.</p>
<p>Now you can check the output of the HPL runs:</p>
<pre><code class="bash">$&gt; grep WR slurm-&lt;jobid&gt;.out    # /!\ ADAPT &lt;jobid&gt; appropriately.
</code></pre>

<p>Of course, we made here a small test and optimizing the HPL parameters to get the best performances and efficiency out of a given HPC platform is not easy.
Below are some plots obtained when benchmarking the <code>iris</code> cluster and seeking the best set of parameters across increasing number of nodes (see <a href="https://hpc.uni.lu/blog/2017/preliminary-performance-results-of-the-iris-cluster/">this blog post</a>)</p>
<p><img alt="" src="https://hpc.uni.lu/images/benchs/benchmark_HPL-iris_25N.png" />
<img alt="" src="https://hpc.uni.lu/images/benchs/benchmark_HPL-iris_50N.png" />
<img alt="" src="https://hpc.uni.lu/images/benchs/benchmark_HPL-iris_75N.png" />
<img alt="" src="https://hpc.uni.lu/images/benchs/benchmark_HPL-iris_100N.png" /></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../HPCG/" class="btn btn-neutral float-right" title="High Performance Conjugate Gradient (HPCG)">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../OSU_MicroBenchmarks/" class="btn btn-neutral" title="OSU Micro-benchmarks"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../OSU_MicroBenchmarks/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../HPCG/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
