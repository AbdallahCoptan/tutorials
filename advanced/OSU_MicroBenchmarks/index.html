<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <title>OSU Micro-benchmarks - UL HPC Tutorials</title>
  

  <link rel="shortcut icon" href="../../img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">

  
  <script>
    // Current page data
    var mkdocs_page_name = "OSU Micro-benchmarks";
    var mkdocs_page_input_path = "advanced/OSU_MicroBenchmarks/index.md";
    var mkdocs_page_url = "/advanced/OSU_MicroBenchmarks/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script>
  <script src="../../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> UL HPC Tutorials</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">Home</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Basic Tutorials</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../basic/getting_started/">Getting Started</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../basic/sequential_jobs/">HPC workflow with sequential jobs</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Debug/">Efficient Debugging</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Advanced Software Management</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../EasyBuild/">Easybuild</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../RESIF/">RESIF</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>MPI / Performance Evaluation</span></li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">OSU Micro-benchmarks</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#ul-hpc-mpi-tutorial-building-and-runnning-osu-micro-benchmarks">UL HPC MPI Tutorial: Building and Runnning OSU Micro-Benchmarks</a></li>
                
                    <li><a class="toctree-l4" href="#objectives">Objectives</a></li>
                
                    <li><a class="toctree-l4" href="#pre-requisites">Pre-requisites</a></li>
                
                    <li><a class="toctree-l4" href="#building-the-osu-micro-benchmarks">Building the OSU Micro-benchmarks</a></li>
                
                    <li><a class="toctree-l4" href="#preparing-batch-runs">Preparing batch runs</a></li>
                
                    <li><a class="toctree-l4" href="#now-for-lazy-frustrated-persons">Now for Lazy / frustrated persons</a></li>
                
            
            </ul>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../HPL/">High Performance Linpack (HPL)</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../HPCG/">High Performance Conjugate Gradient (HPCG)</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Mathematics</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../MATLAB1/README/">MATLAB</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../R/">R - statistical computing</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Bioinformatics</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Bioinformatics/">Running bioinformatics software</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Galaxy/">Galaxy</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Parallel Debuggers</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Allinea/">Allinea</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../TotalView/">TotalView</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Virtualization</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Vagrant/">Vagrant</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../vm5k/">Grid5000 - Automatic VM deployment with VM5K</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>CFD/MD/Chemistry</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../MultiPhysics/">Advanced Parallel execution</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Big Data</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Spark/">Big Data Application Over Apache Spark</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Advanced topics</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../advanced_workflows/README/">Advanced workflows on sequential jobs management</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../advanced_scheduling/">Advanced scheduling with SLURM</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Python/">Prototyping with Python</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../ReproducibleResearch/">Reproducible Research</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../rtfd/">Documentation (RTFD)</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../contributing/">Contributing</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../contacts/">Contacts</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">UL HPC Tutorials</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>MPI / Performance Evaluation &raquo;</li>
        
      
    
    <li>OSU Micro-benchmarks</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>-<em>- mode: markdown; mode: visual-line; fill-column: 80 -</em>-</p>
<p>Copyright (c) 2013-2017 UL HPC Team  <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#104;&#112;&#99;&#45;&#115;&#121;&#115;&#97;&#100;&#109;&#105;&#110;&#115;&#64;&#117;&#110;&#105;&#46;&#108;&#117;">&#104;&#112;&#99;&#45;&#115;&#121;&#115;&#97;&#100;&#109;&#105;&#110;&#115;&#64;&#117;&#110;&#105;&#46;&#108;&#117;</a></p>
<hr />
<h1 id="ul-hpc-mpi-tutorial-building-and-runnning-osu-micro-benchmarks">UL HPC MPI Tutorial: Building and Runnning OSU Micro-Benchmarks</h1>
<p><a href="https://hpc.uni.lu"><img alt="By ULHPC" src="https://img.shields.io/badge/by-ULHPC-blue.svg" /></a> <a href="http://www.gnu.org/licenses/gpl-3.0.html"><img alt="Licence" src="https://img.shields.io/badge/license-GPL--3.0-blue.svg" /></a> <a href="https://github.com/ULHPC/tutorials/issues/"><img alt="GitHub issues" src="https://img.shields.io/github/issues/ULHPC/tutorials.svg" /></a> <a href="https://github.com/ULHPC/tutorials/raw/devel/advanced/OSU_MicroBenchmarks/slides.pdf"><img alt="" src="https://img.shields.io/badge/slides-PDF-red.svg" /></a> <a href="https://github.com/ULHPC/tutorials/tree/devel/advanced/OSU_MicroBenchmarks"><img alt="Github" src="https://img.shields.io/badge/sources-github-green.svg" /></a> <a href="http://ulhpc-tutorials.readthedocs.io/en/latest/advanced/OSU_MicroBenchmarks/"><img alt="Documentation Status" src="http://readthedocs.org/projects/ulhpc-tutorials/badge/?version=latest" /></a> <a href="https://github.com/ULHPC/tutorials"><img alt="GitHub forks" src="https://img.shields.io/github/stars/ULHPC/tutorials.svg?style=social&amp;label=Star" /></a></p>
<p><a href="https://github.com/ULHPC/tutorials/raw/devel/advanced/OSU_MicroBenchmarks/slides.pdf"><img alt="" src="https://github.com/ULHPC/tutorials/raw/devel/advanced/OSU_MicroBenchmarks/cover_slides.png" /></a></p>
<p>The objective of this tutorial is to compile and run on of the <a href="http://mvapich.cse.ohio-state.edu/benchmarks/">OSU micro-benchmarks</a> which permit to measure the performance of an MPI implementation.
<strong>On purpose</strong>, we won't check here if an Easybuild recipe is available for this software to conduct a full build <em>by hand</em> and check the difference in build options between the different MPI suits.</p>
<p>You can work in groups for this training, yet individual work is encouraged to ensure you understand and practice the usage of MPI programs on an HPC platform.</p>
<p>In all cases, ensure you are able to <a href="https://hpc.uni.lu/users/docs/access.html">connect to the UL HPC  clusters</a>.</p>
<pre><code class="bash"># /!\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A COMPUTING NODE
# Have an interactive job
(access)$&gt; si -N 2 --ntasks-per-node=1                    # iris
(access)$&gt; srun -p interactive --qos qos-iteractive -N 2 --ntasks-per-node=1 --pty bash  # iris (long version)
(access)$&gt; oarsub -I -l enclosure=1/nodes=2,walltime=4   # chaos / gaia
</code></pre>

<p><strong>Advanced users only</strong>: rely on <code>screen</code> (see  <a href="http://support.suso.com/supki/Screen_tutorial">tutorial</a> or the <a href="https://hpc.uni.lu/users/docs/ScreenSessions.html">UL HPC tutorial</a> on the  frontend prior to running any <code>oarsub</code> or <code>srun/sbatch</code> command to be more resilient to disconnection.</p>
<p>The latest version of this tutorial is available on <a href="https://github.com/ULHPC/tutorials/tree/devel/advanced/OSU_MicroBenchmarks">Github</a>.
Finally, advanced MPI users might be interested to take a look at the <a href="https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor">Intel Math Kernel Library Link Line Advisor</a>.</p>
<h2 id="objectives">Objectives</h2>
<p>The <a href="http://mvapich.cse.ohio-state.edu/benchmarks/">OSU micro-benchmarks</a> feature a series of MPI benchmarks that measure the performances of various MPI operations:</p>
<ul>
<li><strong>Point-to-Point MPI Benchmarks</strong>: Latency, multi-threaded latency, multi-pair latency, multiple bandwidth / message rate test bandwidth, bidirectional bandwidth</li>
<li><strong>Collective MPI Benchmarks</strong>: Collective latency tests for various MPI collective operations such as MPI_Allgather, MPI_Alltoall, MPI_Allreduce, MPI_Barrier, MPI_Bcast, MPI_Gather, MPI_Reduce, MPI_Reduce_Scatter, MPI_Scatter and vector collectives.</li>
<li><strong>One-sided MPI Benchmarks</strong>: one-sided put latency (active/passive), one-sided put bandwidth (active/passive), one-sided put bidirectional bandwidth, one-sided get latency (active/passive), one-sided get bandwidth (active/passive), one-sided accumulate latency (active/passive), compare and swap latency (passive), and fetch and operate (passive) for MVAPICH2 (MPI-2 and MPI-3).</li>
<li>Since the 4.3 version, the <a href="http://mvapich.cse.ohio-state.edu/benchmarks/">OSU micro-benchmarks</a> also features OpenSHMEM benchmarks, a 1-sided communications library.</li>
</ul>
<p>In this tutorial, we will build <strong>version 5.3.2 of the <a href="http://mvapich.cse.ohio-state.edu/benchmarks/">OSU micro-benchmarks</a></strong> (the latest at the time of writing), and focus on two of the available tests:</p>
<ul>
<li><code>osu_get_latency</code> - Latency Test</li>
<li><code>osu_get_bw</code> - Bandwidth Test</li>
</ul>
<blockquote>
<p>The latency tests are carried out in a ping-pong fashion. The sender sends a message with a certain data size to the receiver and waits for a reply from the receiver. The receiver receives the message from the sender and sends back a reply with the same data size. Many iterations of this ping-pong test are carried out and average one-way latency numbers are obtained. Blocking version of MPI functions (MPI_Send and MPI_Recv) are used in the tests.</p>
<p>The bandwidth tests were carried out by having the sender sending out a fixed number (equal to the window size) of back-to-back messages to the receiver and then waiting for a reply from the receiver. The receiver sends the reply only after receiving all these messages. This process is repeated for several iterations and the bandwidth is calculated based on the elapsed time (from the time sender sends the first message until the time it receives the reply back from the receiver) and the number of bytes sent by the sender. The objective of this bandwidth test is to determine the maximum sustained date rate that can be achieved at the network level. Thus, non-blocking version of MPI functions (MPI_Isend and MPI_Irecv) were used in the test.</p>
</blockquote>
<p>The idea is to compare the different MPI implementations available on the <a href="http://hpc.uni.lu">UL HPC platform</a>.:</p>
<ul>
<li><a href="http://software.intel.com/en-us/intel-mpi-library/">Intel MPI</a></li>
<li><a href="http://www.open-mpi.org/">OpenMPI</a></li>
<li><a href="http://mvapich.cse.ohio-state.edu/overview/">MVAPICH2</a> (MPI-3 over OpenFabrics-IB, Omni-Path, OpenFabrics-iWARP, PSM, and TCP/IP)</li>
</ul>
<p>For the sake of time and simplicity, we will focus on the first two suits. Eventually, the benchmarking campain will typically involves for each MPI suit:</p>
<ul>
<li>two nodes, belonging to the <em>same</em> enclosure</li>
<li>two nodes, belonging to <em>different</em> enclosures</li>
</ul>
<h2 id="pre-requisites">Pre-requisites</h2>
<p>On the <strong>access</strong> and a <strong>computing</strong> node of the cluster you're working on, clone the <a href="https://github.com/ULHPC/tutorials">ULHPC/tutorials</a>  and <a href="https://github.com/ULHPC/launcher-scripts">ULHPC/launcher-scripts</a> repositories</p>
<pre><code class="bash">$&gt; cd
$&gt; mkdir -p git/ULHPC &amp;&amp; cd  git/ULHPC
$&gt; git clone https://github.com/ULHPC/launcher-scripts.git
$&gt; git clone https://github.com/ULHPC/tutorials.git         # If not yet done
</code></pre>

<p>Prepare your working directory</p>
<pre><code class="bash">$&gt; mkdir -p ~/tutorials/OSU-MicroBenchmarks
$&gt; cd ~/tutorials/OSU-MicroBenchmarks
$&gt; ln -s ~/git/ULHPC/tutorials/advanced/OSU_MicroBenchmarks ref.ulhpc.d   # Keep a symlink to the reference tutorial
$&gt; ln -s ref.ulhpc.d/Makefile .     # symlink to the root Makefile
</code></pre>

<p>Fetch and uncompress the latest version of the <a href="http://mvapich.cse.ohio-state.edu/benchmarks/">OSU micro-benchmarks</a></p>
<pre><code class="bash">$&gt; cd ~/tutorials/OSU-MicroBenchmarks
$&gt; mkdir src
$&gt; cd src
# Download the latest version
$&gt; export OSU_VERSION=5.3.2     # Just to abstract from the version to download
$&gt; wget --no-check-certificate http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-${OSU_VERSION}.tar.gz
$&gt; tar xvzf osu-micro-benchmarks-${OSU_VERSION}.tar.gz
$&gt; cd osu-micro-benchmarks-${OSU_VERSION}
</code></pre>

<h2 id="building-the-osu-micro-benchmarks">Building the OSU Micro-benchmarks</h2>
<p>We will build the <a href="http://mvapich.cse.ohio-state.edu/benchmarks/">OSU micro-benchmarks</a> for each considered MPI suit, thus in a separate directory <code>build.&lt;suit&gt;</code> -- that's a good habbit you're encouraged (as with <a href="https://cmake.org/">CMake</a> based projects)
In all cases, you <strong>should</strong> now operate the compilation within an interactive job to be able to use the <code>module</code> command.</p>
<pre><code class="bash"># If not yet done
(access)$&gt; si -N 2 --ntasks-per-node=1                   # on iris (1 core on 2 nodes)
(access)$&gt; oarsub -I -l enclosure=1/nodes=2,walltime=4   # chaos / gaia
</code></pre>

<h3 id="compilation-based-on-the-intel-mpi-suit">Compilation based on the Intel MPI suit</h3>
<p>We are first going to use the <a href="http://software.intel.com/en-us/intel-cluster-toolkit-compiler/">Intel Cluster Toolkit Compiler Edition</a>,
which provides Intel C/C++ and Fortran compilers, Intel MPI.
We will compile the <a href="http://mvapich.cse.ohio-state.edu/benchmarks/">OSU micro-benchmarks</a> in a specific directory (that a good habbit)</p>
<pre><code class="bash">$&gt; cd ~/tutorials/OSU-MicroBenchmarks/
$&gt; mkdir build.intel    # Prepare the specific building directory
$&gt; cd  build.intel
# Load the appropriate module
$&gt; module spider MPI     # Search for available modules featuring MPI
$&gt; module load toolchain/intel   # On iris -- use 'module load toolchain/ictce' otherwise
$&gt; module list
Currently Loaded Modules:
  1) compiler/GCCcore/6.3.0                   4) compiler/ifort/2017.1.132-GCC-6.3.0-2.27                 7) toolchain/iimpi/2017a
  2) tools/binutils/2.27-GCCcore-6.3.0        5) toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27             8) numlib/imkl/2017.1.132-iimpi-2017a
  3) compiler/icc/2017.1.132-GCC-6.3.0-2.27   6) mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27   9) toolchain/intel/2017a

# Configure the Intel MPI-based build for installation in the current directory
$&gt; ../src/osu-micro-benchmarks-5.4/configure CC=mpiicc CXX=mpiicpc CFLAGS=$(pwd)/../src/osu-micro-benchmarks-5.4/util --prefix=$(pwd)
$&gt; make &amp;&amp; make install
</code></pre>

<p>If everything goes fine, you shall have the <a href="http://mvapich.cse.ohio-state.edu/benchmarks/">OSU micro-benchmarks</a> installed in the directory <code>libexec/osu-micro-benchmarks/mpi/</code>.</p>
<p>Once compiled, ensure you are able to run it:</p>
<pre><code class="bash">$&gt; cd libexec/osu-micro-benchmarks/mpi/one-sided/

#### On iris
$&gt; srun -n $SLURM_NTASKS ./osu_get_latency
$&gt; srun -n $SLURM_NTASKS ./osu_get_bw

#### On gaia, chaos
$&gt; mpirun -hostfile $OAR_NODEFILE -perhost 1 ./osu_get_latency
$&gt; mpirun -hostfile $OAR_NODEFILE -perhost 1 ./osu_get_bw
</code></pre>

<h3 id="compilation-based-on-the-openmpi-suit">Compilation based on the OpenMPI suit</h3>
<p>Repeat the procedure for the OpenMPI suit:</p>
<pre><code class="bash">$&gt; cd ~/tutorials/OSU-MicroBenchmarks/
$&gt; mkdir build.openmpi    # Prepare the specific building directory
$&gt; cd  build.openmpi
# Clean the previously loaded module and load the appropriate OpenMPI one
$&gt; module purge
$&gt; module spider OpenMPI
$&gt; module load mpi/OpenMPI
$&gt; module list

Currently Loaded Modules:
  1) compiler/GCCcore/6.3.0              3) compiler/GCC/6.3.0-2.28              5) system/hwloc/1.11.7-GCC-6.3.0-2.28
  2) tools/binutils/2.28-GCCcore-6.3.0   4) tools/numactl/2.0.11-GCCcore-6.3.0   6) mpi/OpenMPI/2.1.1-GCC-6.3.0-2.28

# Configure the OpenMPI-based build for installation in the current directory
$&gt; ../src/osu-micro-benchmarks-5.4/configure CC=mpicc --prefix=$(pwd)
$&gt; make &amp;&amp; make install
</code></pre>

<p>Once compiled, ensure you are able to run it:</p>
<pre><code class="bash">$&gt; cd libexec/osu-micro-benchmarks/mpi/one-sided/

#### On iris
$&gt; srun -n $SLURM_NTASKS ./osu_get_latency   # OR mpirun -npernode 1 --mca btl openib,self,sm  ./osu_get_latency
$&gt; srun -n $SLURM_NTASKS ./osu_get_bw        # OR mpirun -npernode 1 --mca btl openib,self,sm  ./osu_get_bw
# Or, if you don't want to use PMI2
$&gt; mpirun -np $SLURM_NTASKS -perhost 1 --mca btl openib,self,sm ./osu_get_{latency,bw}

#### On gaia, chaos
$&gt; mpirun -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -npernode 1 ./osu_get_latency
$&gt; mpirun -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -npernode 1 ./osu_get_bw
</code></pre>

<h2 id="preparing-batch-runs">Preparing batch runs</h2>
<p>We are now going to prepare launcher scripts to permit passive runs (typically in the <code>{default | batch}</code> queue).
We will place them in a separate directory (<code>runs/</code>) as it will host the outcomes of the executions on the UL HPC platform .</p>
<pre><code class="bash">$&gt; cd ~/tutorials/OSU-MicroBenchmarks/
$&gt; mkdir runs    # Prepare the specific run directory
</code></pre>

<h3 id="slurm-launcher-intel-mpi">Slurm launcher (Intel MPI)</h3>
<p>Copy and adapt the <a href="https://github.com/ULHPC/launcher-scripts/blob/devel/slurm/launcher.default.sh">default SLURM launcher</a> you should have a copy in <code>~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh</code></p>
<pre><code class="bash">$&gt; cd ~/tutorials/OSU-MicroBenchmarks/runs
# Prepare a laucnher for intel suit
$&gt; cp ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh launcher-OSU.intel.sh
</code></pre>

<p>Take your favorite editor (<code>vim</code>, <code>nano</code>, etc.) to modify it according to your needs.
Here is for instance a suggested difference for intel MPI:</p>
<pre><code class="diff">--- ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh  2017-06-11 23:40:34.007152000 +0200
+++ launcher-OSU.intel.sh       2017-06-11 23:41:57.597055000 +0200
@@ -10,8 +10,8 @@
 #
 #          Set number of resources
 #
-#SBATCH -N 1
-#SBATCH --ntasks-per-node=28
+#SBATCH -N 2
+#SBATCH --ntasks-per-node=1
 ### -c, --cpus-per-task=&lt;ncpus&gt;
 ###     (multithreading) Request that ncpus be allocated per process
 #SBATCH -c 1
@@ -64,15 +64,15 @@
 module load toolchain/intel

 # Directory holding your built applications
-APPDIR=&quot;$HOME&quot;
+APPDIR=&quot;$HOME/tutorials/OSU-MicroBenchmarks/build.intel/libexec/osu-micro-benchmarks/mpi/one-sided&quot;
 # The task to be executed i.E. your favorite Java/C/C++/Ruby/Perl/Python/R/whatever program
 # to be invoked in parallel
-TASK=&quot;${APPDIR}/app.exe&quot;
+TASK=&quot;${APPDIR}/$1&quot;

 # The command to run
-CMD=&quot;${TASK}&quot;
+# CMD=&quot;${TASK}&quot;
 ### General MPI Case:
-# CMD=&quot;srun -n $SLURM_NTASKS ${TASK}&quot;
+CMD=&quot;srun -n $SLURM_NTASKS ${TASK}&quot;
 ### OpenMPI case if you wish to specialize the MCA parameters
 #CMD=&quot;mpirun -np $SLURM_NTASKS --mca btl openib,self,sm ${TASK}&quot;
</code></pre>

<p>If you apply the above changes, you can test the script in an interactive job as follows:</p>
<pre><code class="bash">$&gt; cd ~/tutorials/OSU-MicroBenchmarks/runs
$&gt; si -N 2 --ntasks-per-node=1     # create an interactive job, 1 core on 2 nodes
$&gt; ./launcher-OSU.intel.sh osu_get_bw
$&gt; ./launcher-OSU.intel.sh osu_get_latency
</code></pre>

<p>And then test it in batch mode:</p>
<pre><code class="bash">$&gt; cd ~/tutorials/OSU-MicroBenchmarks/runs
$&gt; sbatch ./launcher-OSU.intel.sh osu_get_bw
$&gt; sbatch ./launcher-OSU.intel.sh osu_get_latency
</code></pre>

<h3 id="slurm-launcher-openmpi">Slurm launcher (OpenMPI)</h3>
<pre><code class="bash">$&gt; cd ~/tutorials/OSU-MicroBenchmarks/runs
$&gt; cp launcher-OSU.intel.sh launcher-OSU.openmpi.sh
</code></pre>

<p>Take again your favorite editor (<code>vim</code>, <code>nano</code>, etc.) to modify <code>launcher-OSU.openmpi.sh</code> as follows:</p>
<pre><code class="diff">--- launcher-OSU.intel.sh       2017-06-11 23:41:57.597055000 +0200
+++ launcher-OSU.openmpi.sh     2017-06-11 23:46:04.589924000 +0200
@@ -61,10 +61,10 @@

 # Load the {intel | foss} toolchain and whatever module(s) you need
 module purge
-module load toolchain/intel
+module load mpi/OpenMPI

 # Directory holding your built applications
-APPDIR=&quot;$HOME/tutorials/OSU-MicroBenchmarks/build.intel/libexec/osu-micro-benchmarks/mpi/one-sided&quot;
+APPDIR=&quot;$HOME/tutorials/OSU-MicroBenchmarks/build.openmpi/libexec/osu-micro-benchmarks/mpi/one-sided&quot;
 # The task to be executed i.E. your favorite Java/C/C++/Ruby/Perl/Python/R/whatever program
 # to be invoked in parallel
 TASK=&quot;${APPDIR}/$1&quot;
</code></pre>

<p>And then test it in (passive) batch mode:</p>
<pre><code class="bash">$&gt; cd ~/tutorials/OSU-MicroBenchmarks/runs
$&gt; sbatch ./launcher-OSU.openmpi.sh osu_get_bw
$&gt; sbatch ./launcher-OSU.openmpi.sh osu_get_latency
</code></pre>

<h3 id="slurm-launcher-openmpi-over-ethernet-interface">Slurm launcher (OpenMPI over Ethernet interface)</h3>
<p>By default, the MPI communications are operated over the fast Infiniband interconnect.
With OpenMPI, we can <em>force</em> them over the Ethernet network to highlight the performance impact of using such a slower network.</p>
<pre><code class="bash">$&gt; cd ~/tutorials/OSU-MicroBenchmarks/runs
$&gt; cp launcher-OSU.openmpi.sh launcher-OSU.openmpi-eth.sh
</code></pre>

<p>Take again your favorite editor (<code>vim</code>, <code>nano</code>, etc.) to modify <code>launcher-OSU.openmpi-eth.sh</code> as follows:</p>
<pre><code class="diff">--- launcher-OSU.openmpi.sh     2017-06-11 23:46:04.589924000 +0200
+++ launcher-OSU.openmpi-eth.sh 2017-06-11 23:55:02.239258000 +0200
@@ -72,9 +72,9 @@
 # The command to run
 # CMD=&quot;${TASK}&quot;
 ### General MPI Case:
-CMD=&quot;srun -n $SLURM_NTASKS ${TASK}&quot;
+# CMD=&quot;srun -n $SLURM_NTASKS ${TASK}&quot;
 ### OpenMPI case if you wish to specialize the MCA parameters
-#CMD=&quot;mpirun -np $SLURM_NTASKS --mca btl openib,self,sm ${TASK}&quot;
+CMD=&quot;mpirun -np $SLURM_NTASKS -npernode 1 --mca btl tcp,self ${TASK}&quot;

 ### Prepare logfile
 LOGFILE=&quot;${RUNDIR}/$(date +%Y-%m-%d)_$(basename ${TASK})_${SLURM_JOBID}.log&quot;
</code></pre>

<p>And then test it in (passive) batch mode:</p>
<pre><code class="bash">$&gt; cd ~/tutorials/OSU-MicroBenchmarks/runs
$&gt; sbatch ./launcher-OSU.openmpi-eth.sh osu_get_bw
$&gt; sbatch ./launcher-OSU.openmpi-eth.sh osu_get_latency
</code></pre>

<p>You can find the obtained results on the <code>iris</code> cluster:</p>
<p><img alt="" src="./plots/benchmark_OSU_iris_latency.png" />
<img alt="" src="./plots/benchmark_OSU_iris_bandwidth.png" /></p>
<h3 id="oar-launcher-intel-mpi">OAR launcher (Intel MPI)</h3>
<p>In the case of OAR (<em>i.e.</em> on the <code>gaia</code> and <code>chaos</code> cluster), you can use the <a href="https://github.com/ULHPC/launcher-scripts/blob/devel/bash/MPI/mpi_launcher.sh">MPI generic launcher</a> to run the code:</p>
<pre><code class="bash">$&gt; ~/tutorials/OSU-MicroBenchmarks/runs
$&gt; ln -s ~/git/ULHPC/launcher-scripts/bash/MPI/mpi_launcher.sh launcher-OSU.intel.sh
$&gt; ./launcher-OSU.intel.sh \
     --basedir $HOME/tutorials/OSU-MicroBenchmarks/src/osu-micro-benchmarks-5.4/build.intel/libexec/osu-micro-benchmarks/mpi/one-sided \
     --npernode 1 --module toolchain/intel --exe osu_get_latency,osu_get_bw
</code></pre>

<p>If you want to avoid this long list of arguments, just create a file <code>launcher-OSU.intel.default.conf</code> to contain:</p>
<pre><code class="bash"># Defaults settings for running the OSU Micro benchmarks compiled with Intel MPI
NAME=OSU.intel

MODULE_TO_LOADstr=toolchain/intel
MPI_PROG_BASEDIR=$HOME/tutorials/OSU-MicroBenchmarks/src/osu-micro-benchmarks-5.4/build.intel/libexec/osu-micro-benchmarks/mpi/one-sided/

MPI_PROGstr=osu_get_latency,osu_get_bw
MPI_NPERNODE=1
</code></pre>

<p>Now you can run the launcher script (interactively, or not):</p>
<pre><code class="bash"># IF within an interactive job
$&gt; ./launcher-OSU.intel.sh
# You might want also to host the output files in the local directory (under the date)
$&gt; ./launcher-OSU.intel.sh --datadir data/$(date +%Y-%m-%d)

# Submitting a passive job
$&gt; oarsub -S ./launcher-OSU.intel.sh
</code></pre>

<h3 id="oar-launcher-openmpi">OAR launcher (OpenMPI)</h3>
<p>Again, we will rely on the <a href="https://github.com/ULHPC/launcher-scripts/blob/devel/bash/MPI/mpi_launcher.sh">MPI generic launcher</a> to run the code:</p>
<pre><code class="bash">$&gt; ~/tutorials/OSU-MicroBenchmarks/runs
$&gt; ln -s ~/git/ULHPC/launcher-scripts/bash/MPI/mpi_launcher.sh launcher-OSU.openmpi.sh
$&gt; vim launcher-OSU.openmpi.default.conf
[...] # See below for content
$&gt; cat launcher-OSU.openmpi.default.conf
# Defaults settings for running the OSU Micro benchmarks wompiled with OpenMPI
NAME=OSU.openmpi

MODULE_TO_LOADstr=mpi/OpenMPI
    MPI_PROG_BASEDIR=$HOME/tutorials/OSU-MicroBenchmarks/src/osu-micro-benchmarks-5.4/build.openmpi/libexec/osu-micro-benchmarks/mpi/one-sided/

MPI_PROGstr=osu_get_latency,osu_get_bw
MPI_NPERNODE=1

# Submit a passive job
$&gt; oarsub -S ./launcher-OSU.openmpi.sh
</code></pre>

<h3 id="benchmarking-on-two-nodes">Benchmarking on two nodes</h3>
<p>Operate the benchmarking campain (in the two cases) in the following context:</p>
<ul>
<li>
<p>2 nodes belonging to the same enclosure. Use for that:</p>
<pre><code>$&gt; oarsub -l enclosure=1/nodes=2,walltime=8 […]
</code></pre>
</li>
<li>
<p>2 nodes belonging to the different enclosures:</p>
<pre><code>$&gt; oarsub -l enclosure=2/core=1,walltime=8 […]
</code></pre>
</li>
</ul>
<h2 id="now-for-lazy-frustrated-persons">Now for Lazy / frustrated persons</h2>
<p>You will find in the <a href="https://github.com/ULHPC/tutorials">UL HPC tutorial</a>
repository, under the <code>advanced/OSU_MicroBenchmarks</code> directory, a set of tools / script that
facilitate the running and analysis of this tutorial that you can use/adapt to
suit your needs.</p>
<p>In particular, once in the <code>advanced/OSU_MicroBenchmarks</code> directory:</p>
<ul>
<li>running <code>make fetch</code> will automatically download the archives for the <a href="http://mvapich.cse.ohio-state.edu/benchmarks/">OSU micro-benchmarks</a> in the <code>src/</code> directory</li>
<li>The  different launcher files in <code>runs/</code></li>
<li>Some sample output data in <code>runs/data/</code></li>
<li>run <code>make build</code> to build the different versions of the OSU Micro-benchmarks</li>
<li>run <code>make plot</code> to invoke the <a href="http://www.gnuplot.info/">Gnuplot</a> script
  <code>plots/benchmark_OSU.gnuplot</code> and generate various plots from the sample
  runs.</li>
</ul>
<p>In particular, you'll probably want to see the comparison figure extracted from
the sample run in <code>plots/benchmark_OSU_2H_latency.pdf</code> and <code>plots/benchmark_OSU_2H_bandwidth.pdf</code></p>
<p>A PNG version of these plots is available on Github:
<a href="https://raw.github.com/ULHPC/tutorials/devel/advanced/OSU_MicroBenchmarks/plots/benchmark_OSU_2H_latency.png">OSU latency</a> -- <a href="https://raw.github.com/ULHPC/tutorials/devel/advanced/OSU_MicroBenchmarks/plots/benchmark_OSU_2H_bandwidth.png">OSU Bandwidth</a></p>
<p><img alt="OSU latency" src="https://raw.github.com/ULHPC/tutorials/devel/advanced/OSU_MicroBenchmarks/plots/benchmark_OSU_2H_latency.png" />
<img alt="OSU Bandwidth" src="https://raw.github.com/ULHPC/tutorials/devel/advanced/OSU_MicroBenchmarks/plots/benchmark_OSU_2H_bandwidth.png" /></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../HPL/" class="btn btn-neutral float-right" title="High Performance Linpack (HPL)">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../RESIF/" class="btn btn-neutral" title="RESIF"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../RESIF/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../HPL/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
