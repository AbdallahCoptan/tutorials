{
    "docs": [
        {
            "location": "/",
            "text": "-\n- mode: markdown; mode: visual-line;  -\n-\n\n\nUL HPC Tutorials\n\n\n \n \n \n \n \n\n\n\n\nCopyright (c) 2015-2017 UL HPC Team \nhpc-sysadmins@uni.lu\n\n\n\n\nThis repository holds a set of tutorials to help the users of the \nUL HPC\n platform to better understand or simply use our platform.\nThe list of proposed tutorials are evolving and are used on a regular basis during the \nUL HPC School\n we organise.\nSo far, the following tutorials are proposed:\n\n\n\n\n\n\n\n\nCategory\n\n\nDescription\n\n\nLevel\n\n\n\n\n\n\n\n\n\n\nBasic\n\n\nGetting Started on the UL HPC platform\n\n\nbeginners\n\n\n\n\n\n\n\n\nHPC workflow with sequential jobs\n\n\nbeginners\n\n\n\n\n\n\n\n\nKnow Your Bugs: Weapons for Efficient Debugging\n\n\nintermediate\n\n\n\n\n\n\nSoftware Management\n\n\nBuilding [custom] software with EasyBuild\n\n\nbeginners\n\n\n\n\n\n\nMPI\n\n\nUL HPC MPI Tutorial: OSU Micro-Benchmarks\n\n\nintermediate\n\n\n\n\n\n\n\n\nHigh-Performance Linpack (HPL) benchmarking on UL HPC platform\n\n\nintermediate\n\n\n\n\n\n\n\n\nHPCG benchmarking on UL HPC platform\n\n\nintermediate\n\n\n\n\n\n\nMathematics\n\n\nMATLAB execution on the UL HPC platform\n\n\nintermediate\n\n\n\n\n\n\n\n\nR / Statictical Computing\n\n\nintermediate\n\n\n\n\n\n\nBioinformatics\n\n\nBioinformatics software on the UL HPC platform\n\n\nintermediate\n\n\n\n\n\n\n\n\nGalaxy Introduction Exercise: From Peaks to Genes\n\n\nintermediate\n\n\n\n\n\n\nParallel Debuging\n\n\nUnified profiling and debugging with Allinea\n\n\nintermediate\n\n\n\n\n\n\n\n\nDirect,  Reverse and parallel Memory debugging with TotalView\n\n\nintermediate\n\n\n\n\n\n\nVirtualization\n\n\nCreate and reproduce work environments using Vagrant\n\n\nintermediate\n\n\n\n\n\n\n\n\nDeploying virtual machines with Vm5k on Grid'5000\n\n\nintermediate\n\n\n\n\n\n\nCFD/MD/Chemistry\n\n\nRunning parallel software: test cases on CFD / MD / Chemistry applications\n\n\nadvanced\n\n\n\n\n\n\nBig Data\n\n\nRunning Big Data Application using Apache Spark on UL HPC platform\n\n\nintermediate\n\n\n\n\n\n\nMisc\n\n\nAdvanced workflows on sequential jobs management\n\n\nadvanced\n\n\n\n\n\n\n\n\nAdvanced scheduling with SLURM\n\n\nintermediate\n\n\n\n\n\n\n\n\n[Advanced] Prototyping with Python\n\n\nadvanced\n\n\n\n\n\n\n\n\nReproducible Research at the Cloud Era\n\n\nintermediate\n\n\n\n\n\n\n\n\nList of contributors\n\n\nThe material provided in this reporitory is benefiting from the contributions of the following persons:\n\n\n\n\nThe UL HPC Team of course, \ni.e.\n:\n\n\nDr. \nSebastien Varrette\n,  Research Scientist, \nUL HPC\n Manager\n\n\nValentin Plugaru, MSc, Research Collaborator\n\n\nSarah Peter (formally Sarah Diehl), MSc, Research Associate\n\n\nHyacinthe Cartiaux,\n\n\nCl\u00e9ment Parisot\n\n\n\n\n\n\nDr. Joseph Emeras, Research Associate\n\n\nDr. Xavier Besseron, Research Scientist\n\n\nDr. Aurelien Ginolhac, Research Scientist\n\n\nMaxime Schmitt\n\n\n\n\nIn the advent where you want to contribute yourself to these tutorials, do not hesitate! See below for instructions.\n\n\nIssues / Feature request\n\n\nYou can submit bug / issues / feature request using the \nULHPC/tutorials Tracker\n.\nSee also \ndocs/contributing/\n for further information.\n\n\nDevelopments / Contributing to the code\n\n\nIf you want to contribute to the code, you shall be aware of the way this module is organized.\nThese elements are detailed on \ndocs/contributing.md\n.\n\n\nYou are more than welcome to contribute to its development by \nsending a pull request\n.\n\n\nOnline Documentation\n\n\nRead the Docs\n aka RTFD hosts documentation for the open source community and the \nULHPC/sysadmins\n has its documentation (see the \ndocs/\n directly) hosted on \nreadthedocs\n.\n\n\nSee \ndocs/rtfd.md\n for more details.\n\n\nLicence\n\n\nThis project and the sources proposed within this repository are released under the terms of the \nGPL-3.0\n licence.",
            "title": "Home"
        },
        {
            "location": "/#ul-hpc-tutorials",
            "text": "Copyright (c) 2015-2017 UL HPC Team  hpc-sysadmins@uni.lu   This repository holds a set of tutorials to help the users of the  UL HPC  platform to better understand or simply use our platform.\nThe list of proposed tutorials are evolving and are used on a regular basis during the  UL HPC School  we organise.\nSo far, the following tutorials are proposed:     Category  Description  Level      Basic  Getting Started on the UL HPC platform  beginners     HPC workflow with sequential jobs  beginners     Know Your Bugs: Weapons for Efficient Debugging  intermediate    Software Management  Building [custom] software with EasyBuild  beginners    MPI  UL HPC MPI Tutorial: OSU Micro-Benchmarks  intermediate     High-Performance Linpack (HPL) benchmarking on UL HPC platform  intermediate     HPCG benchmarking on UL HPC platform  intermediate    Mathematics  MATLAB execution on the UL HPC platform  intermediate     R / Statictical Computing  intermediate    Bioinformatics  Bioinformatics software on the UL HPC platform  intermediate     Galaxy Introduction Exercise: From Peaks to Genes  intermediate    Parallel Debuging  Unified profiling and debugging with Allinea  intermediate     Direct,  Reverse and parallel Memory debugging with TotalView  intermediate    Virtualization  Create and reproduce work environments using Vagrant  intermediate     Deploying virtual machines with Vm5k on Grid'5000  intermediate    CFD/MD/Chemistry  Running parallel software: test cases on CFD / MD / Chemistry applications  advanced    Big Data  Running Big Data Application using Apache Spark on UL HPC platform  intermediate    Misc  Advanced workflows on sequential jobs management  advanced     Advanced scheduling with SLURM  intermediate     [Advanced] Prototyping with Python  advanced     Reproducible Research at the Cloud Era  intermediate",
            "title": "UL HPC Tutorials"
        },
        {
            "location": "/#list-of-contributors",
            "text": "The material provided in this reporitory is benefiting from the contributions of the following persons:   The UL HPC Team of course,  i.e. :  Dr.  Sebastien Varrette ,  Research Scientist,  UL HPC  Manager  Valentin Plugaru, MSc, Research Collaborator  Sarah Peter (formally Sarah Diehl), MSc, Research Associate  Hyacinthe Cartiaux,  Cl\u00e9ment Parisot    Dr. Joseph Emeras, Research Associate  Dr. Xavier Besseron, Research Scientist  Dr. Aurelien Ginolhac, Research Scientist  Maxime Schmitt   In the advent where you want to contribute yourself to these tutorials, do not hesitate! See below for instructions.",
            "title": "List of contributors"
        },
        {
            "location": "/#issues-feature-request",
            "text": "You can submit bug / issues / feature request using the  ULHPC/tutorials Tracker .\nSee also  docs/contributing/  for further information.",
            "title": "Issues / Feature request"
        },
        {
            "location": "/#developments-contributing-to-the-code",
            "text": "If you want to contribute to the code, you shall be aware of the way this module is organized.\nThese elements are detailed on  docs/contributing.md .  You are more than welcome to contribute to its development by  sending a pull request .",
            "title": "Developments / Contributing to the code"
        },
        {
            "location": "/#online-documentation",
            "text": "Read the Docs  aka RTFD hosts documentation for the open source community and the  ULHPC/sysadmins  has its documentation (see the  docs/  directly) hosted on  readthedocs .  See  docs/rtfd.md  for more details.",
            "title": "Online Documentation"
        },
        {
            "location": "/#licence",
            "text": "This project and the sources proposed within this repository are released under the terms of the  GPL-3.0  licence.",
            "title": "Licence"
        },
        {
            "location": "/basic/getting_started/",
            "text": "-\n- mode: markdown;mode:visual-line;  fill-column: 80 -\n-\n\n\nCopyright (c) 2016-2017 UL HPC Team  -- see \nhttp://hpc.uni.lu\n\n\n\n\nGetting Started on the UL HPC platform\n\n\n \n \n \n \n \n \n\n\n\n\nThis tutorial will guide you through your first steps on the\n\nUL HPC platform\n.\n\n\nBefore proceeding:\n\n\n\n\nmake sure you have an account (if not, follow \nthis procedure\n), and an SSH client.\n\n\ntake a look at the \nquickstart guide\n\n\nensure you operate from a Linux / Mac environment. Most commands below assumes running in a Terminal in this context. If you're running Windows, you can use MobaXterm, Putty tools etc. as described \non this page\n yet it's probably better that you familiarize \"natively\" with Linux-based environment by having a Linux Virtual Machine (consider for that \nVirtualBox\n).\n\n\n\n\nFrom a general perspective, the \nSupport page\n describes how to get help during your UL HPC usage.\n\n\nConvention\n\n\nIn the below tutorial, you'll proposed terminal commands where the prompt is denoted by \n$>\n.\nM\nIn general, we will prefix to precise the execution context (\ni.e.\n your laptop, a cluster frontend or a node). Remember that \n#\n character is a comment. Example:\n\n\n    # This is a comment\n    $> hostname\n\n    (laptop)$> hostname         # executed from your personal laptop / workstation\n\n    (access-iris)$> hostname    # executed from access server of the Iris cluster\n\n\n\nPlatform overview.\n\n\nYou can find a brief overview of the platform with key characterization numbers \non this page\n.\n\n\nThe general organization of each cluster is depicted below:\n\n\n\n\nDetails on this organization can be found \nhere\n\n\nHands-On/SSH & UL HPC access\n\n\n\n\nAccess / SSH Tutorial\n\n\n\n\nThe way SSH handles the keys and the configuration files is illustrated in the following figure:\n\n\n\n\nIn order to be able to login to the clusters, you have sent us through the Account request form the \npublic key\n (i.e. \nid_rsa.pub\n or the \npublic key\n as saved by MobaXterm/PuttY) you initially generated, enabling us to configure the \n~/.ssh/authorized_keys\n file of your account.\n\n\nStep 1a: Connect to UL HPC (Linux / Mac OS / Unix)\n\n\nRun the following commands in a terminal (substituting \nyourlogin\n with the login name you received from us):\n\n\n    (laptop)$> ssh -p 8022 yourlogin@access-gaia.uni.lu\n\n\n\nIf you want to connect to the iris cluster,\n\n\n    (laptop)$> ssh -p 8022 yourlogin@access-iris.uni.lu\n\n\n\nNow you probably want to avoid taping this long command to connect to the platform. You can customize SSH aliases for that. Edit the file \n~/.ssh/config\n (create it if it does not already exist) and adding the following entries:\n\n\n    Host chaos-cluster\n        Hostname access-chaos.uni.lu\n\n    Host gaia-cluster\n        Hostname access-gaia.uni.lu\n\n    Host iris-cluster\n        Hostname access-iris.uni.lu\n\n    Host *-cluster\n        User yourlogin\n        Port 8022\n        ForwardAgent no\n\n\n\nNow you shall be able to issue the following (simpler) command to connect to the cluster and obtain the welcome banner:\n\n\n    (laptop)$> ssh gaia-cluster\n\n    (laptop)$> ssh iris-cluster\n\n\n\nIn the sequel, we assume these aliases to be defined.\n\n\nStep 1b: Connect to UL HPC (Windows)\n\n\n\n\nDownload \nMobaXterm Installer edition\n\n\nInstall MobaXterm\n\n\nOpen the application \nStart\n > \nProgram Files\n > \nMobaXterm\n\n\nChange the default home directory for a persistent home directory instead of the default Temp directory. Go onto \nSettings\n > \nConfiguration\n > \nGeneral\n > \nPersistent home directory\n. Choose a location for your home directory.\n\n\nload your private SSH key. \nTools\n > \nNetwork\n > \nMobaKeyGen (SSH key generator)\n and choose Load (or create a new RSA key).\n\n\nclick on \nSession\n\n\nIn \nSSH Session\n:\n\n\nRemote host: \naccess-iris.uni.lu\n\n\nCheck the \nSpecify username\n box\n\n\nUsername: \nyourlogin\n\n\n\n\n\n\nPort: 8022\n\n\n\n\n\n\nIn \nAdvanced SSH Settings\n\n\nCheck \nUse private key\n box\n\n\nSelect your previously generated \nid_rsa.ppk\n\n\n\n\n\n\n\n\n\n\nClick on \nSave\n\n\nDo the same thing for the other clusters (chaos, gaia) by changing the \nRemote host\n field.\n\n\n\n\n\n\n\n\nStep 2: Connect from one cluster to the other\n\n\nThe SSH key you provided us secure your connection \nfrom\n your laptop (or personal workstation) \nto\n the cluster frontends. It is thus important to protect them by a passphrase.\n\n\nYou shall have also a new key pair configured in your account to permit a bi-directional transparent connection from one cluster to the other (you can check that in your \n~/.ssh/authorized_keys\n and by successfully running:\n\n\n    (access-gaia)$> ssh chaos-cluster\n\n\n\nor\n\n\n    (access-chaos)$> ssh gaia-cluster\n\n\n\nIf that's the case, you can ignore the rest of this section.\n\nOtherwise\n, you will now have to configure a passphrase-free SSH key pair to permit a transparent connection from one cluster to another. Have a look at this \nFAQ\n\n\n\n\nIf you have some issue to connect to the clusters (for example \nConnection closed by remote host\n error message), you should check the section on how to \nuse SSH proxycommand setup to access the clusters despite port filtering\n\n\n\n\nHands-on/ Transferring files\n\n\nDirectories such as \n$HOME\n, \n$WORK\n or \n$SCRATCH\n are shared among the nodes of the cluster that you are using (including the front-end) via shared filesystems (NFS, Lustre) meaning that:\n\n\n\n\nevery file/directory pushed or created on the front-end is available on the computing nodes\n\n\nevery file/directory pushed or created on the computing nodes is available on the front-end\n\n\n\n\nStep 3a: Linux / OS X / Unix command line tools\n\n\nThe two most common tools you can use for data transfers over SSH:\n\n\n\n\nscp\n: for the full transfer of files and directories (only works fine for single files or directories of small/trivial size)\n\n\nrsync\n: a software application which synchronizes files and directories from one location to another while minimizing data transfer as only the outdated or inexistent elements are transferred (practically required for lengthy complex transfers, which are more likely to be interrupted in the middle).\n\n\n\n\nOf both, normally the second approach should be preferred, as more generic; note that, both ensure a secure transfer of the data, within an encrypted tunnel.\n\n\n\n\n\n\nCreate a new directory on your local machine and download a file to transfer (next-gen sequencing data from the NIH Roadmap Epigenomics Project):\n\n\n(laptop)$> mkdir file_transfer\n(laptop)$> cd file_transfer\n(laptop)$> wget \"ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM409nnn/GSM409307/suppl/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\"\n\n\n\n\n\n\n\nTransfer the file with scp:\n\n\n(laptop)$> scp GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz gaia-cluster:\n\n\n\n\n\n\n\nConnect to the cluster, check if the file is there and delete it.\n\n\n(laptop)$> ssh gaia-cluster\n(access-gaia)$> ls\n(access-gaia)$> rm GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\nrm: remove regular file `GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz'? y\n(access-gaia)$> exit\n\n\n\n\n\n\n\nTransfer the directory with rsync:\n\n\n(laptop)$> cd ..\n(laptop)$> rsync -avzu file_transfer gaia-cluster:\n\n\n\n\n\n\n\nDelete the file and retrieve it from the cluster:\n\n\n(laptop)$> rm file_transfer/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\n(laptop)$> rsync -avzu gaia-cluster:file_transfer .\n\n\n\n\n\n\n\nBonus\n: Check where the file is located on the cluster after the rsync.\n\n\n\n\n\n\nYou can get more information about these transfer methods in the \nfile transfer documentation\n.\n\n\nStep 3b: Windows / Linux / OS X / Unix GUI tools\n\n\n\n\nDownload the FileZilla client application from \nfilezilla-project.org\n and install it.\n\n\nFirst we need to tell FileZilla about our ssh key:\n\n\nStart the application.\n\n\nGo to the \nSettings\n (either under \nEdit\n or \nFileZilla\n depending on the OS).\n\n\nIn the category \nConnection\n select \nSFTP\n.\n\n\nClick on the button \nAdd keyfile...\n and select your private keyfile (you may need to convert it).\n\n\nFinally click \nOK\n to save and close the settings.\n\n\n\n\n\n\n\n\n\n\n\n\nBack in the main window click on the \nSite Manager\n button on the top left or select \nSite Manager\n from the \nFile\n menu.\n\n\nClick on the \nNew Site\n button and enter/select the following:\n\n\nHost: \naccess-gaia.uni.lu\n\n\nPort: 8022\n\n\n\n\n\n\nProtocol: \nSFTP - SSH File Transfer Protocol\n\n\nLogon Type: \nInteractive\n\n\nUser: your login\n\n\n\n\n\n\n\n\nClick on the \nConnect\n button.\n\n\nAccept the certificate.\n\n\n\n\nYou should now see something similar to the following window:\n\n\n\n\nOn the very top, beneath the quick connect, you see the message log. Below you have the directory tree and the contents of the current directory for you local computer on the left and the remote location on the right.\n\n\nTo transfer a file, simply drag and drop it from the directory listing on the left side to destination directory on the right (to transfer from local to remote) or vice versa (to transfer from remote to local). You can also select a file by left clicking on it once and then right click on it to get the context menu and select \"Upload\" or \"Download\" to transfer it.\n\n\nIf you skipped step 3a, you may download the following file (50 MB) for testing: \n\n\nftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM409nnn/GSM409307/suppl/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\n (next-gen sequencing data from the NIH Roadmap Epigenomics Project)\n\n\nWhen you click the fifth icon on the top with the two green arrows to toggle the transfer queue, you can see the status of ongoing transfers on the very bottom of the window.\n\n\n\n\nStep 3c: Windows MobaXterm file transfer\n\n\nIf you are on Windows, you can directly use MobaXterm to transfer files. Connect to your session (see below on how to configure it). On the right panel you should see an \nSFTP\n panel opened.\n\n\n\n\nYou have just to drag and drop your files to this panel to transfer files to the cluster. You can try to upload this file \nftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM409nnn/GSM409307/suppl/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\n (next-gen sequencing data from the NIH Roadmap Epigenomics Project)\n\n\nTo retrieve a file from the cluster, you can right click on it and choose the \nDownload\n option. Please refers to MobaXterm documentation for more informations on the available features.\n\n\nDiscovering, visualizing and reserving UL HPC resources\n\n\nIn the sequel, replace \n<login>\n in the proposed commands with you login on the platform (ex: \nsvarrette\n).\n\n\nStep 1: the working environment\n\n\n\n\nreference documentation\n\n\n\n\nAfter a successful login onto one of the access node (see \nCluster Access\n), you end into your personal homedir \n$HOME\n which is shared over NFS between the access node and the computing nodes.\n\n\nAgain, remember that your homedir is placed on \nseparate\n NFS servers on each site, which \nARE NOT SYNCHRONIZED\n: data synchronization between each of them remain at your own responsibility. We will see below that the UL HPC team prepared for you a script to facilitate the transfer of data between each site.\n\n\nOtherwise, you have to be aware of at least two directories:\n\n\n\n\n$HOME\n: your home directory under NFS.\n\n\n$SCRATCH\n: a non-backed up area put if possible under Lustre for fast I/O operations\n\n\n\n\nYour homedir is under a regular backup policy. Therefore you are asked to pay attention to your disk usage \nand\n the number of files you store there.\n\n\n\n\n\n\nEstimate file space usage and summarize disk usage of each FILE, recursively for directories using the \nncdu\n command:\n\n\n(access)$> ncdu\n\n\n\n\n\n\n\nYou shall also pay attention to the number of files in your home directory. You can count them as follows:\n\n\n(access)$> find . -type f | wc -l\n\n\n\n\n\n\n\nYou can get an overview of the quotas and your current disk usage with the following command:\n\n\n(access)$> df-ulhpc\n\n\n\n\n\n\n\nStep 2: web monitoring interfaces\n\n\nEach cluster offers a set of web services to monitor the platform usage:\n\n\n\n\nA \npie-chart overview of the platform usage\n\n\nMonika\n, the visualization interface of the OAR scheduler, which  display the status of the clusters as regards the jobs running on the platform.\n\n\nDrawGantt\n, the Gantt visualization of jobs scheduled on OAR\n\n\nGanglia\n, a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids.\n\n\n\n\nStep 3a: Reserving resources with Slurm\n\n\nThe basics\n\n\n\n\nreference documentation\n\n\n\n\nSlurm\n Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. It is used on Iris UL HPC cluster.\n\n\n\n\nIt allocates exclusive or non-exclusive access to the resources (compute nodes) to users during a limited amount of time so that they can perform they work\n\n\nIt provides a framework for starting, executing and monitoring work\n\n\nIt arbitrates contention for resources by managing a queue of pending work.\n\n\nit permits to schedule jobs for users on the cluster resource\n\n\n\n\nThere are two types of jobs:\n\n\n\n\ninteractive\n: you get a shell on the first reserve node\n\n\npassive\n: classical batch job where the script passed as argument to \nsbatch\n is executed\n\n\n\n\nWe will now see the basic commands of Slurm.\n\n\n\n\nConnect to \niris-cluster\n. You can request resources in interactive mode:\n(access)$> srun -p interactive --qos qos-interactive --pty bash\n\n\n\n\n\n\n\nNotice that with no other parameters, srun gave you one resource for 1 hour. You were also directly connected to the node you reserved with an interactive shell.\n  Now exit the reservation:\n\n\n    (node)$> exit      # or CTRL-D\n\n\n\nWhen you run exit, you are disconnected and your reservation is terminated.\n\n\nTo avoid anticipated termination of your jobs in case of errors (terminal closed by mistake),\nyou can reserve and connect in two steps using the job id associated to your reservation.\n\n\n\n\nFirst run a passive job \ni.e.\n run a predefined command -- here \nsleep 10d\n to delay the execution for 10 days -- on the first reserved node:\n(access)$> sbatch --qos qos-batch --wrap \"sleep 10d\"\nSubmitted batch job 390\n\n\n\n\n\n\n\nYou noticed that you received a job ID (in the above example: \n390\n), which you can later use to connect to the reserved resource(s):\n\n\n    (access)$> srun -p interactive --qos qos-interactive --jobid 390 --pty bash # adapt the job ID accordingly ;)\n    (node)$> ps aux | grep sleep\n    cparisot 186342  0.0  0.0 107896   604 ?        S    17:58   0:00 sleep 1h\n    cparisot 187197  0.0  0.0 112656   968 pts/0    S+   18:04   0:00 grep --color=auto sleep\n    (node)$> exit             # or CTRL-D\n\n\n\nQuestion: At which moment the job \n390\n will end?\n\n\na. after 10 days\n\n\nb. after 1 hour\n\n\nc. never, only when I'll delete the job\n\n\nQuestion: manipulate the \n$SLURM_*\n variables over the command-line to extract the following information, once connected to your job\n\n\na. the list of hostnames where a core is reserved (one per line)\n   * \nhint\n: \nman echo\n\n\nb. number of reserved cores\n   * \nhint\n: \nsearch for the NPROCS variable\n\n\nc. number of reserved nodes\n   * \nhint\n: \nsearch for the NNODES variable\n\n\nd. number of cores reserved per node together with the node name (one per line)\n   * Example of output:\n\n\n        12 iris-11\n        12 iris-15\n\n\n\n\n\nhint\n: \nNPROCS variable or NODELIST\n\n\n\n\nJob management\n\n\nNormally, the previously run job is still running.\n\n\n\n\nYou can check the status of your running jobs using \nsqueue\n command:\n(access)$> squeue      # access all jobs\n(access)$> squeue -u cparisot  # access all your jobs\n\n\n\n\n\n\n\nThen you can delete your job by running \nscancel\n command:\n\n\n    (access)$> scancel 390\n\n\n\n\n\nyou can see your system-level utilization (memory, I/O, energy) of a running job using \nsstat $jobid\n:\n(access)$> sstat 390\n\n\n\n\n\n\n\nIn all remaining examples of reservation in this section, remember to delete the reserved jobs afterwards (using \nscancel\n or \nCTRL-D\n)\n\n\nYou probably want to use more than one core, and you might want them for a different duration than one hour.\n\n\n\n\nReserve interactively 4 tasks with 2 nodes for 30 minutes (delete the job afterwards)\n(access)$> srun -p interactive --qos qos-interactive --time=0:30:0 -N 2 --ntasks-per-node=4 --pty bash\n\n\n\n\n\n\n\nPausing, resuming jobs\n\n\nTo stop a waiting job from being scheduled and later to allow it to be scheduled:\n\n\n    (access)$> scontrol hold $SLURM_JOB_ID\n    (access)$> scontrol release $SLURM_JOB_ID\n\n\n\nTo pause a running job and then resume it:\n\n\n    (access)$> scontrol suspend $SLURM_JOB_ID\n    (access)$> scontrol resume $SLURM_JOB_ID\n\n\n\nStep 3b: Reserving resources with OAR\n\n\nThe basics\n\n\n\n\nreference documentation\n\n\n\n\nOAR\n is an open-source batch scheduler which provides simple yet flexible facilities for the exploitation of the UL HPC clusters.\n\n\n\n\nit permits to schedule jobs for users on the cluster resource\n\n\na \nOAR resource\n corresponds to a node or part of it (CPU/core)\n\n\na \nOAR job\n is characterized by an execution time (walltime) on a set of resources.\n  There exists two types of jobs:\n\n\ninteractive\n: you get a shell on the first reserve node\n\n\npassive\n: classical batch job where the script passed as argument to \noarsub\n is executed \non the first reserved node\n\n\n\n\nWe will now see the basic commands of OAR.\n\n\n\n\nConnect to one of the UL HPC  frontend. You can request resources in interactive mode:\n(access)$> oarsub -I\n\n\n\n\n\n\n\nNotice that with no parameters, oarsub gave you one resource (one core) for two hours. You were also directly connected to the node you reserved with an interactive shell.\n  Now exit the reservation:\n\n\n    (node)$> exit      # or CTRL-D\n\n\n\nWhen you run exit, you are disconnected and your reservation is terminated.\n\n\nTo avoid anticipated termination of your jobs in case of errors (terminal closed by mistake),\nyou can reserve and connect in two steps using the job id associated to your reservation.\n\n\n\n\nFirst run a passive job \ni.e.\n run a predefined command -- here \nsleep 10d\n to delay the execution for 10 days -- on the first reserved node:\n(access)$> oarsub \"sleep 10d\"\n[ADMISSION RULE] Set default walltime to 7200.\n[ADMISSION RULE] Modify resource description with type constraints\nOAR_JOB_ID=919309\n\n\n\n\n\n\n\nYou noticed that you received a job ID (in the above example: \n919309\n), which you can later use to connect to the reserved resource(s):\n\n\n    (access)$> oarsub -C 919309        # adapt the job ID accordingly ;)\n    Connect to OAR job 919309 via the node e-cluster1-13\n    [OAR] OAR_JOB_ID=919309\n    [OAR] Your nodes are:\n        e-cluster1-13*1\n\n    (e-cluster1-13)$> java -version\n    (e-cluster1-13)$> hostname -f\n    (e-cluster1-13)$> whoami\n    (e-cluster1-13)$> env | grep OAR   # discover environment variables set by OAR\n    (e-cluster1-13)$> exit             # or CTRL-D\n\n\n\nQuestion: At which moment the job \n919309\n will end?\n\n\na. after 10 days\n\n\nb. after 2 hours\n\n\nc. never, only when I'll delete the job\n\n\nQuestion: manipulate the \n$OAR_NODEFILE\n variable over the command-line to extract the following information, once connected to your job\n\n\na. the list of hostnames where a core is reserved (one per line)\n   * \nhint\n: \nman cat\n\n\nb. number of reserved cores (one per line)\n   * \nhint\n: \nman wc\n --  use \nwc -l\n over the pipe \n|\n command\n\n\nc. number of reserved nodes (one per line)\n   * \nhint\n: \nman uniq\n -- use \nuniq\n over the pipe \n|\n command\n\n\nd. number of cores reserved per node together with the node name (one per line)\n   * Example of output:\n\n\n        12 gaia-11\n        12 gaia-15\n\n\n\n\n\nhint\n: \nman uniq\n -- use \nuniq -c\n over the pipe \n|\n command\n\n\n\n\ne. \n(for geeks)\n output the number of reserved nodes times number of cores per node\n   * Example of output:\n\n\n        gaia-11*12\n        gaia-15*12\n\n\n\n\n\nhint\n: \nman awk\n -- use \nprintf\n command of \nawk\n over the pipe command, for instance \nawk '{ printf \"%s*%d\\n\",$2,$1 }'\n. You might prefer \nsed\n or any other advanced geek command.\n\n\n\n\nJob management\n\n\nNormally, the previously run job is still running.\n\n\n\n\nYou can check the status of your running jobs using \noarstat\n command:\n(access)$> oarstat      # access all jobs\n(access)$> oarstat -u   # access all your jobs\n\n\n\n\n\n\n\nThen you can delete your job by running \noardel\n command:\n\n\n    (access)$> oardel 919309\n\n\n\n\n\nyou can see your consumption (in an historical computational measure named \nCPU hour\n i.e. the work done by a CPU in one hour of wall clock time) over a given time period using \noarstat --accounting \"YYYY-MM-DD, YYYY-MM-DD\" -u <youlogin>\n:\n(access)$> oarstat --accounting \"2016-01-01, 2016-12-31\" -u <login>\n\n\n\n\n\n\n\nIn particular, take a look at the difference between the \nasked\n resources and the \nused\n ones\n\n\nIn all remaining examples of reservation in this section, remember to delete the reserved jobs afterwards (using \noardel\n or \nCTRL-D\n)\n\n\nYou probably want to use more than one core, and you might want them for a different duration than two hours.\nThe \n-l\n switch allows you to pass a comma-separated list of parameters specifying the needed resources for the job.\n\n\n\n\n\n\nReserve interactively 4 cores for 6 hours (delete the job afterwards)\n\n\n(access)$> oarsub -I -l core=6,walltime=6\n\n\n\n\n\n\n\nReserve interactively 2 nodes for 3h15 (delete the job afterwards):\n\n\n(access)$> oarsub -I -l nodes=3,walltime=3:15\n\n\n\n\n\n\n\nHierarchical filtering of resources\n\n\nOAR features a very powerful resource filtering/matching engine able to specify resources in a \nhierarchical\n  way using the \n/\n delimiter. The resource property hierarchy is as follows:\n\n\n    enclosure -> nodes -> cpu -> core\n\n\n\n\n\n\n\nReserve interactively 2 cores on 3 different nodes belonging to the same enclosure (\ntotal: 6 cores\n) for 3h15:\n\n\n(access)$> oarsub -I -l /enclosure=1/nodes=3/core=2,walltime=3:15\n\n\n\n\n\n\n\nReserve interactively two full nodes belonging to the different enclosure for 6 hours:\n\n\n(access)$> oarsub -I -l /enclosure=2/nodes=1,walltime=6\n\n\n\n\n\n\n\nQuestion: reserve interactively 2 cpus on 2 nodes belonging to the same enclosure for 4 hours\n\n\nQuestion: in the following statements, explain the advantage and drawback (in terms of latency/bandwidth etc.) of each of the proposed approaches\n\n\na. \noarsub -I -l /nodes=2/cpu=1\n vs \noarsub -I -l cpu=2\n vs \noarsub -I -l /nodes=1/cpu=2\n\n\nb. \noarsub -I -l /enclosure=1/nodes=2\n vs \noarsub -I -l nodes=2\n vs \noarsub -I -l /enclosure=2/nodes=1\n\n\nUsing OAR properties\n\n\nYou might have notice on \nMonika\n for each site a list of properties assigned to each resource.\n\n\nThe \n-p\n switch allows you to specialize (as an SQL syntax) the property you wish to use when selecting the resources. The syntax is as follows: \noarsub -p \"< property >='< value >'\"\n\n\nYou can find the available OAR properties on the \nUL HPC documentation\n. The main ones are described below\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nFull hostname of the resource\n\n\n-p \"host='h-cluster1-14.chaos-cluster.uni.lux'\"\n\n\n\n\n\n\nnetwork_address\n\n\nShort hostname of the resource\n\n\n-p \"network_address='h-cluster1-14'\"\n\n\n\n\n\n\ngpu\n\n\nGPU availability (gaia only)\n\n\n-p \"gpu='YES'\"\n\n\n\n\n\n\n\n\n\n\n\n\nreserve interactively 4 cores on a GPU node for 8 hours (\nthis holds only on the \ngaia\n cluster\n) (\ntotal: 4 cores\n)\n\n\n(access-gaia)$> oarsub -I -l nodes=1/core=4,walltime=8 -p \"gpu='YES'\"\n\n\n\n\n\n\n\nreserve interactively 4 cores on the GPU node \ngaia-65\n for 8 hours (\nthis holds only on the \ngaia\n cluster\n) (\ntotal: 4 cores\n)\n\n\n(access-gaia)$> oarsub -I -l nodes=1/core=4,walltime=8 -p \"gpu='yes'\" -p \"network_address='gaia-65'\"\n\n\n\n\n\n\n\nReserving specific resources \nbigsmp\nand \nbigmem\n\n\nSome nodes are very specific (for instance the nodes with 1TB of memory or the BCS subsystem of Gaia composed of 4 motherboards of 4 processors with a total of 160 cores aggregated in a ccNUMA architecture).\n\nDue to this specificity, they are NOT scheduled by default\n  and can only be reserved with an explicit oarsub parameter: \n-t bigmem\n or \n-t bigsmp\n\n\n\n\nreserve interactively 2 cpu on the bigsmp node belonging to the same board for 3 hours: (\ntotal: 32 cores\n)\n(access-gaia)$> oarsub -t bigsmp -I -l /board=1/cpu=2,walltime=3\n\n\n\n\n\n\n\nQuestion: why are these resources not scheduled by default?\n\n\nReservation at a given period of time\n\n\nYou can use the \n-r \"YYYY-MM-DD HH:MM:SS\"\n option of \noarsub\n to specify the date you wish the reservation to be issued. This is of particular interest for you to book in advance resources out of the working hours (at night and/or over week ends)\n\n\nHands-on/Using modules\n\n\nEnvironment Modules\n is a software package that allows us to provide a \nmultitude of applications and libraries in multiple versions\n on the UL HPC platform. The tool itself is used to manage environment variables such as \nPATH\n, \nLD_LIBRARY_PATH\n and \nMANPATH\n, enabling the easy loading and unloading of application/library profiles and their dependencies.\n\n\nWe will have multiple occasion to use modules in the other tutorials so there is nothing special we foresee here. You are just encouraged to read the following resources:\n\n\n\n\nIntroduction to Environment Modules by Wolfgang Baumann\n\n\nModules tutorial @ NERSC\n\n\nUL HPC documentation on modules\n\n\n\n\nHands-on/Persistent Terminal Sessions using GNU Screen\n\n\nGNU Screen\n is a tool to manage persistent terminal sessions.\nIt becomes interesting since you will probably end at some moment with the following  scenario:\n\n\n\n\nyou frequently program and run computations on the UL HPC platform \ni.e\n on a remote Linux/Unix computer, typically working in six different terminal logins to the access server from your office workstation, cranking up long-running computations that are still not finished and are outputting important information (calculation status or results), when you have not 2 interactive jobs running... But it's time to catch the bus and/or the train to go back home.\n\n\n\n\nProbably what you do in the above scenario is to\n\n\na. clear and shutdown all running terminal sessions\n\n\nb. once at home when the kids are in bed, you're logging in again... And have to set up the whole environment again (six logins, 2 interactive jobs etc. )\n\n\nc. repeat the following morning when you come back to the office.\n\n\nEnter the long-existing and very simple, but totally indispensable \nGNU screen\n command. It has the ability to completely detach running processes from one terminal and reattach it intact (later) from a different terminal login.\n\n\nPre-requisite: screen configuration file \n~/.screenrc\n\n\nWhile not mandatory, we advise you to rely on our customized configuration file for screen \n.screenrc\n available on \nGithub\n.\nNormally, you have nothing to do since we already setup this file for you in your homedir.\nOtherwise, simply clone the \nULHPC dotfile repository\n and make a symbolic link \n~/.screenrc\n targeting the file \nscreen/screenrc\n of the repository.\n\n\nBasic commands\n\n\nYou can start a screen session (\ni.e.\n creates a single window with a shell in it) with the \nscreen\n command.\nIts main command-lines options are listed below:\n\n\n\n\nscreen\n: start a new screen\n\n\nscreen -ls\n: does not start screen, but prints a list of \npid.tty.host\n strings identifying your current screen sessions.\n\n\nscreen -r\n: resumes a detached screen session\n\n\nscreen -x\n: attach to a not detached screen session. (Multi display mode \ni.e.\n when you and another user are trying to access the same session at the same time)\n\n\n\n\nOnce within a screen, you can invoke a screen command which consist of a \"\nCTRL + a\n\" sequence followed by one other character. The main commands are:\n\n\n\n\nCTRL + a c\n: (create) creates a new Screen window. The default Screen number is zero.\n\n\nCTRL + a n\n: (next) switches to the next window.\n\n\nCTRL + a p\n: (prev) switches to the previous window.\n\n\nCTRL + a d\n: (detach) detaches from a Screen\n\n\nCTRL + a A\n: (title) rename the current window\n\n\nCTRL + a 0-9\n: switches between windows 0 through 9.\n\n\nCTRL + a k\n or \nCTRL + d\n: (kill) destroy the current window\n\n\nCTRL + a ?\n: (help) display a list of all the command options available for Screen.\n\n\n\n\nSample Usage on the UL HPC platform: Kernel compilation\n\n\nWe will illustrate the usage of GNU screen by performing a compilation of a recent linux kernel.\n\n\n\n\n\n\nstart a new screen session\n\n\n(access)$> screen\n\n\n\n\n\n\n\nrename the screen window \"Frontend\" (using \nCTRL+a A\n)\n\n\n\n\n\n\ncreate the directory to host the files\n\n\n(access)$> mkdir -p PS1/src\n(access)$> cd PS1/src\n\n\n\n\n\n\n\ncreate a new window and rename it \"Compile\"\n\n\n\n\n\n\nwithin this new window, start a new interactive job over 1 nodes for 4 hours\n\n\n(access)$> srun -p interactive --qos qos-interactive --time 4:00:0 -N 1 --pty bash\n\n\n\n\n\n\n\ndetach from this screen (using \nCTRL+a d\n)\n\n\n\n\nkill your current SSH connection and your terminal\n\n\nre-open your terminal and connect back to the cluster frontend\n\n\n\n\nlist your running screens:\n\n\n(access)$> screen -ls\nThere is a screen on:\n    9143.pts-0.access   (05/04/2014 11:29:43 PM) (Detached)\n1 Socket in /var/run/screen/S-svarrette.\n\n\n\n\n\n\n\nre-attach your previous screen session\n\n\n(access)$> screen -r      # OR screen -r 9143.pts-0.access (see above socket name)\n\n\n\n\n\n\n\nin the \"Compile\" windows, go to the working directory and download the Linux kernel sources\n\n\n(node)$> cd PS1/src\n(node)$> curl -O https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.13.6.tar.gz\n\n\n\n\n\n\n\nIMPORTANT\n to ovoid overloading the \nshared\n file system with the many small files involves in the kernel compilation (\ni.e.\n NFS and/or Lustre), we will perform the compilation in the \nlocal\n file system, \ni.e.\n either in \n/tmp\n or (probably more efficient) in \n/dev/shm\n (\ni.e\n in the RAM):\n\n\n    (node)$> mkdir /dev/shm/PS1\n    (node)$> cd /dev/shm/PS1\n    (node)$> tar xzf PS1/src/linux-3.13.6.tar.gz\n    (node)$> cd linux-3.13.6\n    (node)$> make mrproper\n    (node)$> make alldefconfig\n    (node)$> make 2>&1 | tee /dev/shm/PS1/kernel_compile.log\n\n\n\n\n\nYou can now detach from the screen and take a coffee\n\n\n\n\nThe last compilation command make use of \ntee\n, a nice tool which read from standard input and write to standard output \nand\n files. This permits to save in a log file the message written in the standard output.\n\n\nQuestion: why using the \nmake 2>&1\n sequence in the last command?\n\n\nQuestion: why working in \n/dev/shm\n is more efficient?\n\n\n\n\nReattach from time to time to your screen to see the status of the compilation\n\n\n\n\nYour compilation is successful if it ends with the sequence:\n\n\n[...]\nKernel: arch/x86/boot/bzImage is ready  (#2)\n\n\n\n\n\n\n\nRestart the compilation, this time using parallel jobs within the Makefile invocation (\n-j\n option of make)\n\n\n(node)$> make clean\n(node)$> time make -j `echo $SLURM_NPROCS` 2>&1 | tee /dev/shm/PS1/kernel_compile.2.log\n\n\n\n\n\n\n\nThe table below should convince you to always run \nmake\n with the \n-j\n option whenever you can...\n\n\n\n\n\n\n\n\nContext\n\n\ntime (\nmake\n)\n\n\ntime (\nmake -j 16\n)\n\n\n\n\n\n\n\n\n\n\nCompilation in \n/tmp\n(HDD / chaos)\n\n\n4m6.656s\n\n\n0m22.981s\n\n\n\n\n\n\nCompilation in \n/tmp\n(SSD / gaia)\n\n\n3m52.895s\n\n\n0m17.508s\n\n\n\n\n\n\nCompilation in \n/dev/shm\n (RAM)\n\n\n3m11.649s\n\n\n0m17.990s\n\n\n\n\n\n\n\n\n\n\nUse the \nGanglia\n interface to monitor the impact of the compilation process on the node your job is running on.\n\n\n\n\nUse the following system commands on the node during the compilation:\n\n\n\n\n\n\nhtop\n\n\n\n\ntop\n\n\nfree -m\n\n\nuptime\n\n\nps aux\n\n\n\n\nUsing a command line text editor\n\n\nBefore the next section, you must learn to use a text editor in command line.\nWe can recommend \nnano\n or \nvim\n: \nnano\n is very simple, \nvim\n is complex but very powerful.\n\n\nNano\n\n\n$ nano <path/filename>\n\n\n\n\nquit and save: \nCTRL+x\n\n\nsave: \nCTRL+o\n\n\nhighlight text: \nAlt-a\n\n\nCut the highlighted text: \nCTRL+k\n\n\nPaste: \nCTRL+u\n\n\n\n\nVim\n\n\nvim <path/filename>\n\n\nThere are 2 main modes:\n\n\n\n\nEdition mode: press \ni\n or \ninsert\n once\n\n\nCommand mode: press \nESC\n once\n\n\n\n\nHere is a short list of useful commands:\n\n\n\n\nsave: \n:w\n\n\nsave and quit: \n:wq\n\n\nquit and discard changes: \n:q!\n\n\nsearch: \n/<pattern>\n\n\nsearch & replace: \n:%s/<pattern>/<replacement>/g\n\n\njump to line 100: \n:100\n\n\nhighlight text: \nCTRL+V\n\n\ncut the highlighted text: \nd\n\n\ncut one line: \ndd\n\n\npaste: \np\n\n\nundo: \nu\n\n\n\n\nAdvanced section\n\n\nUsing software modules\n\n\nThe UL HPC provides \nenvironment modules\n with the module command\nto manage the user environment, e.g. changing the environment variables.\n\n\nBy loading appropriate environment modules, the user can select:\n\n\n\n\ncompilers,\n\n\nlibraries, e.g. the MPI library, or\n\n\nother third party software packages.\n\n\n\n\nAn exhaustive list of the available software is proposed \nin this page\n.\n\n\nOn a node, using an interactive jobs, you can:\n\n\n\n\nlist all available softwares: \nmodule avail\n\n\nsearch for one software: \nmodule spider <search terms>\n\n\n\"load\" a software in your environment: \nmodule load <module name>\n\n\nlist the currently loaded modules: \nmodule list\n\n\nclean your environment, unload everything: \nmodule purge\n\n\n\n\nMatlab\n\n\n\n\n\n\nCreate a file named \nfibonacci.m\n in your home directory, copy-paste the following code in this file.\n   This code will calculate the first N numbers of the Fibonacci sequence\n\n\nN=1000;\nfib=zeros(1,N);\nfib(1)=1;\nfib(2)=1;\nk=3;\nwhile k <= N\n  fib(k)=fib(k-2)+fib(k-1);\n  fprintf('%d\\n',fib(k));\n  pause(1);\n  k=k+1;\nend\n\n\n\n\n\n\n\nCreate a new interactive job\n\n\n\n\n\n\nLook for the \nmatlab\n module using the command \nmodule spider\n\n\n\n\n\n\nLoad the module \nbase/MATLAB\n using the command \nmodule load\n\n\n\n\n\n\nExecute the code using matlab\n\n\n(node)$> matlab -nojvm -nodisplay -nosplash < path/to/fibonacci.m\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\nCreate a file named \nfibonacci.R\n in your home directory, copy-paste the following code in this file.\n   This code will calculate the first N numbers of the Fibonacci sequence\n\n\nN <- 130\nfibvals <- numeric(N)\nfibvals[1] <- 1\nfibvals[2] <- 1\nfor (i in 3:N) {\n     fibvals[i] <- fibvals[i-1]+fibvals[i-2]\n     print( fibvals[i], digits=22)\n     Sys.sleep(1)\n}\n\n\n\n\n\n\n\nCreate a new interactive job\n\n\n\n\n\n\nLook for the \nR\n module using the command \nmodule spider\n\n\n\n\n\n\nLoad the module \nlang/R\n using the command \nmodule load\n\n\n\n\n\n\nExecute the code using R\n\n\n(node)$> Rscript path/to/fibonacci.R\n\n\n\n\n\n\n\nCompiling your code\n\n\nIn this section, we will learn to compile small \"hello world\" programs in different languages, using different compilers and toolchains.\n\n\nC\n\n\nCreate a new file called \nhelloworld.c\n, containing the source code of a simple \"Hello World\" program written in C.\n\n\n    #include<stdio.h>\n\n    int main()\n    {\n        printf(\"Hello, world!\");\n        return 0;\n    }\n\n\n\nFirst, compile the program using the \"FOSS\" toochain, containing the GNU C compiler\n\n\n    (node)$> module load toolchain/foss\n    (node)$> gcc helloworld.c -o helloworld\n\n\n\nThen, compile the program using the Intel toolchain, containing the ICC compiler\n\n\n    (node)$> module purge\n    (node)$> module load toolchain/intel\n    (node)$> icc helloworld.c -o helloworld\n\n\n\nIf you use Intel CPUs and ICC is available on the platform, it is advised to use ICC in order to produce optimized binaries and achieve better performance.\n\n\nC++\n\n\nQuestion:\n create a new file \nhelloworld.cpp\n containing the following C++ source code,\ncompile the following program, using GNU C++ compiler (\ng++\n command), and the Intel compiler (\nicpc\n command).\n\n\n    #include <iostream>\n\n    int main() {\n        std::cout << \"Hello, world!\" << std::endl;\n    }\n\n\n\nFortran\n\n\nQuestion:\n create a new file \nhelloworld.f\n containing the following source code,\ncompile the following program, using the GNU Fortran compiler (\ngfortran\n command), and ICC (\nifortran\n command).\n\n\n    program hello\n       print *, \"Hello, World!\"\n    end program hello\n\n\n\nBe careful, the 6 spaces at the beginning of each line are required\n\n\nMPI\n\n\nMPI is a programming interface that enables the communication between processes of a distributed memory system.\n\n\nWe will create a simple MPI program where the MPI process of rank 0 broadcasts an integer (42) to all the other processes.\nThen, each process prints its rank, the total number of processes and the value he received from the process 0.\n\n\nIn your home directory, create a file \nmpi_broadcast.c\n and copy the following source code:\n\n\n    #include <stdio.h>\n    #include <mpi.h>\n    #include <unistd.h>\n    #include <time.h> /* for the work function only */\n\n    int main (int argc, char *argv []) {\n           char hostname[257];\n           int size, rank;\n           int i, pid;\n           int bcast_value = 1;\n\n           gethostname(hostname, sizeof hostname);\n           MPI_Init(&argc, &argv);\n           MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n           MPI_Comm_size(MPI_COMM_WORLD, &size);\n           if (!rank) {\n                bcast_value = 42;\n           }\n           MPI_Bcast(&bcast_value,1 ,MPI_INT, 0, MPI_COMM_WORLD );\n           printf(\"%s\\t- %d - %d - %d\\n\", hostname, rank, size, bcast_value);\n           fflush(stdout);\n\n           MPI_Barrier(MPI_COMM_WORLD);\n           MPI_Finalize();\n           return 0;\n    }\n\n\n\nReserve 2 cores on two distinct node with OAR\n\n\n    (access-gaia)$> oarsub -I -l nodes=2/core=1\n\n\n\nor with Slurm\n\n\n    (access-iris)$> srun -p interactive --qos qos-interactive --time 1:00:0 -N 2 -n 2 --pty bash\n\n\n\nLoad a toolchain and compile the code using \nmpicc\n\n\n    (node)$> mpicc mpi_broadcast.c -o mpi_broadcast -lpthread\n\n\n\nIf you use OAR, execute your mpi program using \nmpirun\n.\nNote that the \n-n\n parameter of mpirun is the number of processes, which should be equal to the number of reserved cpu cores most of the time.\n\n\n    (node)$> OAR_NTASKS=$(cat $OAR_NODEFILE | wc)\n    (node)$> mpirun -n $OAR_NTASKS -hostfile $OAR_NODEFILE ~/mpi_broadcast\n\n\n\nIf you use Slurm, you can use the \nsrun\n command. Create an interactive job, with 2 nodes (\n-N 2\n), and at least 2 tasks (\n-n 2\n).\n\n\n    (node)$> srun -n $SLURM_NTASKS ~/mpi_broadcast\n\n\n\nUsing SSH proxycommand setup to access the clusters despite port filtering\n\n\nIt might happen that the port 8022 is filtered from your working place. You can easily bypass this firewall rule using an SSH proxycommand to setup transparently multi-hop connexions \nthrough\n one host (a gateway) to get to the access frontend of the cluster, as depited below:\n\n\n[laptop] -----||--------> 22 [SSH gateway] ---------> 8022 [access-{chaos,gaia}]\n           firewall\n\n\n\nThe gateway can be any SSH server which have access to the access frontend of the cluster. The \nGforge @ UL\n is typically used in this context but you can prefer any other alternative (your personal NAS @ home etc.). Then alter the SSH config on your laptop (in \n~/.ssh/config\n typically) as follows:\n\n\n\n\ncreate an entry to be able to connect to the gateway:\n\n\n\n\nAlias for the gateway (not really needed, but convenient), below instantiated\n\n\nHost gw\nUser anotherlogin\nHostname host.domain.org\nForwardAgent no\n\n\n\nAutomatic connection to UL HPC from the outside via the gateway\n\n\nHost *.ulhpc\nProxyCommand ssh gw \"nc -q 0 `basename %h .ulhpc` %p\"\n\n\n\nEnsure you can connect to the gateway:\n\n\n(laptop)$> ssh gw\n(gateway)$> exit # or CTRL-D\n\n\n\nThe \n.ulhpc\n suffix we mentioned in the previous configuration is an arbitrary suffix you will now specify in your command lines in order to access the UL HPC platform via the gateway as follows:\n\n\n(laptop)$> ssh gaia.ulhpc",
            "title": "Getting Started"
        },
        {
            "location": "/basic/getting_started/#getting-started-on-the-ul-hpc-platform",
            "text": "This tutorial will guide you through your first steps on the UL HPC platform .  Before proceeding:   make sure you have an account (if not, follow  this procedure ), and an SSH client.  take a look at the  quickstart guide  ensure you operate from a Linux / Mac environment. Most commands below assumes running in a Terminal in this context. If you're running Windows, you can use MobaXterm, Putty tools etc. as described  on this page  yet it's probably better that you familiarize \"natively\" with Linux-based environment by having a Linux Virtual Machine (consider for that  VirtualBox ).   From a general perspective, the  Support page  describes how to get help during your UL HPC usage.  Convention  In the below tutorial, you'll proposed terminal commands where the prompt is denoted by  $> .\nM\nIn general, we will prefix to precise the execution context ( i.e.  your laptop, a cluster frontend or a node). Remember that  #  character is a comment. Example:      # This is a comment\n    $> hostname\n\n    (laptop)$> hostname         # executed from your personal laptop / workstation\n\n    (access-iris)$> hostname    # executed from access server of the Iris cluster",
            "title": "Getting Started on the UL HPC platform"
        },
        {
            "location": "/basic/getting_started/#platform-overview",
            "text": "You can find a brief overview of the platform with key characterization numbers  on this page .  The general organization of each cluster is depicted below:   Details on this organization can be found  here",
            "title": "Platform overview."
        },
        {
            "location": "/basic/getting_started/#hands-onssh-ul-hpc-access",
            "text": "Access / SSH Tutorial   The way SSH handles the keys and the configuration files is illustrated in the following figure:   In order to be able to login to the clusters, you have sent us through the Account request form the  public key  (i.e.  id_rsa.pub  or the  public key  as saved by MobaXterm/PuttY) you initially generated, enabling us to configure the  ~/.ssh/authorized_keys  file of your account.",
            "title": "Hands-On/SSH &amp; UL HPC access"
        },
        {
            "location": "/basic/getting_started/#step-1a-connect-to-ul-hpc-linux-mac-os-unix",
            "text": "Run the following commands in a terminal (substituting  yourlogin  with the login name you received from us):      (laptop)$> ssh -p 8022 yourlogin@access-gaia.uni.lu  If you want to connect to the iris cluster,      (laptop)$> ssh -p 8022 yourlogin@access-iris.uni.lu  Now you probably want to avoid taping this long command to connect to the platform. You can customize SSH aliases for that. Edit the file  ~/.ssh/config  (create it if it does not already exist) and adding the following entries:      Host chaos-cluster\n        Hostname access-chaos.uni.lu\n\n    Host gaia-cluster\n        Hostname access-gaia.uni.lu\n\n    Host iris-cluster\n        Hostname access-iris.uni.lu\n\n    Host *-cluster\n        User yourlogin\n        Port 8022\n        ForwardAgent no  Now you shall be able to issue the following (simpler) command to connect to the cluster and obtain the welcome banner:      (laptop)$> ssh gaia-cluster\n\n    (laptop)$> ssh iris-cluster  In the sequel, we assume these aliases to be defined.",
            "title": "Step 1a: Connect to UL HPC (Linux / Mac OS / Unix)"
        },
        {
            "location": "/basic/getting_started/#step-1b-connect-to-ul-hpc-windows",
            "text": "Download  MobaXterm Installer edition  Install MobaXterm  Open the application  Start  >  Program Files  >  MobaXterm  Change the default home directory for a persistent home directory instead of the default Temp directory. Go onto  Settings  >  Configuration  >  General  >  Persistent home directory . Choose a location for your home directory.  load your private SSH key.  Tools  >  Network  >  MobaKeyGen (SSH key generator)  and choose Load (or create a new RSA key).  click on  Session  In  SSH Session :  Remote host:  access-iris.uni.lu  Check the  Specify username  box  Username:  yourlogin    Port: 8022    In  Advanced SSH Settings  Check  Use private key  box  Select your previously generated  id_rsa.ppk      Click on  Save  Do the same thing for the other clusters (chaos, gaia) by changing the  Remote host  field.",
            "title": "Step 1b: Connect to UL HPC (Windows)"
        },
        {
            "location": "/basic/getting_started/#step-2-connect-from-one-cluster-to-the-other",
            "text": "The SSH key you provided us secure your connection  from  your laptop (or personal workstation)  to  the cluster frontends. It is thus important to protect them by a passphrase.  You shall have also a new key pair configured in your account to permit a bi-directional transparent connection from one cluster to the other (you can check that in your  ~/.ssh/authorized_keys  and by successfully running:      (access-gaia)$> ssh chaos-cluster  or      (access-chaos)$> ssh gaia-cluster  If that's the case, you can ignore the rest of this section. Otherwise , you will now have to configure a passphrase-free SSH key pair to permit a transparent connection from one cluster to another. Have a look at this  FAQ   If you have some issue to connect to the clusters (for example  Connection closed by remote host  error message), you should check the section on how to  use SSH proxycommand setup to access the clusters despite port filtering",
            "title": "Step 2: Connect from one cluster to the other"
        },
        {
            "location": "/basic/getting_started/#hands-on-transferring-files",
            "text": "Directories such as  $HOME ,  $WORK  or  $SCRATCH  are shared among the nodes of the cluster that you are using (including the front-end) via shared filesystems (NFS, Lustre) meaning that:   every file/directory pushed or created on the front-end is available on the computing nodes  every file/directory pushed or created on the computing nodes is available on the front-end",
            "title": "Hands-on/ Transferring files"
        },
        {
            "location": "/basic/getting_started/#step-3a-linux-os-x-unix-command-line-tools",
            "text": "The two most common tools you can use for data transfers over SSH:   scp : for the full transfer of files and directories (only works fine for single files or directories of small/trivial size)  rsync : a software application which synchronizes files and directories from one location to another while minimizing data transfer as only the outdated or inexistent elements are transferred (practically required for lengthy complex transfers, which are more likely to be interrupted in the middle).   Of both, normally the second approach should be preferred, as more generic; note that, both ensure a secure transfer of the data, within an encrypted tunnel.    Create a new directory on your local machine and download a file to transfer (next-gen sequencing data from the NIH Roadmap Epigenomics Project):  (laptop)$> mkdir file_transfer\n(laptop)$> cd file_transfer\n(laptop)$> wget \"ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM409nnn/GSM409307/suppl/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\"    Transfer the file with scp:  (laptop)$> scp GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz gaia-cluster:    Connect to the cluster, check if the file is there and delete it.  (laptop)$> ssh gaia-cluster\n(access-gaia)$> ls\n(access-gaia)$> rm GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\nrm: remove regular file `GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz'? y\n(access-gaia)$> exit    Transfer the directory with rsync:  (laptop)$> cd ..\n(laptop)$> rsync -avzu file_transfer gaia-cluster:    Delete the file and retrieve it from the cluster:  (laptop)$> rm file_transfer/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\n(laptop)$> rsync -avzu gaia-cluster:file_transfer .    Bonus : Check where the file is located on the cluster after the rsync.    You can get more information about these transfer methods in the  file transfer documentation .",
            "title": "Step 3a: Linux / OS X / Unix command line tools"
        },
        {
            "location": "/basic/getting_started/#step-3b-windows-linux-os-x-unix-gui-tools",
            "text": "Download the FileZilla client application from  filezilla-project.org  and install it.  First we need to tell FileZilla about our ssh key:  Start the application.  Go to the  Settings  (either under  Edit  or  FileZilla  depending on the OS).  In the category  Connection  select  SFTP .  Click on the button  Add keyfile...  and select your private keyfile (you may need to convert it).  Finally click  OK  to save and close the settings.       Back in the main window click on the  Site Manager  button on the top left or select  Site Manager  from the  File  menu.  Click on the  New Site  button and enter/select the following:  Host:  access-gaia.uni.lu  Port: 8022    Protocol:  SFTP - SSH File Transfer Protocol  Logon Type:  Interactive  User: your login     Click on the  Connect  button.  Accept the certificate.   You should now see something similar to the following window:   On the very top, beneath the quick connect, you see the message log. Below you have the directory tree and the contents of the current directory for you local computer on the left and the remote location on the right.  To transfer a file, simply drag and drop it from the directory listing on the left side to destination directory on the right (to transfer from local to remote) or vice versa (to transfer from remote to local). You can also select a file by left clicking on it once and then right click on it to get the context menu and select \"Upload\" or \"Download\" to transfer it.  If you skipped step 3a, you may download the following file (50 MB) for testing:   ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM409nnn/GSM409307/suppl/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz  (next-gen sequencing data from the NIH Roadmap Epigenomics Project)  When you click the fifth icon on the top with the two green arrows to toggle the transfer queue, you can see the status of ongoing transfers on the very bottom of the window.",
            "title": "Step 3b: Windows / Linux / OS X / Unix GUI tools"
        },
        {
            "location": "/basic/getting_started/#step-3c-windows-mobaxterm-file-transfer",
            "text": "If you are on Windows, you can directly use MobaXterm to transfer files. Connect to your session (see below on how to configure it). On the right panel you should see an  SFTP  panel opened.   You have just to drag and drop your files to this panel to transfer files to the cluster. You can try to upload this file  ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM409nnn/GSM409307/suppl/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz  (next-gen sequencing data from the NIH Roadmap Epigenomics Project)  To retrieve a file from the cluster, you can right click on it and choose the  Download  option. Please refers to MobaXterm documentation for more informations on the available features.",
            "title": "Step 3c: Windows MobaXterm file transfer"
        },
        {
            "location": "/basic/getting_started/#discovering-visualizing-and-reserving-ul-hpc-resources",
            "text": "In the sequel, replace  <login>  in the proposed commands with you login on the platform (ex:  svarrette ).",
            "title": "Discovering, visualizing and reserving UL HPC resources"
        },
        {
            "location": "/basic/getting_started/#step-1-the-working-environment",
            "text": "reference documentation   After a successful login onto one of the access node (see  Cluster Access ), you end into your personal homedir  $HOME  which is shared over NFS between the access node and the computing nodes.  Again, remember that your homedir is placed on  separate  NFS servers on each site, which  ARE NOT SYNCHRONIZED : data synchronization between each of them remain at your own responsibility. We will see below that the UL HPC team prepared for you a script to facilitate the transfer of data between each site.  Otherwise, you have to be aware of at least two directories:   $HOME : your home directory under NFS.  $SCRATCH : a non-backed up area put if possible under Lustre for fast I/O operations   Your homedir is under a regular backup policy. Therefore you are asked to pay attention to your disk usage  and  the number of files you store there.    Estimate file space usage and summarize disk usage of each FILE, recursively for directories using the  ncdu  command:  (access)$> ncdu    You shall also pay attention to the number of files in your home directory. You can count them as follows:  (access)$> find . -type f | wc -l    You can get an overview of the quotas and your current disk usage with the following command:  (access)$> df-ulhpc",
            "title": "Step 1: the working environment"
        },
        {
            "location": "/basic/getting_started/#step-2-web-monitoring-interfaces",
            "text": "Each cluster offers a set of web services to monitor the platform usage:   A  pie-chart overview of the platform usage  Monika , the visualization interface of the OAR scheduler, which  display the status of the clusters as regards the jobs running on the platform.  DrawGantt , the Gantt visualization of jobs scheduled on OAR  Ganglia , a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids.",
            "title": "Step 2: web monitoring interfaces"
        },
        {
            "location": "/basic/getting_started/#step-3a-reserving-resources-with-slurm",
            "text": "",
            "title": "Step 3a: Reserving resources with Slurm"
        },
        {
            "location": "/basic/getting_started/#the-basics",
            "text": "reference documentation   Slurm  Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. It is used on Iris UL HPC cluster.   It allocates exclusive or non-exclusive access to the resources (compute nodes) to users during a limited amount of time so that they can perform they work  It provides a framework for starting, executing and monitoring work  It arbitrates contention for resources by managing a queue of pending work.  it permits to schedule jobs for users on the cluster resource   There are two types of jobs:   interactive : you get a shell on the first reserve node  passive : classical batch job where the script passed as argument to  sbatch  is executed   We will now see the basic commands of Slurm.   Connect to  iris-cluster . You can request resources in interactive mode: (access)$> srun -p interactive --qos qos-interactive --pty bash    Notice that with no other parameters, srun gave you one resource for 1 hour. You were also directly connected to the node you reserved with an interactive shell.\n  Now exit the reservation:      (node)$> exit      # or CTRL-D  When you run exit, you are disconnected and your reservation is terminated.  To avoid anticipated termination of your jobs in case of errors (terminal closed by mistake),\nyou can reserve and connect in two steps using the job id associated to your reservation.   First run a passive job  i.e.  run a predefined command -- here  sleep 10d  to delay the execution for 10 days -- on the first reserved node: (access)$> sbatch --qos qos-batch --wrap \"sleep 10d\"\nSubmitted batch job 390    You noticed that you received a job ID (in the above example:  390 ), which you can later use to connect to the reserved resource(s):      (access)$> srun -p interactive --qos qos-interactive --jobid 390 --pty bash # adapt the job ID accordingly ;)\n    (node)$> ps aux | grep sleep\n    cparisot 186342  0.0  0.0 107896   604 ?        S    17:58   0:00 sleep 1h\n    cparisot 187197  0.0  0.0 112656   968 pts/0    S+   18:04   0:00 grep --color=auto sleep\n    (node)$> exit             # or CTRL-D  Question: At which moment the job  390  will end?  a. after 10 days  b. after 1 hour  c. never, only when I'll delete the job  Question: manipulate the  $SLURM_*  variables over the command-line to extract the following information, once connected to your job  a. the list of hostnames where a core is reserved (one per line)\n   *  hint :  man echo  b. number of reserved cores\n   *  hint :  search for the NPROCS variable  c. number of reserved nodes\n   *  hint :  search for the NNODES variable  d. number of cores reserved per node together with the node name (one per line)\n   * Example of output:          12 iris-11\n        12 iris-15   hint :  NPROCS variable or NODELIST",
            "title": "The basics"
        },
        {
            "location": "/basic/getting_started/#job-management",
            "text": "Normally, the previously run job is still running.   You can check the status of your running jobs using  squeue  command: (access)$> squeue      # access all jobs\n(access)$> squeue -u cparisot  # access all your jobs    Then you can delete your job by running  scancel  command:      (access)$> scancel 390   you can see your system-level utilization (memory, I/O, energy) of a running job using  sstat $jobid : (access)$> sstat 390    In all remaining examples of reservation in this section, remember to delete the reserved jobs afterwards (using  scancel  or  CTRL-D )  You probably want to use more than one core, and you might want them for a different duration than one hour.   Reserve interactively 4 tasks with 2 nodes for 30 minutes (delete the job afterwards) (access)$> srun -p interactive --qos qos-interactive --time=0:30:0 -N 2 --ntasks-per-node=4 --pty bash",
            "title": "Job management"
        },
        {
            "location": "/basic/getting_started/#pausing-resuming-jobs",
            "text": "To stop a waiting job from being scheduled and later to allow it to be scheduled:      (access)$> scontrol hold $SLURM_JOB_ID\n    (access)$> scontrol release $SLURM_JOB_ID  To pause a running job and then resume it:      (access)$> scontrol suspend $SLURM_JOB_ID\n    (access)$> scontrol resume $SLURM_JOB_ID",
            "title": "Pausing, resuming jobs"
        },
        {
            "location": "/basic/getting_started/#step-3b-reserving-resources-with-oar",
            "text": "",
            "title": "Step 3b: Reserving resources with OAR"
        },
        {
            "location": "/basic/getting_started/#the-basics_1",
            "text": "reference documentation   OAR  is an open-source batch scheduler which provides simple yet flexible facilities for the exploitation of the UL HPC clusters.   it permits to schedule jobs for users on the cluster resource  a  OAR resource  corresponds to a node or part of it (CPU/core)  a  OAR job  is characterized by an execution time (walltime) on a set of resources.\n  There exists two types of jobs:  interactive : you get a shell on the first reserve node  passive : classical batch job where the script passed as argument to  oarsub  is executed  on the first reserved node   We will now see the basic commands of OAR.   Connect to one of the UL HPC  frontend. You can request resources in interactive mode: (access)$> oarsub -I    Notice that with no parameters, oarsub gave you one resource (one core) for two hours. You were also directly connected to the node you reserved with an interactive shell.\n  Now exit the reservation:      (node)$> exit      # or CTRL-D  When you run exit, you are disconnected and your reservation is terminated.  To avoid anticipated termination of your jobs in case of errors (terminal closed by mistake),\nyou can reserve and connect in two steps using the job id associated to your reservation.   First run a passive job  i.e.  run a predefined command -- here  sleep 10d  to delay the execution for 10 days -- on the first reserved node: (access)$> oarsub \"sleep 10d\"\n[ADMISSION RULE] Set default walltime to 7200.\n[ADMISSION RULE] Modify resource description with type constraints\nOAR_JOB_ID=919309    You noticed that you received a job ID (in the above example:  919309 ), which you can later use to connect to the reserved resource(s):      (access)$> oarsub -C 919309        # adapt the job ID accordingly ;)\n    Connect to OAR job 919309 via the node e-cluster1-13\n    [OAR] OAR_JOB_ID=919309\n    [OAR] Your nodes are:\n        e-cluster1-13*1\n\n    (e-cluster1-13)$> java -version\n    (e-cluster1-13)$> hostname -f\n    (e-cluster1-13)$> whoami\n    (e-cluster1-13)$> env | grep OAR   # discover environment variables set by OAR\n    (e-cluster1-13)$> exit             # or CTRL-D  Question: At which moment the job  919309  will end?  a. after 10 days  b. after 2 hours  c. never, only when I'll delete the job  Question: manipulate the  $OAR_NODEFILE  variable over the command-line to extract the following information, once connected to your job  a. the list of hostnames where a core is reserved (one per line)\n   *  hint :  man cat  b. number of reserved cores (one per line)\n   *  hint :  man wc  --  use  wc -l  over the pipe  |  command  c. number of reserved nodes (one per line)\n   *  hint :  man uniq  -- use  uniq  over the pipe  |  command  d. number of cores reserved per node together with the node name (one per line)\n   * Example of output:          12 gaia-11\n        12 gaia-15   hint :  man uniq  -- use  uniq -c  over the pipe  |  command   e.  (for geeks)  output the number of reserved nodes times number of cores per node\n   * Example of output:          gaia-11*12\n        gaia-15*12   hint :  man awk  -- use  printf  command of  awk  over the pipe command, for instance  awk '{ printf \"%s*%d\\n\",$2,$1 }' . You might prefer  sed  or any other advanced geek command.",
            "title": "The basics"
        },
        {
            "location": "/basic/getting_started/#job-management_1",
            "text": "Normally, the previously run job is still running.   You can check the status of your running jobs using  oarstat  command: (access)$> oarstat      # access all jobs\n(access)$> oarstat -u   # access all your jobs    Then you can delete your job by running  oardel  command:      (access)$> oardel 919309   you can see your consumption (in an historical computational measure named  CPU hour  i.e. the work done by a CPU in one hour of wall clock time) over a given time period using  oarstat --accounting \"YYYY-MM-DD, YYYY-MM-DD\" -u <youlogin> : (access)$> oarstat --accounting \"2016-01-01, 2016-12-31\" -u <login>    In particular, take a look at the difference between the  asked  resources and the  used  ones  In all remaining examples of reservation in this section, remember to delete the reserved jobs afterwards (using  oardel  or  CTRL-D )  You probably want to use more than one core, and you might want them for a different duration than two hours.\nThe  -l  switch allows you to pass a comma-separated list of parameters specifying the needed resources for the job.    Reserve interactively 4 cores for 6 hours (delete the job afterwards)  (access)$> oarsub -I -l core=6,walltime=6    Reserve interactively 2 nodes for 3h15 (delete the job afterwards):  (access)$> oarsub -I -l nodes=3,walltime=3:15",
            "title": "Job management"
        },
        {
            "location": "/basic/getting_started/#hierarchical-filtering-of-resources",
            "text": "OAR features a very powerful resource filtering/matching engine able to specify resources in a  hierarchical   way using the  /  delimiter. The resource property hierarchy is as follows:      enclosure -> nodes -> cpu -> core    Reserve interactively 2 cores on 3 different nodes belonging to the same enclosure ( total: 6 cores ) for 3h15:  (access)$> oarsub -I -l /enclosure=1/nodes=3/core=2,walltime=3:15    Reserve interactively two full nodes belonging to the different enclosure for 6 hours:  (access)$> oarsub -I -l /enclosure=2/nodes=1,walltime=6    Question: reserve interactively 2 cpus on 2 nodes belonging to the same enclosure for 4 hours  Question: in the following statements, explain the advantage and drawback (in terms of latency/bandwidth etc.) of each of the proposed approaches  a.  oarsub -I -l /nodes=2/cpu=1  vs  oarsub -I -l cpu=2  vs  oarsub -I -l /nodes=1/cpu=2  b.  oarsub -I -l /enclosure=1/nodes=2  vs  oarsub -I -l nodes=2  vs  oarsub -I -l /enclosure=2/nodes=1",
            "title": "Hierarchical filtering of resources"
        },
        {
            "location": "/basic/getting_started/#using-oar-properties",
            "text": "You might have notice on  Monika  for each site a list of properties assigned to each resource.  The  -p  switch allows you to specialize (as an SQL syntax) the property you wish to use when selecting the resources. The syntax is as follows:  oarsub -p \"< property >='< value >'\"  You can find the available OAR properties on the  UL HPC documentation . The main ones are described below     Property  Description  Example      host  Full hostname of the resource  -p \"host='h-cluster1-14.chaos-cluster.uni.lux'\"    network_address  Short hostname of the resource  -p \"network_address='h-cluster1-14'\"    gpu  GPU availability (gaia only)  -p \"gpu='YES'\"       reserve interactively 4 cores on a GPU node for 8 hours ( this holds only on the  gaia  cluster ) ( total: 4 cores )  (access-gaia)$> oarsub -I -l nodes=1/core=4,walltime=8 -p \"gpu='YES'\"    reserve interactively 4 cores on the GPU node  gaia-65  for 8 hours ( this holds only on the  gaia  cluster ) ( total: 4 cores )  (access-gaia)$> oarsub -I -l nodes=1/core=4,walltime=8 -p \"gpu='yes'\" -p \"network_address='gaia-65'\"",
            "title": "Using OAR properties"
        },
        {
            "location": "/basic/getting_started/#reserving-specific-resources-bigsmpand-bigmem",
            "text": "Some nodes are very specific (for instance the nodes with 1TB of memory or the BCS subsystem of Gaia composed of 4 motherboards of 4 processors with a total of 160 cores aggregated in a ccNUMA architecture). Due to this specificity, they are NOT scheduled by default   and can only be reserved with an explicit oarsub parameter:  -t bigmem  or  -t bigsmp   reserve interactively 2 cpu on the bigsmp node belonging to the same board for 3 hours: ( total: 32 cores ) (access-gaia)$> oarsub -t bigsmp -I -l /board=1/cpu=2,walltime=3    Question: why are these resources not scheduled by default?",
            "title": "Reserving specific resources bigsmpand bigmem"
        },
        {
            "location": "/basic/getting_started/#reservation-at-a-given-period-of-time",
            "text": "You can use the  -r \"YYYY-MM-DD HH:MM:SS\"  option of  oarsub  to specify the date you wish the reservation to be issued. This is of particular interest for you to book in advance resources out of the working hours (at night and/or over week ends)",
            "title": "Reservation at a given period of time"
        },
        {
            "location": "/basic/getting_started/#hands-onusing-modules",
            "text": "Environment Modules  is a software package that allows us to provide a  multitude of applications and libraries in multiple versions  on the UL HPC platform. The tool itself is used to manage environment variables such as  PATH ,  LD_LIBRARY_PATH  and  MANPATH , enabling the easy loading and unloading of application/library profiles and their dependencies.  We will have multiple occasion to use modules in the other tutorials so there is nothing special we foresee here. You are just encouraged to read the following resources:   Introduction to Environment Modules by Wolfgang Baumann  Modules tutorial @ NERSC  UL HPC documentation on modules",
            "title": "Hands-on/Using modules"
        },
        {
            "location": "/basic/getting_started/#hands-onpersistent-terminal-sessions-using-gnu-screen",
            "text": "GNU Screen  is a tool to manage persistent terminal sessions.\nIt becomes interesting since you will probably end at some moment with the following  scenario:   you frequently program and run computations on the UL HPC platform  i.e  on a remote Linux/Unix computer, typically working in six different terminal logins to the access server from your office workstation, cranking up long-running computations that are still not finished and are outputting important information (calculation status or results), when you have not 2 interactive jobs running... But it's time to catch the bus and/or the train to go back home.   Probably what you do in the above scenario is to  a. clear and shutdown all running terminal sessions  b. once at home when the kids are in bed, you're logging in again... And have to set up the whole environment again (six logins, 2 interactive jobs etc. )  c. repeat the following morning when you come back to the office.  Enter the long-existing and very simple, but totally indispensable  GNU screen  command. It has the ability to completely detach running processes from one terminal and reattach it intact (later) from a different terminal login.",
            "title": "Hands-on/Persistent Terminal Sessions using GNU Screen"
        },
        {
            "location": "/basic/getting_started/#pre-requisite-screen-configuration-file-screenrc",
            "text": "While not mandatory, we advise you to rely on our customized configuration file for screen  .screenrc  available on  Github .\nNormally, you have nothing to do since we already setup this file for you in your homedir.\nOtherwise, simply clone the  ULHPC dotfile repository  and make a symbolic link  ~/.screenrc  targeting the file  screen/screenrc  of the repository.",
            "title": "Pre-requisite: screen configuration file ~/.screenrc"
        },
        {
            "location": "/basic/getting_started/#basic-commands",
            "text": "You can start a screen session ( i.e.  creates a single window with a shell in it) with the  screen  command.\nIts main command-lines options are listed below:   screen : start a new screen  screen -ls : does not start screen, but prints a list of  pid.tty.host  strings identifying your current screen sessions.  screen -r : resumes a detached screen session  screen -x : attach to a not detached screen session. (Multi display mode  i.e.  when you and another user are trying to access the same session at the same time)   Once within a screen, you can invoke a screen command which consist of a \" CTRL + a \" sequence followed by one other character. The main commands are:   CTRL + a c : (create) creates a new Screen window. The default Screen number is zero.  CTRL + a n : (next) switches to the next window.  CTRL + a p : (prev) switches to the previous window.  CTRL + a d : (detach) detaches from a Screen  CTRL + a A : (title) rename the current window  CTRL + a 0-9 : switches between windows 0 through 9.  CTRL + a k  or  CTRL + d : (kill) destroy the current window  CTRL + a ? : (help) display a list of all the command options available for Screen.",
            "title": "Basic commands"
        },
        {
            "location": "/basic/getting_started/#sample-usage-on-the-ul-hpc-platform-kernel-compilation",
            "text": "We will illustrate the usage of GNU screen by performing a compilation of a recent linux kernel.    start a new screen session  (access)$> screen    rename the screen window \"Frontend\" (using  CTRL+a A )    create the directory to host the files  (access)$> mkdir -p PS1/src\n(access)$> cd PS1/src    create a new window and rename it \"Compile\"    within this new window, start a new interactive job over 1 nodes for 4 hours  (access)$> srun -p interactive --qos qos-interactive --time 4:00:0 -N 1 --pty bash    detach from this screen (using  CTRL+a d )   kill your current SSH connection and your terminal  re-open your terminal and connect back to the cluster frontend   list your running screens:  (access)$> screen -ls\nThere is a screen on:\n    9143.pts-0.access   (05/04/2014 11:29:43 PM) (Detached)\n1 Socket in /var/run/screen/S-svarrette.    re-attach your previous screen session  (access)$> screen -r      # OR screen -r 9143.pts-0.access (see above socket name)    in the \"Compile\" windows, go to the working directory and download the Linux kernel sources  (node)$> cd PS1/src\n(node)$> curl -O https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.13.6.tar.gz    IMPORTANT  to ovoid overloading the  shared  file system with the many small files involves in the kernel compilation ( i.e.  NFS and/or Lustre), we will perform the compilation in the  local  file system,  i.e.  either in  /tmp  or (probably more efficient) in  /dev/shm  ( i.e  in the RAM):      (node)$> mkdir /dev/shm/PS1\n    (node)$> cd /dev/shm/PS1\n    (node)$> tar xzf PS1/src/linux-3.13.6.tar.gz\n    (node)$> cd linux-3.13.6\n    (node)$> make mrproper\n    (node)$> make alldefconfig\n    (node)$> make 2>&1 | tee /dev/shm/PS1/kernel_compile.log   You can now detach from the screen and take a coffee   The last compilation command make use of  tee , a nice tool which read from standard input and write to standard output  and  files. This permits to save in a log file the message written in the standard output.  Question: why using the  make 2>&1  sequence in the last command?  Question: why working in  /dev/shm  is more efficient?   Reattach from time to time to your screen to see the status of the compilation   Your compilation is successful if it ends with the sequence:  [...]\nKernel: arch/x86/boot/bzImage is ready  (#2)    Restart the compilation, this time using parallel jobs within the Makefile invocation ( -j  option of make)  (node)$> make clean\n(node)$> time make -j `echo $SLURM_NPROCS` 2>&1 | tee /dev/shm/PS1/kernel_compile.2.log    The table below should convince you to always run  make  with the  -j  option whenever you can...     Context  time ( make )  time ( make -j 16 )      Compilation in  /tmp (HDD / chaos)  4m6.656s  0m22.981s    Compilation in  /tmp (SSD / gaia)  3m52.895s  0m17.508s    Compilation in  /dev/shm  (RAM)  3m11.649s  0m17.990s      Use the  Ganglia  interface to monitor the impact of the compilation process on the node your job is running on.   Use the following system commands on the node during the compilation:    htop   top  free -m  uptime  ps aux",
            "title": "Sample Usage on the UL HPC platform: Kernel compilation"
        },
        {
            "location": "/basic/getting_started/#using-a-command-line-text-editor",
            "text": "Before the next section, you must learn to use a text editor in command line.\nWe can recommend  nano  or  vim :  nano  is very simple,  vim  is complex but very powerful.",
            "title": "Using a command line text editor"
        },
        {
            "location": "/basic/getting_started/#nano",
            "text": "$ nano <path/filename>   quit and save:  CTRL+x  save:  CTRL+o  highlight text:  Alt-a  Cut the highlighted text:  CTRL+k  Paste:  CTRL+u",
            "title": "Nano"
        },
        {
            "location": "/basic/getting_started/#vim",
            "text": "vim <path/filename>  There are 2 main modes:   Edition mode: press  i  or  insert  once  Command mode: press  ESC  once   Here is a short list of useful commands:   save:  :w  save and quit:  :wq  quit and discard changes:  :q!  search:  /<pattern>  search & replace:  :%s/<pattern>/<replacement>/g  jump to line 100:  :100  highlight text:  CTRL+V  cut the highlighted text:  d  cut one line:  dd  paste:  p  undo:  u",
            "title": "Vim"
        },
        {
            "location": "/basic/getting_started/#advanced-section",
            "text": "",
            "title": "Advanced section"
        },
        {
            "location": "/basic/getting_started/#using-software-modules",
            "text": "The UL HPC provides  environment modules  with the module command\nto manage the user environment, e.g. changing the environment variables.  By loading appropriate environment modules, the user can select:   compilers,  libraries, e.g. the MPI library, or  other third party software packages.   An exhaustive list of the available software is proposed  in this page .  On a node, using an interactive jobs, you can:   list all available softwares:  module avail  search for one software:  module spider <search terms>  \"load\" a software in your environment:  module load <module name>  list the currently loaded modules:  module list  clean your environment, unload everything:  module purge",
            "title": "Using software modules"
        },
        {
            "location": "/basic/getting_started/#matlab",
            "text": "Create a file named  fibonacci.m  in your home directory, copy-paste the following code in this file.\n   This code will calculate the first N numbers of the Fibonacci sequence  N=1000;\nfib=zeros(1,N);\nfib(1)=1;\nfib(2)=1;\nk=3;\nwhile k <= N\n  fib(k)=fib(k-2)+fib(k-1);\n  fprintf('%d\\n',fib(k));\n  pause(1);\n  k=k+1;\nend    Create a new interactive job    Look for the  matlab  module using the command  module spider    Load the module  base/MATLAB  using the command  module load    Execute the code using matlab  (node)$> matlab -nojvm -nodisplay -nosplash < path/to/fibonacci.m",
            "title": "Matlab"
        },
        {
            "location": "/basic/getting_started/#r",
            "text": "Create a file named  fibonacci.R  in your home directory, copy-paste the following code in this file.\n   This code will calculate the first N numbers of the Fibonacci sequence  N <- 130\nfibvals <- numeric(N)\nfibvals[1] <- 1\nfibvals[2] <- 1\nfor (i in 3:N) {\n     fibvals[i] <- fibvals[i-1]+fibvals[i-2]\n     print( fibvals[i], digits=22)\n     Sys.sleep(1)\n}    Create a new interactive job    Look for the  R  module using the command  module spider    Load the module  lang/R  using the command  module load    Execute the code using R  (node)$> Rscript path/to/fibonacci.R",
            "title": "R"
        },
        {
            "location": "/basic/getting_started/#compiling-your-code",
            "text": "In this section, we will learn to compile small \"hello world\" programs in different languages, using different compilers and toolchains.",
            "title": "Compiling your code"
        },
        {
            "location": "/basic/getting_started/#c",
            "text": "Create a new file called  helloworld.c , containing the source code of a simple \"Hello World\" program written in C.      #include<stdio.h>\n\n    int main()\n    {\n        printf(\"Hello, world!\");\n        return 0;\n    }  First, compile the program using the \"FOSS\" toochain, containing the GNU C compiler      (node)$> module load toolchain/foss\n    (node)$> gcc helloworld.c -o helloworld  Then, compile the program using the Intel toolchain, containing the ICC compiler      (node)$> module purge\n    (node)$> module load toolchain/intel\n    (node)$> icc helloworld.c -o helloworld  If you use Intel CPUs and ICC is available on the platform, it is advised to use ICC in order to produce optimized binaries and achieve better performance.",
            "title": "C"
        },
        {
            "location": "/basic/getting_started/#c_1",
            "text": "Question:  create a new file  helloworld.cpp  containing the following C++ source code,\ncompile the following program, using GNU C++ compiler ( g++  command), and the Intel compiler ( icpc  command).      #include <iostream>\n\n    int main() {\n        std::cout << \"Hello, world!\" << std::endl;\n    }",
            "title": "C++"
        },
        {
            "location": "/basic/getting_started/#fortran",
            "text": "Question:  create a new file  helloworld.f  containing the following source code,\ncompile the following program, using the GNU Fortran compiler ( gfortran  command), and ICC ( ifortran  command).      program hello\n       print *, \"Hello, World!\"\n    end program hello  Be careful, the 6 spaces at the beginning of each line are required",
            "title": "Fortran"
        },
        {
            "location": "/basic/getting_started/#mpi",
            "text": "MPI is a programming interface that enables the communication between processes of a distributed memory system.  We will create a simple MPI program where the MPI process of rank 0 broadcasts an integer (42) to all the other processes.\nThen, each process prints its rank, the total number of processes and the value he received from the process 0.  In your home directory, create a file  mpi_broadcast.c  and copy the following source code:      #include <stdio.h>\n    #include <mpi.h>\n    #include <unistd.h>\n    #include <time.h> /* for the work function only */\n\n    int main (int argc, char *argv []) {\n           char hostname[257];\n           int size, rank;\n           int i, pid;\n           int bcast_value = 1;\n\n           gethostname(hostname, sizeof hostname);\n           MPI_Init(&argc, &argv);\n           MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n           MPI_Comm_size(MPI_COMM_WORLD, &size);\n           if (!rank) {\n                bcast_value = 42;\n           }\n           MPI_Bcast(&bcast_value,1 ,MPI_INT, 0, MPI_COMM_WORLD );\n           printf(\"%s\\t- %d - %d - %d\\n\", hostname, rank, size, bcast_value);\n           fflush(stdout);\n\n           MPI_Barrier(MPI_COMM_WORLD);\n           MPI_Finalize();\n           return 0;\n    }  Reserve 2 cores on two distinct node with OAR      (access-gaia)$> oarsub -I -l nodes=2/core=1  or with Slurm      (access-iris)$> srun -p interactive --qos qos-interactive --time 1:00:0 -N 2 -n 2 --pty bash  Load a toolchain and compile the code using  mpicc      (node)$> mpicc mpi_broadcast.c -o mpi_broadcast -lpthread  If you use OAR, execute your mpi program using  mpirun .\nNote that the  -n  parameter of mpirun is the number of processes, which should be equal to the number of reserved cpu cores most of the time.      (node)$> OAR_NTASKS=$(cat $OAR_NODEFILE | wc)\n    (node)$> mpirun -n $OAR_NTASKS -hostfile $OAR_NODEFILE ~/mpi_broadcast  If you use Slurm, you can use the  srun  command. Create an interactive job, with 2 nodes ( -N 2 ), and at least 2 tasks ( -n 2 ).      (node)$> srun -n $SLURM_NTASKS ~/mpi_broadcast",
            "title": "MPI"
        },
        {
            "location": "/basic/getting_started/#using-ssh-proxycommand-setup-to-access-the-clusters-despite-port-filtering",
            "text": "It might happen that the port 8022 is filtered from your working place. You can easily bypass this firewall rule using an SSH proxycommand to setup transparently multi-hop connexions  through  one host (a gateway) to get to the access frontend of the cluster, as depited below:  [laptop] -----||--------> 22 [SSH gateway] ---------> 8022 [access-{chaos,gaia}]\n           firewall  The gateway can be any SSH server which have access to the access frontend of the cluster. The  Gforge @ UL  is typically used in this context but you can prefer any other alternative (your personal NAS @ home etc.). Then alter the SSH config on your laptop (in  ~/.ssh/config  typically) as follows:   create an entry to be able to connect to the gateway:",
            "title": "Using SSH proxycommand setup to access the clusters despite port filtering"
        },
        {
            "location": "/basic/getting_started/#alias-for-the-gateway-not-really-needed-but-convenient-below-instantiated",
            "text": "Host gw\nUser anotherlogin\nHostname host.domain.org\nForwardAgent no",
            "title": "Alias for the gateway (not really needed, but convenient), below instantiated"
        },
        {
            "location": "/basic/getting_started/#automatic-connection-to-ul-hpc-from-the-outside-via-the-gateway",
            "text": "Host *.ulhpc\nProxyCommand ssh gw \"nc -q 0 `basename %h .ulhpc` %p\"  Ensure you can connect to the gateway:  (laptop)$> ssh gw\n(gateway)$> exit # or CTRL-D  The  .ulhpc  suffix we mentioned in the previous configuration is an arbitrary suffix you will now specify in your command lines in order to access the UL HPC platform via the gateway as follows:  (laptop)$> ssh gaia.ulhpc",
            "title": "Automatic connection to UL HPC from the outside via the gateway"
        },
        {
            "location": "/basic/sequential_jobs/",
            "text": "-\n- mode: markdown; mode: auto-fill; fill-column: 80 -\n-\n\n\nCopyright (c) 2016-2017 \nULHPC management team\n --  \nwww\n\n\n\n\nUL HPC Tutorial: HPC workflow with sequential jobs\n\n\n \n \n \n \n \n \n\n\n\n\nPrerequisites\n\n\nMake sure you have followed the tutorial \n\"Getting started\"\n.\n\n\nIntroduction\n\n\nFor many users, the typical usage of the HPC facilities is to execute 1 program with many parameters.\nOn your local machine, you can just start your program 100 times sequentially.\nHowever, you will obtain better results if you parallelize the executions on a HPC Cluster.\n\n\nDuring this session, we will see 3 use cases:\n\n\n\n\nExercise 1\n: Use the serial launcher (1 node, in sequential and parallel mode);\n\n\nExercise 2\n: Use the generic launcher, distribute your executions on several nodes (python script);\n\n\nExercise 3\n: Advanced use case, using a Java program: \"JCell\".\n\n\n\n\nWe will use the following github repositories:\n\n\n\n\nULHPC/launcher-scripts\n\n\nULHPC/tutorials\n\n\n\n\nPre-requisites\n\n\nConnect to the cluster access node, and set-up the environment for this tutorial\n\n\nYou can chose one of the 3 production cluster hosted by the University of Luxembourg.\n\n\nFor the next sections, note that you will use \nSlurm\n on Iris, and \nOAR\n on Chaos & Gaia.\n\n\n(yourmachine)$> ssh iris-cluster\n(yourmachine)$> ssh chaos-cluster\n(yourmachine)$> ssh gaia-cluster\n\n\n\n\nIf your network connection is unstable, use \nscreen\n:\n\n\n(access)$> screen\n\n\n\n\nWe will work in \nthe home directory\n.\n\n\nYou can check the usage of your directories using the command \ndf-ulhpc\n on Gaia\n\n\n(access)$> df-ulhpc\nDirectory                         Used  Soft quota  Hard quota  Grace period\n---------                         ----  ----------  ----------  ------------\n/home/users/hcartiaux             3.2G  100G        -           none\n/work/users/hcartiaux             39M   3.0T        -           none\n\n\n\n\nNote that the user directories are not yet all available on Iris, and that the quota are not yet enabled.\n\n\nCreate a sub directory $HOME/PS2, and work inside it\n\n\n(access)$> mkdir $HOME/PS2\n(access)$> cd $HOME/PS2\n\n\n\n\nIn the following parts, we will assume that you are working in this directory.\n\n\nClone the repositories \nULHPC/tutorials\n and \nULHPC/launcher-scripts.git\n\n\n(access)$> git clone https://github.com/ULHPC/launcher-scripts.git\n(access)$> git clone https://github.com/ULHPC/tutorials.git\n\n\n\nIn order to edit files in your terminal, you are expected to use your preferred text editor:\n\n\n\n\nnano\n\n\nvim\n\n\nemacs\n\n\n...\n\n\n\n\nIf you have never used any of them, \nnano\n is intuitive, but vim and emacs are more powerful.\n\n\nExercise 1: Parametric experiment with Gromacs\n\n\nGromacs is a popular molecular dynamics software.\nIn this exercise, we will process some example input files, and make the parameter \nfourier_spacing\n varies from 0.1 to 0.2 in increments of 0.005.\n\n\nCreate a file which contains the list of parameters:\n\n\n(access)$> seq 0.1 0.002 0.2 > $HOME/PS2/param_file\n\n\n\nStep 1: Naive workflow\n\n\nWe will use the launcher \nNAIVE_AKA_BAD_launcher_serial.sh\n (full path: \n$HOME/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n).\n\n\nEdit the following variables:\n\n\n\n\nMODULE_TO_LOAD\n must contain the list of modules to load before executing \n$TASK\n,\n\n\nTASK\n must contain the path of the executable,\n\n\nARG_TASK_FILE\n must contain the path of your parameter file.\n(node)$> nano $HOME/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n\n    MODULE_TO_LOAD=(bio/GROMACS)\n    TASK=\"$HOME/PS2/tutorials/basic/sequential_jobs/scripts/run_gromacs_sim.sh\"\n    ARG_TASK_FILE=$HOME/PS2/param_file\n\n\n\n\n\n\n\nStep 1a: using OAR on Chaos & Gaia\n\n\nLaunch the job, in interactive mode and execute the launcher:\n\n\n(access)$> oarsub -I -l core=1\n\n    [ADMISSION RULE] Set default walltime to 7200.\n    [ADMISSION RULE] Modify resource description with type constraints\n    OAR_JOB_ID=1542591\n    Interactive mode : waiting...\n    Starting...\n\n    Connect to OAR job 1542591 via the node d-cluster1-1\n    Linux d-cluster1-1 3.2.0-4-amd64 unknown\n     14:27:19 up 29 days, 10 min,  1 user,  load average: 0.00, 0.00, 0.06\n    [OAR] OAR_JOB_ID=1542591\n    [OAR] Your nodes are:\n          d-cluster1-1*1\n\n\n(node)$ $HOME/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n\n\n\nOr\n in passive mode (the output will be written in a file named \nOAR.<JOBID>.stdout\n)\n\n\n(access)$> oarsub -l core=1 $HOME/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n\n    [ADMISSION RULE] Set default walltime to 7200.\n    [ADMISSION RULE] Modify resource description with type constraints\n    OAR_JOB_ID=1542592\n\n\n\nYou can use the command \noarstat -f -j <JOBID>\n to read all the details about your job:\n\n\n(access)$> oarstat -f -j 1542592\n    Job_Id: 1542592\n        project = default\n        owner = hcartiaux\n        state = Running\n        wanted_resources = -l \"{type = 'default'}/core=1,walltime=2:0:0\"\n        assigned_resources = 434\n        assigned_hostnames = d-cluster1-1\n        queue = default\n        command = /work/users/hcartiaux//PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n        ...\n\n\n\nIn all cases, you can connect to a reserved node using the command \noarsub -C <JOBID>\n\nand check the status of the system using standard linux command (\nfree\n, \ntop\n, \nhtop\n, etc)\n\n\n$ (access)$> oarsub -C 1542592\n    Connect to OAR job 1542592 via the node d-cluster1-1\n    Linux d-cluster1-1 3.2.0-4-amd64 unknown\n     14:51:56 up 29 days, 35 min,  2 users,  load average: 1.57, 0.98, 0.70\n    [OAR] OAR_JOB_ID=1542592\n    [OAR] Your nodes are:\n          d-cluster1-1*1\n\n0 14:51:57 hcartiaux@d-cluster1-1(chaos-cluster)[OAR1542592->119] ~ $ free -m\n             total       used       free     shared    buffers     cached\nMem:         48393      41830       6563          0        204      25120\n-/+ buffers/cache:      16505      31888\nSwap:         4095         47       4048\n0 14:51:59 hcartiaux@d-cluster1-1(chaos-cluster)[OAR1542592->119] ~ $ htop\n\n\n\n\n\nDuring the execution, you can try to locate your job on the \nmonika web interface\n.\n\n\n\n\nUsing the \nsystem monitoring tool ganglia\n, check the activity on your node.\n\n\nStep 1b: using Slurm on Iris\n\n\nLaunch the job, in interactive mode and execute the launcher:\n\n\n(access)$> srun -p interactive -N 1 --qos qos-interactive --pty bash -i\n\n(node)$ $HOME/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n\n\n\nOr\n in passive mode (the output will be written in a file named \nBADSerial-<JOBID>.out\n)\n\n\n(access)$> sbatch $HOME/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n\n\n\nYou can use the command \nscontrol show job <JOBID>\n to read all the details about your job:\n\n\n(access)$> scontrol show job 2124\nJobId=2124 JobName=BADSerial\n   UserId=hcartiaux(5079) GroupId=clusterusers(666) MCS_label=N/A\n   Priority=100 Nice=0 Account=ulhpc QOS=qos-batch\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:04:58 TimeLimit=01:00:00 TimeMin=N/\n   SubmitTime=2017-06-11T16:12:27 EligibleTime=2017-06-11T16:12:27\n   StartTime=2017-06-11T16:12:28 EndTime=2017-06-11T17:12:28 Deadline=N/A\n\n\n\nAnd the command \nsacct\n to see the start and end date\n\n\n(access)$> sacct --format=start,end --j 2125\n              Start                 End\n------------------- -------------------\n2017-06-11T16:23:23 2017-06-11T16:23:51\n2017-06-11T16:23:23 2017-06-11T16:23:51\n\n\n\nIn all cases, you can connect to a reserved node using the command \nsrun\n\nand check the status of the system using standard linux command (\nfree\n, \ntop\n, \nhtop\n, etc)\n\n\n(access)$> srun -p interactive --qos qos-interactive --jobid <JOBID> --pty bash\n\n\n\nDuring the execution, you can see the job in the queue with the command \nsqueue\n:\n\n\n(access)$> squeue\n         JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          2124     batch BADSeria hcartiau  R       2:16      1 iris-053\n          2122 interacti     bash svarrett  R       5:12      1 iris-081\n\n\n\nUsing the \nsystem monitoring tool ganglia\n, check the activity on your node.\n\n\nStep 2: Optimal method using GNU parallel (GNU Parallel)\n\n\nWe will use the launcher \nlauncher_serial.sh\n (full path: \n$HOME/PS2/launcher-scripts/bash/serial/launcher_serial.sh\n).\n\n\nEdit the following variables:\n\n\n(access)$> nano $HOME/PS2/launcher-scripts/bash/serial/launcher_serial.sh\n\nMODULE_TO_LOAD=(bio/GROMACS)\nTASK=\"$HOME/PS2/tutorials/basic/sequential_jobs/scripts/run_gromacs_sim.sh\"\nARG_TASK_FILE=$HOME/PS2/param_file\n\n\n\nSubmit the (passive) job with \noarsub\n if you are using Chaos or Gaia\n\n\n(access)$> oarsub -l nodes=1 $HOME/PS2/launcher-scripts/bash/serial/launcher_serial.sh\n\n\n\nOr with \nsbatch\n if you are using Iris\n\n\n(access)$> sbatch $HOME/PS2/launcher-scripts/bash/serial/launcher_serial.sh\n\n\n\nQuestion\n: compare and explain the execution time with both launchers:\n\n\n\n\n\n\nNaive workflow: time = \n16m 32s\n\n  \n\n\n\n\n\n\nParallel workflow: time = \n2m 11s\n\n  \n\n\n\n\n\n\n/!\\ Gaia and Chaos nodes are heterogeneous. In order to compare execution times,\nyou must always use the same type of nodes (CPU/Memory), using \nproperties\n\nin your \noarsub\n command.\n\n\nExercise 2: Watermarking images in Python\n\n\nWe will use another program, \nwatermark.py\n (full path: \n$HOME/PS2/tutorials/basic/sequential_jobs/scripts/watermark.py\n),\nand we will distribute the computation on 2 nodes with the launcher \nparallel_launcher.sh\n\n(full path: \n$HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n).\n\n\nThis python script will apply a watermark to the images (using the Python Imaging library).\n\n\nThe command works like this:\n\n\npython watermark.py <path/to/watermark_image> <source_image>\n\n\n\nWe will work with 2 files:\n\n\n\n\ncopyright.png\n: a transparent images, which can be applied as a watermark\n\n\nimages.tgz\n: a compressed file, containing 30 JPG pictures (of the Gaia Cluster :) ).\n\n\n\n\nStep 0: python image manipulation module installation\n\n\nIn an interactive job, install \npillow\n in your home directory using this command:\n\n\n(access IRIS)>$ srun -p interactive -N 1 --qos qos-interactive --pty bash -i\n(access Chaos/Gaia)>$ oarsub -I\n\n\n\n(node)>$ pip install --user pillow\n\n\n\nStep 1: Prepare the input files\n\n\nCopy the source files in your $HOME directory.\n\n\n(access)>$ tar xvf /mnt/isilon/projects/ulhpc-tutorials/sequential/images.tgz -C $HOME/PS2/\n(access)>$ cp /mnt/isilon/projects/ulhpc-tutorials/sequential/copyright.png $HOME/PS2\n\n(access)>$ cd $HOME/PS2\n\n\n\nStep 2: Create a list of parameters\n\n\nWe must create a file containing a list of parameters, each line will be passed to \nwatermark.py\n.\n\n\nls -d -1 $HOME/PS2/images/*.JPG | awk -v watermark=$HOME/PS2/copyright.png '{print watermark \" \" $1}' > $HOME/PS2/generic_launcher_param\n\\_____________________________/   \\_________________________________________________________________/ \\_________________________________/\n               1                                                    2                                                3\n\n\n\n\n\nls -d -1\n: list the images\n\n\nawk ...\n: prefix each line with the first parameter (watermark file)\n\n\n>\n: redirect the output to the file $HOME/generic_launcher_param\n\n\n\n\nStep 3: Configure the launcher\n\n\nWe will use the launcher \nparallel_launcher.sh\n (full path: \n$HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n).\n\n\nEdit the following variables:\n\n\n(access)$> nano $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\nTASK=\"$HOME/PS2/tutorials/basic/sequential_jobs/scripts/watermark.py\"\nARG_TASK_FILE=\"$HOME/PS2/generic_launcher_param\"\n# number of cores needed for 1 task\nNB_CORE_PER_TASK=2\n\n\n\nStep 4: Submit the job\n\n\nWe will spawn 1 process per 2 cores on 2 nodes\n\n\nOn Iris, the Slurm job submission command is \nsbatch\n\n\n(access IRIS)>$ sbatch $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\n\n\nOn Chaos and Gaia, the OAR job submission command is \noarsub\n\n\n(access Chaos/Gaia)>$ oarsub $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\n\n\nStep 5: Download the files\n\n\nOn your laptop, transfer the files in the current directory and look at them with your favorite viewer.\nUse one of these commands according to the cluster you have used:\n\n\n(yourmachine)$> rsync -avz chaos-cluster:/home/users/<LOGIN>/PS2/images .\n(yourmachine)$> rsync -avz gaia-cluster:/home/users/<LOGIN>/PS2/images .\n(yourmachine)$> rsync -avz iris-cluster:/home/users/<LOGIN>/PS2/images .\n\n\n\nQuestion\n: which nodes are you using, identify your nodes with the command \noarstat -f -j <JOBID>\n or Monika\n(\nChaos\n, \nGaia\n)\n\n\nExercise 3: Advanced use case, using a Java program: \"JCell\"\n\n\nLet's use \nJCell\n, a framework for working with genetic algorithms, programmed in Java.\n\n\nWe will use 3 scripts:\n\n\n\n\njcell_config_gen.sh\n (full path: \n$HOME/PS2/tutorials/basic/sequential_jobs/scripts/jcell_config_gen.sh\n)\n\n\n\n\nWe want to execute Jcell, and change the parameters MutationProb and CrossoverProb.\nThis script will install JCell, generate a tarball containing all the configuration files,\nand the list of parameters to be given to the launcher.\n\n\n\n\njcell_wrapper.sh\n (full path: \n$HOME/PS2/tutorials/basic/sequential_jobs/scripts/jcell_wrapper.sh\n)\n\n\n\n\nThis script is a wrapper, and will start one execution of jcell with the configuration file given in parameter.\nIf a result already exists, then the execution will be skipped.\nThanks to this simple test, our workflow is fault tolerant,\nif the job is interrupted and restarted, only the missing results will be computed.\n\n\n\n\nparallel_launcher.sh\n (full path: \n$HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n)\n\n\n\n\nThis script will drive the experiment, start and balance the java processes on all the reserved resources.\n\n\nStep 1: Generate the configuration files:\n\n\nExecute this script:\n\n\n    (access)$> $HOME/PS2/tutorials/basic/sequential_jobs/scripts/jcell_config_gen.sh\n\n\n\nThis script will generate the following files in \n$HOME/PS2/jcell\n:\n\n\n\n\nconfig.tgz\n\n\njcell_param\n\n\n\n\nStep 2: Edit the launcher configuration, in the file \n$HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n.\n\n\nThis application is cpu-bound and not memory-bound, so we can set the value of \nNB_CORE_PER_TASK\n to 1.\nUsing these parameters, the launcher will spaw one java process per core on all the reserved nodes.\n\n\n    (access)$> nano $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\n    TASK=\"$HOME/PS2/tutorials/basic/sequential_jobs/scripts/jcell_wrapper.sh\"\n    ARG_TASK_FILE=\"$HOME/PS2/jcell/jcell_param\"\n    # number of cores needed for 1 task\n    NB_CORE_PER_TASK=1\n\n\n\nStep 3: Submit the job\n\n\nOn Iris, the Slurm job submission command is \nsbatch\n\n\n(access IRIS)>$ sbatch $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\n\n\nOn Chaos and Gaia, the OAR job submission command is \noarsub\n\n\n(access Chaos/Gaia)>$ oarsub $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\n\n\nStep 4. Retrieve the results on your laptop:\n\n\nUse one of these commands according to the cluster you have used:\n\n\n    (yourmachine)$> rsync -avz chaos-cluster:/home/users/<LOGIN>/PS2/jcell/results .\n    (yourmachine)$> rsync -avz gaia-cluster:/home/users/<LOGIN>/PS2/jcell/results .\n    (yourmachine)$> rsync -avz iris-cluster:/home/users/<LOGIN>/PS2/jcell/results .\n\n\n\nQuestion\n: check the system load and memory usage with Ganglia\n(\nChaos\n, \nGaia\n)\n\n\nConclusion\n\n\nAt the end, please clean up your home and work directories :)\n\n\nPlease\n do not store unnecessary files on the cluster's storage servers:\n\n\n(access)$> rm -rf $HOME/PS2\n\n\n\nFor going further:\n\n\n\n\nCheckpoint / restart with BLCR\n\n\nOAR array jobs (fr)",
            "title": "HPC workflow with sequential jobs"
        },
        {
            "location": "/basic/sequential_jobs/#ul-hpc-tutorial-hpc-workflow-with-sequential-jobs",
            "text": "Prerequisites  Make sure you have followed the tutorial  \"Getting started\" .",
            "title": "UL HPC Tutorial: HPC workflow with sequential jobs"
        },
        {
            "location": "/basic/sequential_jobs/#introduction",
            "text": "For many users, the typical usage of the HPC facilities is to execute 1 program with many parameters.\nOn your local machine, you can just start your program 100 times sequentially.\nHowever, you will obtain better results if you parallelize the executions on a HPC Cluster.  During this session, we will see 3 use cases:   Exercise 1 : Use the serial launcher (1 node, in sequential and parallel mode);  Exercise 2 : Use the generic launcher, distribute your executions on several nodes (python script);  Exercise 3 : Advanced use case, using a Java program: \"JCell\".   We will use the following github repositories:   ULHPC/launcher-scripts  ULHPC/tutorials",
            "title": "Introduction"
        },
        {
            "location": "/basic/sequential_jobs/#pre-requisites",
            "text": "",
            "title": "Pre-requisites"
        },
        {
            "location": "/basic/sequential_jobs/#connect-to-the-cluster-access-node-and-set-up-the-environment-for-this-tutorial",
            "text": "You can chose one of the 3 production cluster hosted by the University of Luxembourg.  For the next sections, note that you will use  Slurm  on Iris, and  OAR  on Chaos & Gaia.  (yourmachine)$> ssh iris-cluster\n(yourmachine)$> ssh chaos-cluster\n(yourmachine)$> ssh gaia-cluster  If your network connection is unstable, use  screen :  (access)$> screen  We will work in  the home directory .  You can check the usage of your directories using the command  df-ulhpc  on Gaia  (access)$> df-ulhpc\nDirectory                         Used  Soft quota  Hard quota  Grace period\n---------                         ----  ----------  ----------  ------------\n/home/users/hcartiaux             3.2G  100G        -           none\n/work/users/hcartiaux             39M   3.0T        -           none  Note that the user directories are not yet all available on Iris, and that the quota are not yet enabled.  Create a sub directory $HOME/PS2, and work inside it  (access)$> mkdir $HOME/PS2\n(access)$> cd $HOME/PS2  In the following parts, we will assume that you are working in this directory.  Clone the repositories  ULHPC/tutorials  and  ULHPC/launcher-scripts.git  (access)$> git clone https://github.com/ULHPC/launcher-scripts.git\n(access)$> git clone https://github.com/ULHPC/tutorials.git  In order to edit files in your terminal, you are expected to use your preferred text editor:   nano  vim  emacs  ...   If you have never used any of them,  nano  is intuitive, but vim and emacs are more powerful.",
            "title": "Connect to the cluster access node, and set-up the environment for this tutorial"
        },
        {
            "location": "/basic/sequential_jobs/#exercise-1-parametric-experiment-with-gromacs",
            "text": "Gromacs is a popular molecular dynamics software.\nIn this exercise, we will process some example input files, and make the parameter  fourier_spacing  varies from 0.1 to 0.2 in increments of 0.005.  Create a file which contains the list of parameters:  (access)$> seq 0.1 0.002 0.2 > $HOME/PS2/param_file",
            "title": "Exercise 1: Parametric experiment with Gromacs"
        },
        {
            "location": "/basic/sequential_jobs/#step-1-naive-workflow",
            "text": "We will use the launcher  NAIVE_AKA_BAD_launcher_serial.sh  (full path:  $HOME/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh ).  Edit the following variables:   MODULE_TO_LOAD  must contain the list of modules to load before executing  $TASK ,  TASK  must contain the path of the executable,  ARG_TASK_FILE  must contain the path of your parameter file. (node)$> nano $HOME/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n\n    MODULE_TO_LOAD=(bio/GROMACS)\n    TASK=\"$HOME/PS2/tutorials/basic/sequential_jobs/scripts/run_gromacs_sim.sh\"\n    ARG_TASK_FILE=$HOME/PS2/param_file",
            "title": "Step 1: Naive workflow"
        },
        {
            "location": "/basic/sequential_jobs/#step-1a-using-oar-on-chaos-gaia",
            "text": "Launch the job, in interactive mode and execute the launcher:  (access)$> oarsub -I -l core=1\n\n    [ADMISSION RULE] Set default walltime to 7200.\n    [ADMISSION RULE] Modify resource description with type constraints\n    OAR_JOB_ID=1542591\n    Interactive mode : waiting...\n    Starting...\n\n    Connect to OAR job 1542591 via the node d-cluster1-1\n    Linux d-cluster1-1 3.2.0-4-amd64 unknown\n     14:27:19 up 29 days, 10 min,  1 user,  load average: 0.00, 0.00, 0.06\n    [OAR] OAR_JOB_ID=1542591\n    [OAR] Your nodes are:\n          d-cluster1-1*1\n\n\n(node)$ $HOME/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh  Or  in passive mode (the output will be written in a file named  OAR.<JOBID>.stdout )  (access)$> oarsub -l core=1 $HOME/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n\n    [ADMISSION RULE] Set default walltime to 7200.\n    [ADMISSION RULE] Modify resource description with type constraints\n    OAR_JOB_ID=1542592  You can use the command  oarstat -f -j <JOBID>  to read all the details about your job:  (access)$> oarstat -f -j 1542592\n    Job_Id: 1542592\n        project = default\n        owner = hcartiaux\n        state = Running\n        wanted_resources = -l \"{type = 'default'}/core=1,walltime=2:0:0\"\n        assigned_resources = 434\n        assigned_hostnames = d-cluster1-1\n        queue = default\n        command = /work/users/hcartiaux//PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n        ...  In all cases, you can connect to a reserved node using the command  oarsub -C <JOBID> \nand check the status of the system using standard linux command ( free ,  top ,  htop , etc)  $ (access)$> oarsub -C 1542592\n    Connect to OAR job 1542592 via the node d-cluster1-1\n    Linux d-cluster1-1 3.2.0-4-amd64 unknown\n     14:51:56 up 29 days, 35 min,  2 users,  load average: 1.57, 0.98, 0.70\n    [OAR] OAR_JOB_ID=1542592\n    [OAR] Your nodes are:\n          d-cluster1-1*1\n\n0 14:51:57 hcartiaux@d-cluster1-1(chaos-cluster)[OAR1542592->119] ~ $ free -m\n             total       used       free     shared    buffers     cached\nMem:         48393      41830       6563          0        204      25120\n-/+ buffers/cache:      16505      31888\nSwap:         4095         47       4048\n0 14:51:59 hcartiaux@d-cluster1-1(chaos-cluster)[OAR1542592->119] ~ $ htop   During the execution, you can try to locate your job on the  monika web interface .   Using the  system monitoring tool ganglia , check the activity on your node.",
            "title": "Step 1a: using OAR on Chaos &amp; Gaia"
        },
        {
            "location": "/basic/sequential_jobs/#step-1b-using-slurm-on-iris",
            "text": "Launch the job, in interactive mode and execute the launcher:  (access)$> srun -p interactive -N 1 --qos qos-interactive --pty bash -i\n\n(node)$ $HOME/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh  Or  in passive mode (the output will be written in a file named  BADSerial-<JOBID>.out )  (access)$> sbatch $HOME/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh  You can use the command  scontrol show job <JOBID>  to read all the details about your job:  (access)$> scontrol show job 2124\nJobId=2124 JobName=BADSerial\n   UserId=hcartiaux(5079) GroupId=clusterusers(666) MCS_label=N/A\n   Priority=100 Nice=0 Account=ulhpc QOS=qos-batch\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:04:58 TimeLimit=01:00:00 TimeMin=N/\n   SubmitTime=2017-06-11T16:12:27 EligibleTime=2017-06-11T16:12:27\n   StartTime=2017-06-11T16:12:28 EndTime=2017-06-11T17:12:28 Deadline=N/A  And the command  sacct  to see the start and end date  (access)$> sacct --format=start,end --j 2125\n              Start                 End\n------------------- -------------------\n2017-06-11T16:23:23 2017-06-11T16:23:51\n2017-06-11T16:23:23 2017-06-11T16:23:51  In all cases, you can connect to a reserved node using the command  srun \nand check the status of the system using standard linux command ( free ,  top ,  htop , etc)  (access)$> srun -p interactive --qos qos-interactive --jobid <JOBID> --pty bash  During the execution, you can see the job in the queue with the command  squeue :  (access)$> squeue\n         JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          2124     batch BADSeria hcartiau  R       2:16      1 iris-053\n          2122 interacti     bash svarrett  R       5:12      1 iris-081  Using the  system monitoring tool ganglia , check the activity on your node.",
            "title": "Step 1b: using Slurm on Iris"
        },
        {
            "location": "/basic/sequential_jobs/#step-2-optimal-method-using-gnu-parallel-gnu-parallel",
            "text": "We will use the launcher  launcher_serial.sh  (full path:  $HOME/PS2/launcher-scripts/bash/serial/launcher_serial.sh ).  Edit the following variables:  (access)$> nano $HOME/PS2/launcher-scripts/bash/serial/launcher_serial.sh\n\nMODULE_TO_LOAD=(bio/GROMACS)\nTASK=\"$HOME/PS2/tutorials/basic/sequential_jobs/scripts/run_gromacs_sim.sh\"\nARG_TASK_FILE=$HOME/PS2/param_file  Submit the (passive) job with  oarsub  if you are using Chaos or Gaia  (access)$> oarsub -l nodes=1 $HOME/PS2/launcher-scripts/bash/serial/launcher_serial.sh  Or with  sbatch  if you are using Iris  (access)$> sbatch $HOME/PS2/launcher-scripts/bash/serial/launcher_serial.sh  Question : compare and explain the execution time with both launchers:    Naive workflow: time =  16m 32s \n      Parallel workflow: time =  2m 11s \n      /!\\ Gaia and Chaos nodes are heterogeneous. In order to compare execution times,\nyou must always use the same type of nodes (CPU/Memory), using  properties \nin your  oarsub  command.",
            "title": "Step 2: Optimal method using GNU parallel (GNU Parallel)"
        },
        {
            "location": "/basic/sequential_jobs/#exercise-2-watermarking-images-in-python",
            "text": "We will use another program,  watermark.py  (full path:  $HOME/PS2/tutorials/basic/sequential_jobs/scripts/watermark.py ),\nand we will distribute the computation on 2 nodes with the launcher  parallel_launcher.sh \n(full path:  $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh ).  This python script will apply a watermark to the images (using the Python Imaging library).  The command works like this:  python watermark.py <path/to/watermark_image> <source_image>  We will work with 2 files:   copyright.png : a transparent images, which can be applied as a watermark  images.tgz : a compressed file, containing 30 JPG pictures (of the Gaia Cluster :) ).",
            "title": "Exercise 2: Watermarking images in Python"
        },
        {
            "location": "/basic/sequential_jobs/#step-0-python-image-manipulation-module-installation",
            "text": "In an interactive job, install  pillow  in your home directory using this command:  (access IRIS)>$ srun -p interactive -N 1 --qos qos-interactive --pty bash -i\n(access Chaos/Gaia)>$ oarsub -I\n\n\n\n(node)>$ pip install --user pillow",
            "title": "Step 0: python image manipulation module installation"
        },
        {
            "location": "/basic/sequential_jobs/#step-1-prepare-the-input-files",
            "text": "Copy the source files in your $HOME directory.  (access)>$ tar xvf /mnt/isilon/projects/ulhpc-tutorials/sequential/images.tgz -C $HOME/PS2/\n(access)>$ cp /mnt/isilon/projects/ulhpc-tutorials/sequential/copyright.png $HOME/PS2\n\n(access)>$ cd $HOME/PS2",
            "title": "Step 1: Prepare the input files"
        },
        {
            "location": "/basic/sequential_jobs/#step-2-create-a-list-of-parameters",
            "text": "We must create a file containing a list of parameters, each line will be passed to  watermark.py .  ls -d -1 $HOME/PS2/images/*.JPG | awk -v watermark=$HOME/PS2/copyright.png '{print watermark \" \" $1}' > $HOME/PS2/generic_launcher_param\n\\_____________________________/   \\_________________________________________________________________/ \\_________________________________/\n               1                                                    2                                                3   ls -d -1 : list the images  awk ... : prefix each line with the first parameter (watermark file)  > : redirect the output to the file $HOME/generic_launcher_param",
            "title": "Step 2: Create a list of parameters"
        },
        {
            "location": "/basic/sequential_jobs/#step-3-configure-the-launcher",
            "text": "We will use the launcher  parallel_launcher.sh  (full path:  $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh ).  Edit the following variables:  (access)$> nano $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\nTASK=\"$HOME/PS2/tutorials/basic/sequential_jobs/scripts/watermark.py\"\nARG_TASK_FILE=\"$HOME/PS2/generic_launcher_param\"\n# number of cores needed for 1 task\nNB_CORE_PER_TASK=2",
            "title": "Step 3: Configure the launcher"
        },
        {
            "location": "/basic/sequential_jobs/#step-4-submit-the-job",
            "text": "We will spawn 1 process per 2 cores on 2 nodes  On Iris, the Slurm job submission command is  sbatch  (access IRIS)>$ sbatch $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh  On Chaos and Gaia, the OAR job submission command is  oarsub  (access Chaos/Gaia)>$ oarsub $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh",
            "title": "Step 4: Submit the job"
        },
        {
            "location": "/basic/sequential_jobs/#step-5-download-the-files",
            "text": "On your laptop, transfer the files in the current directory and look at them with your favorite viewer.\nUse one of these commands according to the cluster you have used:  (yourmachine)$> rsync -avz chaos-cluster:/home/users/<LOGIN>/PS2/images .\n(yourmachine)$> rsync -avz gaia-cluster:/home/users/<LOGIN>/PS2/images .\n(yourmachine)$> rsync -avz iris-cluster:/home/users/<LOGIN>/PS2/images .  Question : which nodes are you using, identify your nodes with the command  oarstat -f -j <JOBID>  or Monika\n( Chaos ,  Gaia )",
            "title": "Step 5: Download the files"
        },
        {
            "location": "/basic/sequential_jobs/#exercise-3-advanced-use-case-using-a-java-program-jcell",
            "text": "Let's use  JCell , a framework for working with genetic algorithms, programmed in Java.  We will use 3 scripts:   jcell_config_gen.sh  (full path:  $HOME/PS2/tutorials/basic/sequential_jobs/scripts/jcell_config_gen.sh )   We want to execute Jcell, and change the parameters MutationProb and CrossoverProb.\nThis script will install JCell, generate a tarball containing all the configuration files,\nand the list of parameters to be given to the launcher.   jcell_wrapper.sh  (full path:  $HOME/PS2/tutorials/basic/sequential_jobs/scripts/jcell_wrapper.sh )   This script is a wrapper, and will start one execution of jcell with the configuration file given in parameter.\nIf a result already exists, then the execution will be skipped.\nThanks to this simple test, our workflow is fault tolerant,\nif the job is interrupted and restarted, only the missing results will be computed.   parallel_launcher.sh  (full path:  $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh )   This script will drive the experiment, start and balance the java processes on all the reserved resources.",
            "title": "Exercise 3: Advanced use case, using a Java program: \"JCell\""
        },
        {
            "location": "/basic/sequential_jobs/#step-1-generate-the-configuration-files",
            "text": "Execute this script:      (access)$> $HOME/PS2/tutorials/basic/sequential_jobs/scripts/jcell_config_gen.sh  This script will generate the following files in  $HOME/PS2/jcell :   config.tgz  jcell_param",
            "title": "Step 1: Generate the configuration files:"
        },
        {
            "location": "/basic/sequential_jobs/#step-2-edit-the-launcher-configuration-in-the-file-homeps2launcher-scriptsbashgenericparallel_launchersh",
            "text": "This application is cpu-bound and not memory-bound, so we can set the value of  NB_CORE_PER_TASK  to 1.\nUsing these parameters, the launcher will spaw one java process per core on all the reserved nodes.      (access)$> nano $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\n    TASK=\"$HOME/PS2/tutorials/basic/sequential_jobs/scripts/jcell_wrapper.sh\"\n    ARG_TASK_FILE=\"$HOME/PS2/jcell/jcell_param\"\n    # number of cores needed for 1 task\n    NB_CORE_PER_TASK=1",
            "title": "Step 2: Edit the launcher configuration, in the file $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh."
        },
        {
            "location": "/basic/sequential_jobs/#step-3-submit-the-job",
            "text": "On Iris, the Slurm job submission command is  sbatch  (access IRIS)>$ sbatch $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh  On Chaos and Gaia, the OAR job submission command is  oarsub  (access Chaos/Gaia)>$ oarsub $HOME/PS2/launcher-scripts/bash/generic/parallel_launcher.sh",
            "title": "Step 3: Submit the job"
        },
        {
            "location": "/basic/sequential_jobs/#step-4-retrieve-the-results-on-your-laptop",
            "text": "Use one of these commands according to the cluster you have used:      (yourmachine)$> rsync -avz chaos-cluster:/home/users/<LOGIN>/PS2/jcell/results .\n    (yourmachine)$> rsync -avz gaia-cluster:/home/users/<LOGIN>/PS2/jcell/results .\n    (yourmachine)$> rsync -avz iris-cluster:/home/users/<LOGIN>/PS2/jcell/results .  Question : check the system load and memory usage with Ganglia\n( Chaos ,  Gaia )",
            "title": "Step 4. Retrieve the results on your laptop:"
        },
        {
            "location": "/basic/sequential_jobs/#conclusion",
            "text": "At the end, please clean up your home and work directories :)  Please  do not store unnecessary files on the cluster's storage servers:  (access)$> rm -rf $HOME/PS2  For going further:   Checkpoint / restart with BLCR  OAR array jobs (fr)",
            "title": "Conclusion"
        },
        {
            "location": "/advanced/Debug/",
            "text": "-\n- mode: markdown; mode: auto-fill; fill-column: 80 -\n-\n\n\nCopyright (c) 2013-2017 X. Besseron and  S. Varrette / UL HPC Team  -- see \nhttp://hpc.uni.lu\n\n\n\n\nUL HPC Tutorial:  Know Your Bugs: Weapons for Efficient Debugging\n\n\n \n \n \n \n \n \n\n\n\n\nThe objective of this tutorial is to review the main tools that can be used to\ndebug your [parallel] programs.\n\n\nHands/On 0 - Pre-requisites\n\n\nReserve 1 core (for 3h) over the UL HPC platform\n\n\n  $> ssh gaia-cluster    # OR chaos-cluster\n  $> oarsub -I -l core=1,walltime=\"03:00:00\"\n\n\n\nHands/On 1 - GDB Tutorial\n\n\nTutorial from \nA GDB Tutorial with Examples\n\n\nYou'll need to load the latest GDB module:\n\n\n  $> module spider gdb\n  $> module load  debugger/GDB\n\n\n\nHands/On 2 - Valgrind Tutorial\n\n\nTutorial from \nUsing Valgrind to Find Memory Leaks and Invalid Memory Use\n\n\nYou'll also need to load the appropriate module\n\n\n  $> module spider valgrind\n  $> module load debugger/Valgrind\n\n\n\nHands/On 3 - Bug Hunting\n\n\nA list of programs demonstrating the different kind of bus are available in the \nexercises\n directory.\nTry the different debugging tools on every example to see how they behave and find the bugs.\n\n\nRun the following command to download all the exercises:\n\n\n$> git clone https://github.com/ULHPC/tutorials.git ulhpc-tutorials\n$> cd ulhpc-tutorials/advanced/Debug/exercises/\n\n\n\n\nNotes\n:\n\n\n\n\nYou can compile each program manually using \ngcc\n or \nicc\n (the latest coming from the \ntoolchains/ictce\n module). You are encouraged to try both to see how differently they behave. Example: \ngcc program.c -o program\n. Add any additional parameter you might need.\n\n\nSome program required additional options to be compiled. They are indicated in comment at the beginning of each source file.",
            "title": "Efficient Debugging"
        },
        {
            "location": "/advanced/Debug/#ul-hpc-tutorial-know-your-bugs-weapons-for-efficient-debugging",
            "text": "The objective of this tutorial is to review the main tools that can be used to\ndebug your [parallel] programs.",
            "title": "UL HPC Tutorial:  Know Your Bugs: Weapons for Efficient Debugging"
        },
        {
            "location": "/advanced/Debug/#handson-0-pre-requisites",
            "text": "Reserve 1 core (for 3h) over the UL HPC platform    $> ssh gaia-cluster    # OR chaos-cluster\n  $> oarsub -I -l core=1,walltime=\"03:00:00\"",
            "title": "Hands/On 0 - Pre-requisites"
        },
        {
            "location": "/advanced/Debug/#handson-1-gdb-tutorial",
            "text": "Tutorial from  A GDB Tutorial with Examples  You'll need to load the latest GDB module:    $> module spider gdb\n  $> module load  debugger/GDB",
            "title": "Hands/On 1 - GDB Tutorial"
        },
        {
            "location": "/advanced/Debug/#handson-2-valgrind-tutorial",
            "text": "Tutorial from  Using Valgrind to Find Memory Leaks and Invalid Memory Use  You'll also need to load the appropriate module    $> module spider valgrind\n  $> module load debugger/Valgrind",
            "title": "Hands/On 2 - Valgrind Tutorial"
        },
        {
            "location": "/advanced/Debug/#handson-3-bug-hunting",
            "text": "A list of programs demonstrating the different kind of bus are available in the  exercises  directory.\nTry the different debugging tools on every example to see how they behave and find the bugs.  Run the following command to download all the exercises:  $> git clone https://github.com/ULHPC/tutorials.git ulhpc-tutorials\n$> cd ulhpc-tutorials/advanced/Debug/exercises/  Notes :   You can compile each program manually using  gcc  or  icc  (the latest coming from the  toolchains/ictce  module). You are encouraged to try both to see how differently they behave. Example:  gcc program.c -o program . Add any additional parameter you might need.  Some program required additional options to be compiled. They are indicated in comment at the beginning of each source file.",
            "title": "Hands/On 3 - Bug Hunting"
        },
        {
            "location": "/advanced/EasyBuild/",
            "text": "-\n- mode: markdown;mode:visual-line;  fill-column: 80 -\n-\n\n\nCopyright (c) 2014-2017 UL HPC Team  -- see \nhttp://hpc.uni.lu\n\n\n\n\nBuilding [custom] software with EasyBuild on UL HPC platform\n\n\n \n \n \n \n \n \n\n\n\n\nThe objective of this tutorial is to show how \nEasyBuild\n can be used to ease, automate and script the build of software on the UL HPC platforms.\n\n\nTwo use-cases are considered. First, we are going to build software that are supported by EasyBuild. In a second time, we will see through a simple example how to add support for a new software in EasyBuild.\n\n\nThe benefit of using EasyBuild for your builds is that it allows automated and reproducable build of software. Once a build has been made, the build script (via the \nEasyConfig file\n) or the installed software (via the \nmodule file\n) can be shared with other users.\n\n\nBefore starting this tutorial, ensure you are able to \nconnect to the chaos and gaia cluster\n.\n\nFor all your compilation with Easybuild, you must work on a computing node:\n\n\n\n\n\n\nGaia\n\n\n(access-gaia)$> oarsub -I -l core=1,walltime=4\n\n\n\n\n\n\n\nIris\n\n\n(access-iris)$> srun -p interactive --qos qos-interactive -t 0-4:0:0 --pty bash\n\n\n\n\n\n\n\nThe latest version of this tutorial is available on\n\nGithub\n.\n\n\nShort introduction to EasyBuild\n\n\nEasyBuild is a tool that allows to perform automated and reproducible compilation and installation of software. A large number of scientific software are supported (649 software packages in the last release).\n\n\nAll builds and installations are performed at user level, so you don't need the admin rights.\nThe software are installed in your home directory (by default in \n$HOME/.local/easybuild/software/\n) and a module file is generated (by default in \n$HOME/.local/easybuild/modules/\n) to use the software.\n\n\nEasyBuild relies on two main concepts: \nToolchains\n and \nEasyConfig file\n.\n\n\nA \ntoolchain\n corresponds to a compiler and a set of libraries which are commonly used to build a software. The two main toolchains frequently used on the UL HPC platform are the GOOLF and the ICTCE toolchains. GOOLF is based on the GCC compiler and on open-source libraries (OpenMPI, OpenBLAS, etc.). ICTCE is based on the Intel compiler and on Intel libraries (Intel MPI, Intel Math Kernel Library, etc.).\n\n\nAn \nEasyConfig file\n is a simple text file that describes the build process of a software. For most software that uses standard procedure (like \nconfigure\n, \nmake\n and \nmake install\n), this file is very simple. Many EasyConfig files are already provided with EasyBuild.\n\n\nBy default, EasyConfig files and generated modules are named using the following convention:\n\n<Software-Name>-<Software-Version>-<Toolchain-Name>-<Toolchain-Version>\n\n\nOn the cluster however, for the module names we use a custom naming convention that is explained in the RESIF tutorial:\n\n<Software-Class>/<Software-Name>/<Software-Version>-<Toolchain-Name>-<Toolchain-Version>\n\n\nAdditional details are available on EasyBuild website:\n\n\n\n\nEasyBuild homepage\n\n\nEasyBuild documentation\n\n\nWhat is EasyBuild?\n\n\nToolchains\n\n\nEasyConfig files\n\n\nList of supported software packages\n\n\n\n\nInstalling Easybuild\n\n\nYou probably want the latest version of Easybuild so we are going here to install it following \nthe official instructions\n.\n\n\nAdd the following entries to your \n~/.bashrc\n:\n\n\nexport EASYBUILD_PREFIX=$HOME/.local/easybuild\nexport EASYBUILD_MODULES_TOOL=Lmod\nexport EASYBUILD_MODULE_NAMING_SCHEME=CategorizedModuleNamingScheme\n# Use the below variable to run:\n#    module use $LOCAL_MODULES\n#    module load tools/EasyBuild\nexport LOCAL_MODULES=${EASYBUILD_PREFIX}/modules/all\n\n\n\n\nThen source this file to expose the environment variables:\n\n\n$> source ~/.bashrc\n$> echo $EASYBUILD_PREFIX\n/home/users/svarrette/.local/easybuild\n\n\n\n\nNow let's install Easybuild following the \nboostrapping procedure\n\n\n$> cd /tmp/\n# download script\ncurl -o /tmp/bootstrap_eb.py  https://raw.githubusercontent.com/hpcugent/easybuild-framework/develop/easybuild/scripts/bootstrap_eb.py\n\n# install Easybuild\n$> python /tmp/bootstrap_eb.py $EASYBUILD_PREFIX\n\n# Load it\n$> echo $MODULEPATH\n$> module use $LOCAL_MODULES\n$> echo $MODULEPATH\n$> module spider Easybuild\n$> module load tools/EasyBuild\n\n\n\n\nEasyBuild on UL HPC platform\n\n\nTo use EasyBuild on a compute node, load the EasyBuild module (if available):\n\n\n$> module avail EasyBuild\n\n------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools -------------\n    tools/EasyBuild/2.0.0\n\n------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/base -------------\n    base/EasyBuild/install-2.1.0\n\n$> module load base/EasyBuild/install-2.1.0\n\n\n\nYou can also install EasyBuild yourself with the \nbootstrap_eb.py\n script provided by EasyBuild:\n\n\n$> wget https://raw.githubusercontent.com/hpcugent/easybuild-framework/develop/easybuild/scripts/bootstrap_eb.py\n$> EASYBUILD_MODULES_TOOL=Lmod EASYBUILD_MODULE_NAMING_SCHEME=CategorizedModuleNamingScheme python bootstrap_eb.py $HOME/.local/easybuild\n$> module use $HOME/.local/easybuild/modules/all\n$> module load tools/EasyBuild\n$> echo \"export EASYBUILD_MODULE_NAMING_SCHEME=CategorizedModuleNamingScheme\" >> ~/.bashrc\n$> source ~/.bashrc\n\n\n\nThe EasyBuild command is \neb\n. Check the version you have loaded:\n\n\n$> eb --version\n\nThis is EasyBuild 2.1.0dev-r6fee583a88e99d1384314790a419c83e85f18f3d (framework: 2.1.0dev-r2aa673bb5f61cb2d65e4a3037cc2337e6df2d3e6, easyblocks: 2.1.0dev-r6fee583a88e99d1384314790a419c83e85f18f3d) on host h-cluster1-11.\n\n\n\nNote that this version number from the modules on Gaia and Chaos are a bit peculiar because this is a custom installation on the cluster.\n\n\nTo get help on the EasyBuild options, use the \n-h\n or \n-H\n option flags:\n\n\n$> eb -h\n$> eb -H\n\n\n\nBuild software using provided EasyConfig file\n\n\nIn this part, we propose to build High Performance Linpack (HPL) using EasyBuild.\nHPL is supported by EasyBuild, this means that an EasyConfig file allowing to build HPL is already provided with EasyBuild.\n\n\nGaia\n\n\nFirst, let's see which HPL are available on the cluster:\n\n\n$> module avail HPL\n\n------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools -------------\n    tools/HPL/2.0-goolf-1.4.10\n\n\n\nThen, search for available EasyConfig files with HPL in their name. The EasyConfig files are named with the \n.eb\n extension.\n\n\n$> eb -S HPL\n\n== temporary log file in case of crash /tmp/eb-p2DT7H/easybuild-ligIot.log\n== Searching (case-insensitive) for 'HPL' in /opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs\nCFGS1=/opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs/h/HPL\n * $CFGS1/HPL-2.0-cgmpolf-1.1.6.eb\n * $CFGS1/HPL-2.0-cgmvolf-1.1.12rc1.eb\n * $CFGS1/HPL-2.0-cgmvolf-1.2.7.eb\n * $CFGS1/HPL-2.0-cgoolf-1.1.7.eb\n * $CFGS1/HPL-2.0-foss-2014b.eb\n * $CFGS1/HPL-2.0-goalf-1.1.0-no-OFED.eb\n * $CFGS1/HPL-2.0-goolf-1.4.10.eb\n * $CFGS1/HPL-2.0-goolf-1.5.16.eb\n * $CFGS1/HPL-2.0-ictce-4.0.6.eb\n * $CFGS1/HPL-2.0-ictce-5.3.0.eb\n * $CFGS1/HPL-2.0-ictce-6.0.5.eb\n * $CFGS1/HPL-2.0-ictce-6.1.5.eb\n * $CFGS1/HPL-2.0-iomkl-4.6.13.eb\n * $CFGS1/HPL-2.1-foss-2015a.eb\n * $CFGS1/HPL-2.1-gimkl-1.5.9.eb\n * $CFGS1/HPL-2.1-gmpolf-1.4.8.eb\n * $CFGS1/HPL-2.1-gmvolf-1.7.20.eb\n * $CFGS1/HPL-2.1-goolf-1.7.20.eb\n * $CFGS1/HPL-2.1-goolfc-1.4.10.eb\n * $CFGS1/HPL-2.1-goolfc-2.6.10.eb\n * $CFGS1/HPL-2.1-gpsolf-2014.12.eb\n * $CFGS1/HPL-2.1-ictce-6.3.5.eb\n * $CFGS1/HPL-2.1-ictce-7.1.2.eb\n * $CFGS1/HPL-2.1-intel-2014.10.eb\n * $CFGS1/HPL-2.1-intel-2014.11.eb\n * $CFGS1/HPL-2.1-intel-2014b.eb\n * $CFGS1/HPL-2.1-intel-2015.02.eb\n * $CFGS1/HPL-2.1-intel-2015a.eb\n * $CFGS1/HPL-2.1-intel-para-2014.12.eb\n * $CFGS1/HPL-2.1-iomkl-2015.01.eb\n * $CFGS1/HPL-2.1-iomkl-2015.02.eb\n * $CFGS1/HPL_parallel-make.patch\n== temporary log file(s) /tmp/eb-p2DT7H/easybuild-ligIot.log* have been removed.\n== temporary directory /tmp/eb-p2DT7H has been removed.\n\n\n\nIf we try to build \nHPL-2.0-goolf-1.4.10\n, nothing will be done as it is already installed on the cluster.\n\n\n$> eb HPL-2.0-goolf-1.4.10.eb\n\n== temporary log file in case of crash /tmp/eb-JKadCH/easybuild-SoXdix.log\n== tools/HPL/2.0-goolf-1.4.10 is already installed (module found), skipping\n== No easyconfigs left to be built.\n== Build succeeded for 0 out of 0\n== temporary log file(s) /tmp/eb-JKadCH/easybuild-SoXdix.log* have been removed.\n== temporary directory /tmp/eb-JKadCH has been removed.\n\n\n\nHowever the build can be forced using the \n-f\n option flag. Then this software will be re-built.\n(Tip: prefix your command with \ntime\n to know its duration)\n\n\n$> time eb HPL-2.0-goolf-1.4.10.eb -f\n\n== temporary log file in case of crash /tmp/eb-FAO8AO/easybuild-ea15Cq.log\n== processing EasyBuild easyconfig /opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs/h/HPL/HPL-2.0-goolf-1.4.10.eb\n== building and installing tools/HPL/2.0-goolf-1.4.10...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/tools/HPL/2.0-goolf-1.4.10/easybuild/easybuild-HPL-2.0-20150624.113223.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-FAO8AO/easybuild-ea15Cq.log* have been removed.\n== temporary directory /tmp/eb-FAO8AO has been removed.\n\nreal    1m10.619s\nuser    0m49.387s\nsys     0m7.828s\n\n\n\nLet's have a look at \nHPL-2.0-ictce-5.3.0\n which is not installed yet.\nWe can check if a software and its dependencies are installed using the \n-Dr\n option flag:\n\n\n$> eb HPL-2.0-ictce-5.3.0.eb -Dr\n\n== temporary log file in case of crash /tmp/eb-HlZDMR/easybuild-JbndYN.log\nDry run: printing build status of easyconfigs and dependencies\nCFGS=/opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs\n * [x] $CFGS/i/icc/icc-2013.3.163.eb (module: compiler/icc/2013.3.163)\n * [x] $CFGS/i/ifort/ifort-2013.3.163.eb (module: compiler/ifort/2013.3.163)\n * [x] $CFGS/i/iccifort/iccifort-2013.3.163.eb (module: toolchain/iccifort/2013.3.163)\n * [x] $CFGS/i/impi/impi-4.1.0.030-iccifort-2013.3.163.eb (module: mpi/impi/4.1.0.030-iccifort-2013.3.163)\n * [x] $CFGS/i/iimpi/iimpi-5.3.0.eb (module: toolchain/iimpi/5.3.0)\n * [x] $CFGS/i/imkl/imkl-11.0.3.163-iimpi-5.3.0.eb (module: numlib/imkl/11.0.3.163-iimpi-5.3.0)\n * [x] $CFGS/i/ictce/ictce-5.3.0.eb (module: toolchain/ictce/5.3.0)\n * [ ] $CFGS/h/HPL/HPL-2.0-ictce-5.3.0.eb (module: tools/HPL/2.0-ictce-5.3.0)\n== temporary log file(s) /tmp/eb-HlZDMR/easybuild-JbndYN.log* have been removed.\n== temporary directory /tmp/eb-HlZDMR has been removed.\n\n\n\nHPL-2.0-ictce-5.3.0\n is not available but all it dependencies are. Let's build it:\n\n\n$> time eb HPL-2.0-ictce-5.3.0.eb\n\n== temporary log file in case of crash /tmp/eb-UFlEv7/easybuild-uVbm24.log\n== processing EasyBuild easyconfig /opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs/h/HPL/HPL-2.0-ictce-5.3.0.eb\n== building and installing tools/HPL/2.0-ictce-5.3.0...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/tools/HPL/2.0-ictce-5.3.0/easybuild/easybuild-HPL-2.0-20150624.113547.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-UFlEv7/easybuild-uVbm24.log* have been removed.\n== temporary directory /tmp/eb-UFlEv7 has been removed.\n\nreal    1m25.849s\nuser    0m49.039s\nsys     0m10.961s\n\n\n\nTo see the newly installed modules, you need to add the path where they were installed to the MODULEPATH. On the cluster you have to use the \nmodule use\n command:\n\n\n$> module use $HOME/.local/easybuild/modules/all/\n\n\n\nCheck which HPL modules are available now:\n\n\n$> module avail HPL\n\n------------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all -------------\n    tools/HPL/2.0-goolf-1.4.10    tools/HPL/2.0-ictce-5.3.0 (D)\n\n---------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools ----------------\n    tools/HPL/2.0-goolf-1.4.10\n\n\n\nThe two newly-built versions of HPL are now available for your user. You can use them with the usually \nmodule load\n command.\n\n\nIris\n\n\nLet's search for available EasyConfig files with HPL in their name. The EasyConfig files are named with the \n.eb\n extension.\n\n\n$> eb -S HPL\n\n    CFGS1=/home/users/sdiehl/.local/easybuild/software/tools/EasyBuild/3.2.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.2.1-py2.7.egg/easybuild/easyconfigs\n     * $CFGS1/h/HPL/HPL-2.0-foss-2014b.eb\n     * $CFGS1/h/HPL/HPL-2.0-goolf-1.4.10.eb\n     * $CFGS1/h/HPL/HPL-2.0-goolf-1.5.16.eb\n     * $CFGS1/h/HPL/HPL-2.0-ictce-5.3.0.eb\n     * $CFGS1/h/HPL/HPL-2.0-ictce-6.1.5.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayCCE-2015.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayCCE-2015.11.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayGNU-2015.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayGNU-2015.11.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayGNU-2016.03.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayGNU-2016.04.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayGNU-2016.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayIntel-2015.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayIntel-2015.11.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayIntel-2016.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2015.05.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2015a.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2015b.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2016.04.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2016.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2016a.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2016b.eb\n     * $CFGS1/h/HPL/HPL-2.1-gimkl-2.11.5.eb\n     * $CFGS1/h/HPL/HPL-2.1-gmpolf-2016a.eb\n     * $CFGS1/h/HPL/HPL-2.1-gmvolf-1.7.20.eb\n     * $CFGS1/h/HPL/HPL-2.1-gmvolf-2016a.eb\n     * $CFGS1/h/HPL/HPL-2.1-goolf-1.7.20.eb\n     * $CFGS1/h/HPL/HPL-2.1-ictce-7.1.2.eb\n     * $CFGS1/h/HPL/HPL-2.1-ictce-7.3.5.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2014.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2014.10.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2014.11.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2014b.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2015.02.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2015.08.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2015a.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2015b.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.00.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.01.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.02-GCC-4.9.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.02-GCC-5.3.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.03-GCC-4.9.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.03-GCC-5.3.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.03-GCC-5.4.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016a.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016b.eb\n     * $CFGS1/h/HPL/HPL-2.1-iomkl-2015.01.eb\n     * $CFGS1/h/HPL/HPL-2.1-iomkl-2015.02.eb\n     * $CFGS1/h/HPL/HPL-2.1-iomkl-2015.03.eb\n     * $CFGS1/h/HPL/HPL-2.1-iomkl-2016.07.eb\n     * $CFGS1/h/HPL/HPL-2.1-pomkl-2016.03.eb\n     * $CFGS1/h/HPL/HPL-2.1-pomkl-2016.04.eb\n     * $CFGS1/h/HPL/HPL-2.1-pomkl-2016.09.eb\n     * $CFGS1/h/HPL/HPL-2.1_LINKER-ld.patch\n     * $CFGS1/h/HPL/HPL-2.2-foss-2016.07.eb\n     * $CFGS1/h/HPL/HPL-2.2-foss-2016.09.eb\n     * $CFGS1/h/HPL/HPL-2.2-foss-2017a.eb\n     * $CFGS1/h/HPL/HPL-2.2-goolfc-2016.08.eb\n     * $CFGS1/h/HPL/HPL-2.2-goolfc-2016.10.eb\n     * $CFGS1/h/HPL/HPL-2.2-intel-2017.00.eb\n     * $CFGS1/h/HPL/HPL-2.2-intel-2017.01.eb\n     * $CFGS1/h/HPL/HPL-2.2-intel-2017.02.eb\n     * $CFGS1/h/HPL/HPL-2.2-intel-2017a.eb\n     * $CFGS1/h/HPL/HPL-2.2-intelcuda-2016.10.eb\n     * $CFGS1/h/HPL/HPL-2.2-iomkl-2016.09-GCC-4.9.3-2.25.eb\n     * $CFGS1/h/HPL/HPL-2.2-iomkl-2016.09-GCC-5.4.0-2.26.eb\n     * $CFGS1/h/HPL/HPL-2.2-iomkl-2017.01.eb\n     * $CFGS1/h/HPL/HPL-2.2-iomkl-2017a.eb\n     * $CFGS1/h/HPL/HPL-2.2-pomkl-2016.09.eb\n     * $CFGS1/h/HPL/HPL_parallel-make.patch\n\n    Note: 15 matching archived easyconfig(s) found, use --consider-archived-easyconfigs to see them\n\n\n\nLet's have a look at \nHPL-2.2-intel-2017a\n which is not installed yet.\nWe can check if a software and its dependencies are installed using the \n-Dr\n option flag:\n\n\n$> eb HPL-2.2-intel-2017a.eb -Dr\n\n    == temporary log file in case of crash /tmp/eb-K1VnEh/easybuild-4C6ZpN.log\n    Dry run: printing build status of easyconfigs and dependencies\n    CFGS=/home/users/sdiehl/.local/easybuild/software/tools/EasyBuild/3.2.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.2.1-py2.7.egg/easybuild/easyconfigs\n     * [x] $CFGS/m/M4/M4-1.4.17.eb (module: devel/M4/1.4.17)\n     * [x] $CFGS/b/Bison/Bison-3.0.4.eb (module: lang/Bison/3.0.4)\n     * [x] $CFGS/f/flex/flex-2.6.0.eb (module: lang/flex/2.6.0)\n     * [x] $CFGS/z/zlib/zlib-1.2.8.eb (module: lib/zlib/1.2.8)\n     * [x] $CFGS/b/binutils/binutils-2.27.eb (module: tools/binutils/2.27)\n     * [x] $CFGS/g/GCCcore/GCCcore-6.3.0.eb (module: compiler/GCCcore/6.3.0)\n     * [x] $CFGS/m/M4/M4-1.4.18-GCCcore-6.3.0.eb (module: devel/M4/1.4.18-GCCcore-6.3.0)\n     * [x] $CFGS/z/zlib/zlib-1.2.11-GCCcore-6.3.0.eb (module: lib/zlib/1.2.11-GCCcore-6.3.0)\n     * [x] $CFGS/h/help2man/help2man-1.47.4-GCCcore-6.3.0.eb (module: tools/help2man/1.47.4-GCCcore-6.3.0)\n     * [x] $CFGS/b/Bison/Bison-3.0.4-GCCcore-6.3.0.eb (module: lang/Bison/3.0.4-GCCcore-6.3.0)\n     * [x] $CFGS/f/flex/flex-2.6.3-GCCcore-6.3.0.eb (module: lang/flex/2.6.3-GCCcore-6.3.0)\n     * [x] $CFGS/b/binutils/binutils-2.27-GCCcore-6.3.0.eb (module: tools/binutils/2.27-GCCcore-6.3.0)\n     * [x] $CFGS/i/icc/icc-2017.1.132-GCC-6.3.0-2.27.eb (module: compiler/icc/2017.1.132-GCC-6.3.0-2.27)\n     * [x] $CFGS/i/ifort/ifort-2017.1.132-GCC-6.3.0-2.27.eb (module: compiler/ifort/2017.1.132-GCC-6.3.0-2.27)\n     * [x] $CFGS/i/iccifort/iccifort-2017.1.132-GCC-6.3.0-2.27.eb (module: toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27)\n     * [x] $CFGS/i/impi/impi-2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27.eb (module: mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27)\n     * [x] $CFGS/i/iimpi/iimpi-2017a.eb (module: toolchain/iimpi/2017a)\n     * [x] $CFGS/i/imkl/imkl-2017.1.132-iimpi-2017a.eb (module: numlib/imkl/2017.1.132-iimpi-2017a)\n     * [x] $CFGS/i/intel/intel-2017a.eb (module: toolchain/intel/2017a)\n     * [ ] $CFGS/h/HPL/HPL-2.2-intel-2017a.eb (module: tools/HPL/2.2-intel-2017a)\n    == Temporary log file(s) /tmp/eb-K1VnEh/easybuild-4C6ZpN.log* have been removed.\n    == Temporary directory /tmp/eb-K1VnEh has been removed.\n\n\n\nHPL-2.2-intel-2017a\n is not available but all it dependencies are. Let's build it:\n\n\n$> time eb HPL-2.2-intel-2017a.eb\n\n    == temporary log file in case of crash /tmp/eb-152mYB/easybuild-myA4bD.log\n    == processing EasyBuild easyconfig /home/users/sdiehl/.local/easybuild/software/tools/EasyBuild/3.2.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.2.1-py2.7.egg/easybuild/easyconfigs/h/HPL/HPL-2.2-intel-2017a.eb\n    == building and installing tools/HPL/2.2-intel-2017a...\n    == fetching files...\n    == creating build dir, resetting environment...\n    == unpacking...\n    == patching...\n    == preparing...\n    == configuring...\n    == building...\n    == testing...\n    == installing...\n    == taking care of extensions...\n    == postprocessing...\n    == sanity checking...\n    == cleaning up...\n    == creating module...\n    == permissions...\n    == packaging...\n    == COMPLETED: Installation ended successfully\n    == Results of the build can be found in the log file(s) /home/users/sdiehl/.local/easybuild/software/tools/HPL/2.2-intel-2017a/easybuild/easybuild-HPL-2.2-20170609.155430.log\n    == Build succeeded for 1 out of 1\n    == Temporary log file(s) /tmp/eb-152mYB/easybuild-myA4bD.log* have been removed.\n    == Temporary directory /tmp/eb-152mYB has been removed.\n\n    real    0m54.624s\n    user    0m14.651s\n    sys 0m21.476s\n\n\n\nTo see the newly installed modules, you need to add the path where they were installed to the MODULEPATH. On the cluster you have to use the \nmodule use\n command:\n\n\n$> module use $HOME/.local/easybuild/modules/all\n\n\n\nCheck which HPL modules are available now:\n\n\n$> module avail HPL\n\n    ---------------------- /home/users/sdiehl/.local/easybuild/modules/all ----------------------\n       tools/HPL/2.2-intel-2017a\n\n\n\nThe newly-built version of HPL is now available for your user. You can use them with the usually \nmodule load\n command.\n\n\nAmending an existing EasyConfig file (Gaia only)\n\n\nIt is possible to amend existing EasyConfig file to build software with slightly different parameters.\n\n\nAs a example, we are going to build the lastest version of HPL (2.1) with ICTCE toolchain. We use the \n--try-software-version\n option flag to overide the HPL version.\n\n\n$> time eb HPL-2.0-ictce-5.3.0.eb --try-software-version=2.1\n\n== temporary log file in case of crash /tmp/eb-ocChbK/easybuild-liMmlk.log\n== processing EasyBuild easyconfig /tmp/eb-ocChbK/tweaked_easyconfigs/HPL-2.1-ictce-5.3.0.eb\n== building and installing tools/HPL/2.1-ictce-5.3.0...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/tools/HPL/2.1-ictce-5.3.0/easybuild/easybuild-HPL-2.1-20150624.114243.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-ocChbK/easybuild-liMmlk.log* have been removed.\n== temporary directory /tmp/eb-ocChbK has been removed.\n\nreal    1m24.933s\nuser    0m53.167s\nsys     0m11.533s\n\n$> module avail HPL\n\n------------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all -------------\n    tools/HPL/2.0-goolf-1.4.10    tools/HPL/2.0-ictce-5.3.0    tools/HPL/2.1-ictce-5.3.0 (D)\n\n---------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools ----------------\n    tools/HPL/2.0-goolf-1.4.10\n\n\n\nWe obtained HPL 2.1 without writing any EasyConfig file.\n\n\nIMPORTANT\n: LMod cache the modules available such that it may append that the \nmodule avail HPL\n command \ndoes not\n report the newly created 2.1 version. In that case, you can use the following option:\n\n\n$> module --ignore-cache avail HPL\n\n\n\nThere are multiple ways to amend a EasyConfig file. Check the \n--try-*\n option flags for all the possibilities.\n\n\nBuild software using your own EasyConfig file (Gaia only)\n\n\nFor this example, we create an EasyConfig file to build GZip 1.4 with the GOOLF toolchain.\nOpen your favorite editor and create a file named \ngzip-1.4-goolf-1.4.10.eb\n with the following content:\n\n\neasyblock = 'ConfigureMake'\n\nname = 'gzip'\nversion = '1.4'\n\nhomepage = 'http://www.gnu.org/software/gzip/'\ndescription = \"gzip (GNU zip) is a popular data compression program as a replacement for compress\"\n\n# use the GOOLF toolchain\ntoolchain = {'name': 'goolf', 'version': '1.4.10'}\n\n# specify that GCC compiler should be used to build gzip\npreconfigopts = \"CC='gcc'\"\n\n# source tarball filename\nsources = ['%s-%s.tar.gz'%(name,version)]\n\n# download location for source files\nsource_urls = ['http://ftpmirror.gnu.org/gzip']\n\n# make sure the gzip and gunzip binaries are available after installation\nsanity_check_paths = {\n                      'files': [\"bin/gunzip\", \"bin/gzip\"],\n                      'dirs': []\n                     }\n\n# run 'gzip -h' and 'gzip --version' after installation\nsanity_check_commands = [True, ('gzip', '--version')]\n\n\n\nThis is a simple EasyConfig. Most of the fields are self-descriptive. No build method is explicitely defined, so it uses by default the standard \nconfigure/make/make install\n approach.\n\n\nLet's build GZip with this EasyConfig file:\n\n\n$> time eb gzip-1.4-goolf-1.4.10.eb\n\n== temporary log file in case of crash /tmp/eb-hiyyN1/easybuild-ynLsHC.log\n== processing EasyBuild easyconfig /mnt/nfs/users/homedirs/mschmitt/gzip-1.4-goolf-1.4.10.eb\n== building and installing base/gzip/1.4-goolf-1.4.10...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/base/gzip/1.4-goolf-1.4.10/easybuild/easybuild-gzip-1.4-20150624.114745.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-hiyyN1/easybuild-ynLsHC.log* have been removed.\n== temporary directory /tmp/eb-hiyyN1 has been removed.\n\nreal    1m39.982s\nuser    0m52.743s\nsys     0m11.297s\n\n\n\nWe can now check that our version of GZip is available via the modules:\n\n\n$> module avail gzip\n\n--------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all ---------\n    base/gzip/1.4-goolf-1.4.10\n\n\n\nTo go further\n\n\n\n\nEasyBuild homepage\n\n\nEasyBuild documentation\n\n\nGetting started\n\n\nUsing EasyBuild\n\n\nStep-by-step guide",
            "title": "Easybuild"
        },
        {
            "location": "/advanced/EasyBuild/#building-custom-software-with-easybuild-on-ul-hpc-platform",
            "text": "The objective of this tutorial is to show how  EasyBuild  can be used to ease, automate and script the build of software on the UL HPC platforms.  Two use-cases are considered. First, we are going to build software that are supported by EasyBuild. In a second time, we will see through a simple example how to add support for a new software in EasyBuild.  The benefit of using EasyBuild for your builds is that it allows automated and reproducable build of software. Once a build has been made, the build script (via the  EasyConfig file ) or the installed software (via the  module file ) can be shared with other users.  Before starting this tutorial, ensure you are able to  connect to the chaos and gaia cluster . For all your compilation with Easybuild, you must work on a computing node:    Gaia  (access-gaia)$> oarsub -I -l core=1,walltime=4    Iris  (access-iris)$> srun -p interactive --qos qos-interactive -t 0-4:0:0 --pty bash    The latest version of this tutorial is available on Github .",
            "title": "Building [custom] software with EasyBuild on UL HPC platform"
        },
        {
            "location": "/advanced/EasyBuild/#short-introduction-to-easybuild",
            "text": "EasyBuild is a tool that allows to perform automated and reproducible compilation and installation of software. A large number of scientific software are supported (649 software packages in the last release).  All builds and installations are performed at user level, so you don't need the admin rights.\nThe software are installed in your home directory (by default in  $HOME/.local/easybuild/software/ ) and a module file is generated (by default in  $HOME/.local/easybuild/modules/ ) to use the software.  EasyBuild relies on two main concepts:  Toolchains  and  EasyConfig file .  A  toolchain  corresponds to a compiler and a set of libraries which are commonly used to build a software. The two main toolchains frequently used on the UL HPC platform are the GOOLF and the ICTCE toolchains. GOOLF is based on the GCC compiler and on open-source libraries (OpenMPI, OpenBLAS, etc.). ICTCE is based on the Intel compiler and on Intel libraries (Intel MPI, Intel Math Kernel Library, etc.).  An  EasyConfig file  is a simple text file that describes the build process of a software. For most software that uses standard procedure (like  configure ,  make  and  make install ), this file is very simple. Many EasyConfig files are already provided with EasyBuild.  By default, EasyConfig files and generated modules are named using the following convention: <Software-Name>-<Software-Version>-<Toolchain-Name>-<Toolchain-Version>  On the cluster however, for the module names we use a custom naming convention that is explained in the RESIF tutorial: <Software-Class>/<Software-Name>/<Software-Version>-<Toolchain-Name>-<Toolchain-Version>  Additional details are available on EasyBuild website:   EasyBuild homepage  EasyBuild documentation  What is EasyBuild?  Toolchains  EasyConfig files  List of supported software packages",
            "title": "Short introduction to EasyBuild"
        },
        {
            "location": "/advanced/EasyBuild/#installing-easybuild",
            "text": "You probably want the latest version of Easybuild so we are going here to install it following  the official instructions .  Add the following entries to your  ~/.bashrc :  export EASYBUILD_PREFIX=$HOME/.local/easybuild\nexport EASYBUILD_MODULES_TOOL=Lmod\nexport EASYBUILD_MODULE_NAMING_SCHEME=CategorizedModuleNamingScheme\n# Use the below variable to run:\n#    module use $LOCAL_MODULES\n#    module load tools/EasyBuild\nexport LOCAL_MODULES=${EASYBUILD_PREFIX}/modules/all  Then source this file to expose the environment variables:  $> source ~/.bashrc\n$> echo $EASYBUILD_PREFIX\n/home/users/svarrette/.local/easybuild  Now let's install Easybuild following the  boostrapping procedure  $> cd /tmp/\n# download script\ncurl -o /tmp/bootstrap_eb.py  https://raw.githubusercontent.com/hpcugent/easybuild-framework/develop/easybuild/scripts/bootstrap_eb.py\n\n# install Easybuild\n$> python /tmp/bootstrap_eb.py $EASYBUILD_PREFIX\n\n# Load it\n$> echo $MODULEPATH\n$> module use $LOCAL_MODULES\n$> echo $MODULEPATH\n$> module spider Easybuild\n$> module load tools/EasyBuild",
            "title": "Installing Easybuild"
        },
        {
            "location": "/advanced/EasyBuild/#easybuild-on-ul-hpc-platform",
            "text": "To use EasyBuild on a compute node, load the EasyBuild module (if available):  $> module avail EasyBuild\n\n------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools -------------\n    tools/EasyBuild/2.0.0\n\n------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/base -------------\n    base/EasyBuild/install-2.1.0\n\n$> module load base/EasyBuild/install-2.1.0  You can also install EasyBuild yourself with the  bootstrap_eb.py  script provided by EasyBuild:  $> wget https://raw.githubusercontent.com/hpcugent/easybuild-framework/develop/easybuild/scripts/bootstrap_eb.py\n$> EASYBUILD_MODULES_TOOL=Lmod EASYBUILD_MODULE_NAMING_SCHEME=CategorizedModuleNamingScheme python bootstrap_eb.py $HOME/.local/easybuild\n$> module use $HOME/.local/easybuild/modules/all\n$> module load tools/EasyBuild\n$> echo \"export EASYBUILD_MODULE_NAMING_SCHEME=CategorizedModuleNamingScheme\" >> ~/.bashrc\n$> source ~/.bashrc  The EasyBuild command is  eb . Check the version you have loaded:  $> eb --version\n\nThis is EasyBuild 2.1.0dev-r6fee583a88e99d1384314790a419c83e85f18f3d (framework: 2.1.0dev-r2aa673bb5f61cb2d65e4a3037cc2337e6df2d3e6, easyblocks: 2.1.0dev-r6fee583a88e99d1384314790a419c83e85f18f3d) on host h-cluster1-11.  Note that this version number from the modules on Gaia and Chaos are a bit peculiar because this is a custom installation on the cluster.  To get help on the EasyBuild options, use the  -h  or  -H  option flags:  $> eb -h\n$> eb -H",
            "title": "EasyBuild on UL HPC platform"
        },
        {
            "location": "/advanced/EasyBuild/#build-software-using-provided-easyconfig-file",
            "text": "In this part, we propose to build High Performance Linpack (HPL) using EasyBuild.\nHPL is supported by EasyBuild, this means that an EasyConfig file allowing to build HPL is already provided with EasyBuild.",
            "title": "Build software using provided EasyConfig file"
        },
        {
            "location": "/advanced/EasyBuild/#gaia",
            "text": "First, let's see which HPL are available on the cluster:  $> module avail HPL\n\n------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools -------------\n    tools/HPL/2.0-goolf-1.4.10  Then, search for available EasyConfig files with HPL in their name. The EasyConfig files are named with the  .eb  extension.  $> eb -S HPL\n\n== temporary log file in case of crash /tmp/eb-p2DT7H/easybuild-ligIot.log\n== Searching (case-insensitive) for 'HPL' in /opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs\nCFGS1=/opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs/h/HPL\n * $CFGS1/HPL-2.0-cgmpolf-1.1.6.eb\n * $CFGS1/HPL-2.0-cgmvolf-1.1.12rc1.eb\n * $CFGS1/HPL-2.0-cgmvolf-1.2.7.eb\n * $CFGS1/HPL-2.0-cgoolf-1.1.7.eb\n * $CFGS1/HPL-2.0-foss-2014b.eb\n * $CFGS1/HPL-2.0-goalf-1.1.0-no-OFED.eb\n * $CFGS1/HPL-2.0-goolf-1.4.10.eb\n * $CFGS1/HPL-2.0-goolf-1.5.16.eb\n * $CFGS1/HPL-2.0-ictce-4.0.6.eb\n * $CFGS1/HPL-2.0-ictce-5.3.0.eb\n * $CFGS1/HPL-2.0-ictce-6.0.5.eb\n * $CFGS1/HPL-2.0-ictce-6.1.5.eb\n * $CFGS1/HPL-2.0-iomkl-4.6.13.eb\n * $CFGS1/HPL-2.1-foss-2015a.eb\n * $CFGS1/HPL-2.1-gimkl-1.5.9.eb\n * $CFGS1/HPL-2.1-gmpolf-1.4.8.eb\n * $CFGS1/HPL-2.1-gmvolf-1.7.20.eb\n * $CFGS1/HPL-2.1-goolf-1.7.20.eb\n * $CFGS1/HPL-2.1-goolfc-1.4.10.eb\n * $CFGS1/HPL-2.1-goolfc-2.6.10.eb\n * $CFGS1/HPL-2.1-gpsolf-2014.12.eb\n * $CFGS1/HPL-2.1-ictce-6.3.5.eb\n * $CFGS1/HPL-2.1-ictce-7.1.2.eb\n * $CFGS1/HPL-2.1-intel-2014.10.eb\n * $CFGS1/HPL-2.1-intel-2014.11.eb\n * $CFGS1/HPL-2.1-intel-2014b.eb\n * $CFGS1/HPL-2.1-intel-2015.02.eb\n * $CFGS1/HPL-2.1-intel-2015a.eb\n * $CFGS1/HPL-2.1-intel-para-2014.12.eb\n * $CFGS1/HPL-2.1-iomkl-2015.01.eb\n * $CFGS1/HPL-2.1-iomkl-2015.02.eb\n * $CFGS1/HPL_parallel-make.patch\n== temporary log file(s) /tmp/eb-p2DT7H/easybuild-ligIot.log* have been removed.\n== temporary directory /tmp/eb-p2DT7H has been removed.  If we try to build  HPL-2.0-goolf-1.4.10 , nothing will be done as it is already installed on the cluster.  $> eb HPL-2.0-goolf-1.4.10.eb\n\n== temporary log file in case of crash /tmp/eb-JKadCH/easybuild-SoXdix.log\n== tools/HPL/2.0-goolf-1.4.10 is already installed (module found), skipping\n== No easyconfigs left to be built.\n== Build succeeded for 0 out of 0\n== temporary log file(s) /tmp/eb-JKadCH/easybuild-SoXdix.log* have been removed.\n== temporary directory /tmp/eb-JKadCH has been removed.  However the build can be forced using the  -f  option flag. Then this software will be re-built.\n(Tip: prefix your command with  time  to know its duration)  $> time eb HPL-2.0-goolf-1.4.10.eb -f\n\n== temporary log file in case of crash /tmp/eb-FAO8AO/easybuild-ea15Cq.log\n== processing EasyBuild easyconfig /opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs/h/HPL/HPL-2.0-goolf-1.4.10.eb\n== building and installing tools/HPL/2.0-goolf-1.4.10...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/tools/HPL/2.0-goolf-1.4.10/easybuild/easybuild-HPL-2.0-20150624.113223.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-FAO8AO/easybuild-ea15Cq.log* have been removed.\n== temporary directory /tmp/eb-FAO8AO has been removed.\n\nreal    1m10.619s\nuser    0m49.387s\nsys     0m7.828s  Let's have a look at  HPL-2.0-ictce-5.3.0  which is not installed yet.\nWe can check if a software and its dependencies are installed using the  -Dr  option flag:  $> eb HPL-2.0-ictce-5.3.0.eb -Dr\n\n== temporary log file in case of crash /tmp/eb-HlZDMR/easybuild-JbndYN.log\nDry run: printing build status of easyconfigs and dependencies\nCFGS=/opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs\n * [x] $CFGS/i/icc/icc-2013.3.163.eb (module: compiler/icc/2013.3.163)\n * [x] $CFGS/i/ifort/ifort-2013.3.163.eb (module: compiler/ifort/2013.3.163)\n * [x] $CFGS/i/iccifort/iccifort-2013.3.163.eb (module: toolchain/iccifort/2013.3.163)\n * [x] $CFGS/i/impi/impi-4.1.0.030-iccifort-2013.3.163.eb (module: mpi/impi/4.1.0.030-iccifort-2013.3.163)\n * [x] $CFGS/i/iimpi/iimpi-5.3.0.eb (module: toolchain/iimpi/5.3.0)\n * [x] $CFGS/i/imkl/imkl-11.0.3.163-iimpi-5.3.0.eb (module: numlib/imkl/11.0.3.163-iimpi-5.3.0)\n * [x] $CFGS/i/ictce/ictce-5.3.0.eb (module: toolchain/ictce/5.3.0)\n * [ ] $CFGS/h/HPL/HPL-2.0-ictce-5.3.0.eb (module: tools/HPL/2.0-ictce-5.3.0)\n== temporary log file(s) /tmp/eb-HlZDMR/easybuild-JbndYN.log* have been removed.\n== temporary directory /tmp/eb-HlZDMR has been removed.  HPL-2.0-ictce-5.3.0  is not available but all it dependencies are. Let's build it:  $> time eb HPL-2.0-ictce-5.3.0.eb\n\n== temporary log file in case of crash /tmp/eb-UFlEv7/easybuild-uVbm24.log\n== processing EasyBuild easyconfig /opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs/h/HPL/HPL-2.0-ictce-5.3.0.eb\n== building and installing tools/HPL/2.0-ictce-5.3.0...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/tools/HPL/2.0-ictce-5.3.0/easybuild/easybuild-HPL-2.0-20150624.113547.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-UFlEv7/easybuild-uVbm24.log* have been removed.\n== temporary directory /tmp/eb-UFlEv7 has been removed.\n\nreal    1m25.849s\nuser    0m49.039s\nsys     0m10.961s  To see the newly installed modules, you need to add the path where they were installed to the MODULEPATH. On the cluster you have to use the  module use  command:  $> module use $HOME/.local/easybuild/modules/all/  Check which HPL modules are available now:  $> module avail HPL\n\n------------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all -------------\n    tools/HPL/2.0-goolf-1.4.10    tools/HPL/2.0-ictce-5.3.0 (D)\n\n---------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools ----------------\n    tools/HPL/2.0-goolf-1.4.10  The two newly-built versions of HPL are now available for your user. You can use them with the usually  module load  command.",
            "title": "Gaia"
        },
        {
            "location": "/advanced/EasyBuild/#iris",
            "text": "Let's search for available EasyConfig files with HPL in their name. The EasyConfig files are named with the  .eb  extension.  $> eb -S HPL\n\n    CFGS1=/home/users/sdiehl/.local/easybuild/software/tools/EasyBuild/3.2.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.2.1-py2.7.egg/easybuild/easyconfigs\n     * $CFGS1/h/HPL/HPL-2.0-foss-2014b.eb\n     * $CFGS1/h/HPL/HPL-2.0-goolf-1.4.10.eb\n     * $CFGS1/h/HPL/HPL-2.0-goolf-1.5.16.eb\n     * $CFGS1/h/HPL/HPL-2.0-ictce-5.3.0.eb\n     * $CFGS1/h/HPL/HPL-2.0-ictce-6.1.5.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayCCE-2015.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayCCE-2015.11.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayGNU-2015.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayGNU-2015.11.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayGNU-2016.03.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayGNU-2016.04.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayGNU-2016.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayIntel-2015.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayIntel-2015.11.eb\n     * $CFGS1/h/HPL/HPL-2.1-CrayIntel-2016.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2015.05.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2015a.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2015b.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2016.04.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2016.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2016a.eb\n     * $CFGS1/h/HPL/HPL-2.1-foss-2016b.eb\n     * $CFGS1/h/HPL/HPL-2.1-gimkl-2.11.5.eb\n     * $CFGS1/h/HPL/HPL-2.1-gmpolf-2016a.eb\n     * $CFGS1/h/HPL/HPL-2.1-gmvolf-1.7.20.eb\n     * $CFGS1/h/HPL/HPL-2.1-gmvolf-2016a.eb\n     * $CFGS1/h/HPL/HPL-2.1-goolf-1.7.20.eb\n     * $CFGS1/h/HPL/HPL-2.1-ictce-7.1.2.eb\n     * $CFGS1/h/HPL/HPL-2.1-ictce-7.3.5.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2014.06.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2014.10.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2014.11.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2014b.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2015.02.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2015.08.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2015a.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2015b.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.00.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.01.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.02-GCC-4.9.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.02-GCC-5.3.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.03-GCC-4.9.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.03-GCC-5.3.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016.03-GCC-5.4.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016a.eb\n     * $CFGS1/h/HPL/HPL-2.1-intel-2016b.eb\n     * $CFGS1/h/HPL/HPL-2.1-iomkl-2015.01.eb\n     * $CFGS1/h/HPL/HPL-2.1-iomkl-2015.02.eb\n     * $CFGS1/h/HPL/HPL-2.1-iomkl-2015.03.eb\n     * $CFGS1/h/HPL/HPL-2.1-iomkl-2016.07.eb\n     * $CFGS1/h/HPL/HPL-2.1-pomkl-2016.03.eb\n     * $CFGS1/h/HPL/HPL-2.1-pomkl-2016.04.eb\n     * $CFGS1/h/HPL/HPL-2.1-pomkl-2016.09.eb\n     * $CFGS1/h/HPL/HPL-2.1_LINKER-ld.patch\n     * $CFGS1/h/HPL/HPL-2.2-foss-2016.07.eb\n     * $CFGS1/h/HPL/HPL-2.2-foss-2016.09.eb\n     * $CFGS1/h/HPL/HPL-2.2-foss-2017a.eb\n     * $CFGS1/h/HPL/HPL-2.2-goolfc-2016.08.eb\n     * $CFGS1/h/HPL/HPL-2.2-goolfc-2016.10.eb\n     * $CFGS1/h/HPL/HPL-2.2-intel-2017.00.eb\n     * $CFGS1/h/HPL/HPL-2.2-intel-2017.01.eb\n     * $CFGS1/h/HPL/HPL-2.2-intel-2017.02.eb\n     * $CFGS1/h/HPL/HPL-2.2-intel-2017a.eb\n     * $CFGS1/h/HPL/HPL-2.2-intelcuda-2016.10.eb\n     * $CFGS1/h/HPL/HPL-2.2-iomkl-2016.09-GCC-4.9.3-2.25.eb\n     * $CFGS1/h/HPL/HPL-2.2-iomkl-2016.09-GCC-5.4.0-2.26.eb\n     * $CFGS1/h/HPL/HPL-2.2-iomkl-2017.01.eb\n     * $CFGS1/h/HPL/HPL-2.2-iomkl-2017a.eb\n     * $CFGS1/h/HPL/HPL-2.2-pomkl-2016.09.eb\n     * $CFGS1/h/HPL/HPL_parallel-make.patch\n\n    Note: 15 matching archived easyconfig(s) found, use --consider-archived-easyconfigs to see them  Let's have a look at  HPL-2.2-intel-2017a  which is not installed yet.\nWe can check if a software and its dependencies are installed using the  -Dr  option flag:  $> eb HPL-2.2-intel-2017a.eb -Dr\n\n    == temporary log file in case of crash /tmp/eb-K1VnEh/easybuild-4C6ZpN.log\n    Dry run: printing build status of easyconfigs and dependencies\n    CFGS=/home/users/sdiehl/.local/easybuild/software/tools/EasyBuild/3.2.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.2.1-py2.7.egg/easybuild/easyconfigs\n     * [x] $CFGS/m/M4/M4-1.4.17.eb (module: devel/M4/1.4.17)\n     * [x] $CFGS/b/Bison/Bison-3.0.4.eb (module: lang/Bison/3.0.4)\n     * [x] $CFGS/f/flex/flex-2.6.0.eb (module: lang/flex/2.6.0)\n     * [x] $CFGS/z/zlib/zlib-1.2.8.eb (module: lib/zlib/1.2.8)\n     * [x] $CFGS/b/binutils/binutils-2.27.eb (module: tools/binutils/2.27)\n     * [x] $CFGS/g/GCCcore/GCCcore-6.3.0.eb (module: compiler/GCCcore/6.3.0)\n     * [x] $CFGS/m/M4/M4-1.4.18-GCCcore-6.3.0.eb (module: devel/M4/1.4.18-GCCcore-6.3.0)\n     * [x] $CFGS/z/zlib/zlib-1.2.11-GCCcore-6.3.0.eb (module: lib/zlib/1.2.11-GCCcore-6.3.0)\n     * [x] $CFGS/h/help2man/help2man-1.47.4-GCCcore-6.3.0.eb (module: tools/help2man/1.47.4-GCCcore-6.3.0)\n     * [x] $CFGS/b/Bison/Bison-3.0.4-GCCcore-6.3.0.eb (module: lang/Bison/3.0.4-GCCcore-6.3.0)\n     * [x] $CFGS/f/flex/flex-2.6.3-GCCcore-6.3.0.eb (module: lang/flex/2.6.3-GCCcore-6.3.0)\n     * [x] $CFGS/b/binutils/binutils-2.27-GCCcore-6.3.0.eb (module: tools/binutils/2.27-GCCcore-6.3.0)\n     * [x] $CFGS/i/icc/icc-2017.1.132-GCC-6.3.0-2.27.eb (module: compiler/icc/2017.1.132-GCC-6.3.0-2.27)\n     * [x] $CFGS/i/ifort/ifort-2017.1.132-GCC-6.3.0-2.27.eb (module: compiler/ifort/2017.1.132-GCC-6.3.0-2.27)\n     * [x] $CFGS/i/iccifort/iccifort-2017.1.132-GCC-6.3.0-2.27.eb (module: toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27)\n     * [x] $CFGS/i/impi/impi-2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27.eb (module: mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27)\n     * [x] $CFGS/i/iimpi/iimpi-2017a.eb (module: toolchain/iimpi/2017a)\n     * [x] $CFGS/i/imkl/imkl-2017.1.132-iimpi-2017a.eb (module: numlib/imkl/2017.1.132-iimpi-2017a)\n     * [x] $CFGS/i/intel/intel-2017a.eb (module: toolchain/intel/2017a)\n     * [ ] $CFGS/h/HPL/HPL-2.2-intel-2017a.eb (module: tools/HPL/2.2-intel-2017a)\n    == Temporary log file(s) /tmp/eb-K1VnEh/easybuild-4C6ZpN.log* have been removed.\n    == Temporary directory /tmp/eb-K1VnEh has been removed.  HPL-2.2-intel-2017a  is not available but all it dependencies are. Let's build it:  $> time eb HPL-2.2-intel-2017a.eb\n\n    == temporary log file in case of crash /tmp/eb-152mYB/easybuild-myA4bD.log\n    == processing EasyBuild easyconfig /home/users/sdiehl/.local/easybuild/software/tools/EasyBuild/3.2.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.2.1-py2.7.egg/easybuild/easyconfigs/h/HPL/HPL-2.2-intel-2017a.eb\n    == building and installing tools/HPL/2.2-intel-2017a...\n    == fetching files...\n    == creating build dir, resetting environment...\n    == unpacking...\n    == patching...\n    == preparing...\n    == configuring...\n    == building...\n    == testing...\n    == installing...\n    == taking care of extensions...\n    == postprocessing...\n    == sanity checking...\n    == cleaning up...\n    == creating module...\n    == permissions...\n    == packaging...\n    == COMPLETED: Installation ended successfully\n    == Results of the build can be found in the log file(s) /home/users/sdiehl/.local/easybuild/software/tools/HPL/2.2-intel-2017a/easybuild/easybuild-HPL-2.2-20170609.155430.log\n    == Build succeeded for 1 out of 1\n    == Temporary log file(s) /tmp/eb-152mYB/easybuild-myA4bD.log* have been removed.\n    == Temporary directory /tmp/eb-152mYB has been removed.\n\n    real    0m54.624s\n    user    0m14.651s\n    sys 0m21.476s  To see the newly installed modules, you need to add the path where they were installed to the MODULEPATH. On the cluster you have to use the  module use  command:  $> module use $HOME/.local/easybuild/modules/all  Check which HPL modules are available now:  $> module avail HPL\n\n    ---------------------- /home/users/sdiehl/.local/easybuild/modules/all ----------------------\n       tools/HPL/2.2-intel-2017a  The newly-built version of HPL is now available for your user. You can use them with the usually  module load  command.",
            "title": "Iris"
        },
        {
            "location": "/advanced/EasyBuild/#amending-an-existing-easyconfig-file-gaia-only",
            "text": "It is possible to amend existing EasyConfig file to build software with slightly different parameters.  As a example, we are going to build the lastest version of HPL (2.1) with ICTCE toolchain. We use the  --try-software-version  option flag to overide the HPL version.  $> time eb HPL-2.0-ictce-5.3.0.eb --try-software-version=2.1\n\n== temporary log file in case of crash /tmp/eb-ocChbK/easybuild-liMmlk.log\n== processing EasyBuild easyconfig /tmp/eb-ocChbK/tweaked_easyconfigs/HPL-2.1-ictce-5.3.0.eb\n== building and installing tools/HPL/2.1-ictce-5.3.0...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/tools/HPL/2.1-ictce-5.3.0/easybuild/easybuild-HPL-2.1-20150624.114243.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-ocChbK/easybuild-liMmlk.log* have been removed.\n== temporary directory /tmp/eb-ocChbK has been removed.\n\nreal    1m24.933s\nuser    0m53.167s\nsys     0m11.533s\n\n$> module avail HPL\n\n------------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all -------------\n    tools/HPL/2.0-goolf-1.4.10    tools/HPL/2.0-ictce-5.3.0    tools/HPL/2.1-ictce-5.3.0 (D)\n\n---------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools ----------------\n    tools/HPL/2.0-goolf-1.4.10  We obtained HPL 2.1 without writing any EasyConfig file.  IMPORTANT : LMod cache the modules available such that it may append that the  module avail HPL  command  does not  report the newly created 2.1 version. In that case, you can use the following option:  $> module --ignore-cache avail HPL  There are multiple ways to amend a EasyConfig file. Check the  --try-*  option flags for all the possibilities.",
            "title": "Amending an existing EasyConfig file (Gaia only)"
        },
        {
            "location": "/advanced/EasyBuild/#build-software-using-your-own-easyconfig-file-gaia-only",
            "text": "For this example, we create an EasyConfig file to build GZip 1.4 with the GOOLF toolchain.\nOpen your favorite editor and create a file named  gzip-1.4-goolf-1.4.10.eb  with the following content:  easyblock = 'ConfigureMake'\n\nname = 'gzip'\nversion = '1.4'\n\nhomepage = 'http://www.gnu.org/software/gzip/'\ndescription = \"gzip (GNU zip) is a popular data compression program as a replacement for compress\"\n\n# use the GOOLF toolchain\ntoolchain = {'name': 'goolf', 'version': '1.4.10'}\n\n# specify that GCC compiler should be used to build gzip\npreconfigopts = \"CC='gcc'\"\n\n# source tarball filename\nsources = ['%s-%s.tar.gz'%(name,version)]\n\n# download location for source files\nsource_urls = ['http://ftpmirror.gnu.org/gzip']\n\n# make sure the gzip and gunzip binaries are available after installation\nsanity_check_paths = {\n                      'files': [\"bin/gunzip\", \"bin/gzip\"],\n                      'dirs': []\n                     }\n\n# run 'gzip -h' and 'gzip --version' after installation\nsanity_check_commands = [True, ('gzip', '--version')]  This is a simple EasyConfig. Most of the fields are self-descriptive. No build method is explicitely defined, so it uses by default the standard  configure/make/make install  approach.  Let's build GZip with this EasyConfig file:  $> time eb gzip-1.4-goolf-1.4.10.eb\n\n== temporary log file in case of crash /tmp/eb-hiyyN1/easybuild-ynLsHC.log\n== processing EasyBuild easyconfig /mnt/nfs/users/homedirs/mschmitt/gzip-1.4-goolf-1.4.10.eb\n== building and installing base/gzip/1.4-goolf-1.4.10...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/base/gzip/1.4-goolf-1.4.10/easybuild/easybuild-gzip-1.4-20150624.114745.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-hiyyN1/easybuild-ynLsHC.log* have been removed.\n== temporary directory /tmp/eb-hiyyN1 has been removed.\n\nreal    1m39.982s\nuser    0m52.743s\nsys     0m11.297s  We can now check that our version of GZip is available via the modules:  $> module avail gzip\n\n--------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all ---------\n    base/gzip/1.4-goolf-1.4.10",
            "title": "Build software using your own EasyConfig file (Gaia only)"
        },
        {
            "location": "/advanced/EasyBuild/#to-go-further",
            "text": "EasyBuild homepage  EasyBuild documentation  Getting started  Using EasyBuild  Step-by-step guide",
            "title": "To go further"
        },
        {
            "location": "/advanced/RESIF/",
            "text": "-\n- mode: markdown; mode: visual-line; fill-column: 80 -\n-\n\n\nCopyright (c) 2013-2017 UL HPC Team  \nhpc-sysadmins@uni.lu\n\n\n\n\nUL HPC Tutorial: Using RESIF to manage software modules\n\n\n \n \n \n \n \n\n\n/!\\ IMPORTANT\n: \nThis tutorial is obsolete as we are about to release the new RESIF code completely refactored\n.\nThus the below content has to be reworked with regards our new changes and is only kept for archives purposes.\n\n\n\n\nThe objective of this tutorial is to present how to interact with the software installed on the UL HPC platform, from using provided software to extending the software collection by adding new software on the platform, and also reproducing the software environment on a local workstation.\n\n\nThis course is divided in three chapters that are going through each of these use cases.\n\n\nThe \nfirst chapter\n details the architecture of the software collection and the basic tools to start using the software.\nThe \nsecond chapter\n defines the process of adding new software to the existing stack as a user using RESIF, the tool developed internally to manage the software stack on the UL HPC platform.\nThe \nthird chapter\n explains the process of reproducing the software stack present on the platform, or part of it, in a local environment.\n\n\nThe latest version of this document can be found on \nGithub\n.\n\n\nUsing the software environment available on the UL HPC platform\n\n\nBefore starting this tutorial, please make sure you are on a compute node of Gaia/Chaos and not on the access node. To get resources on a compute node, use the following command:\n\n(access)$> oarsub -I -l core=1,walltime=1:00:00\n\n(for more details about this command and the node reservation process on the clusters, please referer to the \nULHPC documentation\n.)\n\n\nUsing the software available on the UL HPC platform is done through \nLmod\n which provide a \nmodule\n command that we review in the following section.\nLmod allows us to provide a multitude of applications and libraries in multiple versions. These tools use special files named \"modules\" that define ways to manage environment variables such as PATH, LD_LIBRARY_PATH and MANPATH, enabling the easy loading and unloading of application/library profiles and their dependencies.\n\n\nmodule\n command basics and workflow\n\n\nFirstly, we activate the newest software stack:\n\n\nsource /opt/apps/resif/default_user.sh\n\n\n\nBy using \nmodule available\n (or the shorter forms \nmodule avail\n or \nmodule av\n) we can list all the software modules of the software stack:\n\n\n(node)$> module avail\n------------------- /opt/apps/devel/v1.1-20150414/core/modules/bio ----------------\nbio/ABySS/1.3.4-goolf-1.4.10-Python-2.7.3        bio/Bowtie2/2.2.2-goolf-1.4.10   (D)\nbio/ABySS/1.3.4-ictce-5.3.0-Python-2.7.3  (D)    bio/Cufflinks/2.0.2-goolf-1.4.10\n[...]\n\n\n\nNote that this command returns a lot of information since there is a lot of installed software. To reduce the output we can search for what we are interested in, for example:\n\n\n(node)$> module avail gromacs\n----------- /opt/apps/devel/v1.1-20150414/core/modules/bio --------------\nbio/GROMACS/4.6.1-ictce-5.3.0-hybrid    bio/GROMACS/4.6.1-ictce-5.3.0-mt\nbio/GROMACS/4.6.5-goolf-1.4.10-mt (D)\n[...]\n\n\n\nThis will only output the software modules from the software stack that contain \"gromacs\" (case insensitive) in their name.\n\n\nTo start using an application in the version we require, for example \nbio/GROMACS/4.6.5-goolf-1.4.10-mt\n, we are going to \nload\n its software module:\n\n(node)$> module load bio/GROMACS/4.6.5-goolf-1.4.10-mt\n\n\nWe can now use the software by running its commands (e.g. for Gromacs we can now use \nmdrun\n).\n\n\nTo check the currently loaded modules, we use the \nmodule list\n command:\n\n\n(node)$> module list\nCurrently Loaded Modules:\n    1) compiler/GCC/4.7.2             4) toolchain/gompi/1.4.10                            7) numlib/ScaLAPACK/2.0.2-gompi-1.4.10-OpenBLAS-0.2.6-LAPACK-3.4.2\n    2) system/hwloc/1.6.2-GCC-4.7.2   5) numlib/OpenBLAS/0.2.6-gompi-1.4.10-LAPACK-3.4.2   8) toolchain/goolf/1.4.10\n    3) mpi/OpenMPI/1.6.4-GCC-4.7.2    6) numlib/FFTW/3.3.3-gompi-1.4.10                    9) bio/GROMACS/4.6.5-goolf-1.4.10-mt\n\n\n\nWhen we've finished working with the application, we can remove its environment profile with \nmodule unload\n:\n\n(node)$> module unload bio/GROMACS/4.6.5-goolf-1.4.10-mt\n\n\nHowever, this will not remove its dependencies from the environment:\n\n\n(node)$> module list\nCurrently Loaded Modules:\n    1) compiler/GCC/4.7.2             4) toolchain/gompi/1.4.10                            7) numlib/ScaLAPACK/2.0.2-gompi-1.4.10-OpenBLAS-0.2.6-LAPACK-3.4.2\n    2) system/hwloc/1.6.2-GCC-4.7.2   5) numlib/OpenBLAS/0.2.6-gompi-1.4.10-LAPACK-3.4.2   8) toolchain/goolf/1.4.10\n    3) mpi/OpenMPI/1.6.4-GCC-4.7.2    6) numlib/FFTW/3.3.3-gompi-1.4.10\n\n\n\nTo remove all the loaded modules at once we use the \nmodule purge\n command:\n\n(node)$> module purge\n\n\nNext we are going to look at the hierarchical architecture of the modules.\n\n\nSoftware stack architecture\n\n\nThe upper layer of the architecture is what we call a \nsoftware set\n. It is a collection of software, for example we define a \ncore\n set that only contains commonly-used and tested software and an \nexperimental\n set that contains untested software.\nThe main goal of these categories is to provide information on the degree of support for the various software.\n\n\nInside these sets, software is named in regards to a \nnaming scheme\n which classifies the software (e.g. compilers, physics) and allows for a better structuring of results with the \nmodule avail\n command.\nThe software named using this scheme has the following format: \nsoftware_class/software_name/software_complete_version\n where\n\n\n\n\nsoftware_class\n describes the category among the following classes: [base, bio, cae, chem, compiler, data, debugger, devel, lang, lib, math, mpi, numlib, phys, system, toolchain, tools, vis]\n\n\nsoftware_name\n is the name of the software (e.g. GROMACS, MATLAB, R or ABySS)\n\n\nsoftware_complete_version\n is the full version of the software: containing the version of the software itself followed by the type and version of the main dependencies it relies on (e.g. compiler) with the following format: software_version-dependencies_versions\n\n\n\n\nThe \nmodule avail\n command will thus have the output shown below, where we can note:\n\n\n\n\nthe core software set is shown first\n\n\napplication names are prefixed with the category (class)\n\n\nfull versions including tool dependencies are shown\n\n\nthe default module for each application is marked with a \n(D)\n, thus by loading \ncompiler/GCC\n the system would in effect load \ncompiler/GCC/4.8.2\n```\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/bio -------------------------------------------------------------------------------\n   bio/ABySS/1.3.4-goolf-1.4.10-Python-2.7.3        bio/Bowtie2/2.2.2-goolf-1.4.10   (D)    bio/GROMACS/4.6.1-ictce-5.3.0-hybrid     bio/GROMACS/4.6.5-goolf-1.4.10-mt (D)\n   bio/Bowtie2/2.0.2-ictce-5.3.0                    bio/Cufflinks/2.0.2-ictce-5.3.0  (D)    bio/GROMACS/4.6.5-goolf-1.4.10-hybrid    bio/SAMtools/0.1.18-ictce-5.3.0   (D)\n\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/cae -------------------------------------------------------------------------------\n   cae/ABAQUS/6.11.1    cae/OpenFOAM/2.3.0-goolf-1.4.10    cae/OpenFOAM/2.3.0-ictce-5.3.0 (D)\n\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/chem ------------------------------------------------------------------------------\n   chem/ABINIT/7.2.1-x86_64_linux_gnu4.5                chem/GPAW/0.9.0.8965-goolf-1.4.10-Python-2.7.3        chem/QuantumESPRESSO/5.0.2-goolf-1.4.10              chem/libctl/3.2.1-goolf-1.4.10\n   chem/ASE/3.6.0.2515-ictce-5.3.0-Python-2.7.3  (D)    chem/QuantumESPRESSO/5.0.2-goolf-1.4.10-hybrid        chem/QuantumESPRESSO/5.0.2-ictce-5.3.0        (D)\n\n---------------------------------------------------------------------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/compiler ----------------------------------------------------------------------------\n   compiler/GCC/4.7.2    compiler/GCC/4.8.2 (D)    compiler/icc/2013.3.163    compiler/ifort/2013.3.163\n\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/data ------------------------------------------------------------------------------\n   data/HDF5/1.8.7-goolf-1.4.10    data/HDF5/1.8.10-patch1-goolf-1.4.10        data/netCDF/4.2-goolf-1.4.10            data/netCDF-C++/4.2-goolf-1.4.10\n   data/HDF5/1.8.9-ictce-5.3.0     data/h5utils/1.12.1-ictce-5.3.0      (D)    data/netCDF/4.2.1.1-ictce-5.3.0  (D)    data/netCDF-Fortran/4.2-ictce-5.3.0  (D)\n```\n\n\n\n\n\n\n\nAdding a software to an existing stack on UL HPC\n\n\nIn this part, we are going to show the steps to add a software that is not already provided on the platform yet is available as a module in the \nEasyBuild database\n. Using the RESIF tool is the preferred way to add new software.\nFirst of all we are going to install RESIF and then use it to add a software (bzip2 in this example).\n\n\nNote: The following commands must be executed in an OAR job.\n\n\nInstallation of RESIF\n\n\nRESIF requires the following prerequisites (already available on the platform):\n\n\n\n\nPython 2.6\n or above\n\n\npip (included in the latest Python)\n\n\ngit\n\n\n\n\nRESIF installation:\n\n\n    (node)$> pip install --install-option=\"--prefix=$HOME/.local\" resif\n\n\n\nInitially, we need to add the following paths to the environment:\n\n\n    (node)$> export PATH=$PATH:~/.local/bin\n    (node)$> export PYTHONPATH=$PYTHONPATH:~/.local/lib/python2.7/site-packages\n\n\n\nAlso,\n\n\nRESIF initialization which will download the required module sources to build new software:\n\n\n    (node)$> resif init\n\n\n\nInstallation of additional software\n\n\nFirst we need to create a file that lists the applications we want to install.\nTo search for the names of the application configuration files, we use RESIF as follow:\n\n\n    (node)$> resif search bzip2\n    [...]\n    * bzip2-1.0.6.eb\n    [...]\n\n\n\nCreate a file (we are assuming it is in the home directory), name it \nswsets.yaml\n and insert the following content:\n\n\n    mysoftware:\n      - bzip2-1.0.6.eb\n\n\n\nThis is a \nYAML\n format file that RESIF reads, the internal layout is the following:\n\n\n    software_set1_name:\n      - software1_configurationfile\n      - software2_configurationfile\n    software_set2_name:\n      - software3_configurationfile\n\n\n\nIt can list as many software, divided in as many software sets as we require to structure our installation.\n\n\nNow we install the software using \nresif build\n:\n\n\n    (node)$> resif build --installdir $HOME/.local/resif --swsets-config ~/swsets.yaml mysoftware\n\n\n\nThis will install the software using ~/.local/resif as the root of the installation.\n\n\nTo make the software modules available through the \nmodule\n command, we need to add their path:\n\n\n    (node)$> module use $HOME/.local/resif/mysoftware/modules/all\n\n\n\nNow, we can see \nbzip2\n at the very beginning of the output of the list of the software modules:\n\n\n    (node)$> module avail\n    ----- /home/users/username/.local/resif/mysoftware/core/modules/all -----\n    tools/bzip2/1.0.6\n\n\n\nThe software is installed, and we can load its profile with \nmodule load tools/bzip2/1.0.6\n.\n\n\nRESIF offers many more possibilities than this basic functionality, for more details check the \ndocumentation\n.\n\n\nReplicating the software sets in a local environment\n\n\nIn this part, we are going to create a software stack from scratch. This is especially useful if we want to work on a local workstation (e.g. a laptop) with the same tools as those available on the platform.\n\n\nWe assume that RESIF is already installed as per the instructions described \nabove\n.\nNote: RESIF internally depends on \nEasyBuild\n which is compatible with Linux/OSX but not Windows. On Windows, you can configure a Linux Virtual Machine to use RESIF and the software built using it.\n\n\nAt this point, please make sure to unset the RESIF_ROOTINSTALL environment variable.\n This is a necessary step since this variable would interfere with the creation of a new software stack (which defines a new rootinstall).\nTo do so, execute the following command:\n\n\n    unset RESIF_ROOTINSTALL\n\n\n\nDirect method\n\n\nA \nswsets.yaml\n file that describes which software we want to install needs to be created.\nAs an example, create it in your home directory with the following content:\n\n\n    core:\n      - bzip2-1.0.6.eb\n      - ABINIT-7.2.1-x86_64_linux_gnu4.5.eb\n\n\n\nWe include here only the bzip2 library and the ABINIT software as they are fast to deploy.\n\n\nWe can now install this simple software stack with the following command:\n\n\n    $> resif cleaninstall --swsets-config ~/swsets.yaml core\n\n\n\nThis will install everything using \n~/.local/resif\n as the root of the installation.\n\n\nTo use this newly installed software stack, you need to source the LOADME file inside of the rootinstall directory.\nThis path should look like this: \n~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh\n.\n\n\n    $> source ~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh\n\n\n\nThe software stack is now ready to be used with the \nmodule\n command.\n\n\nIndirect method\n\n\nIn this method we are going to create the architecture and then add software to it.\n\n\nWe create the architecture using the \nbootstrap\n subcommand:\n\n\n    $> resif bootstrap\n\n\n\nThis will create the architecture using \n~/.local/resif\n as the root of the installation.\n\n\nWe now need to make this architecture active: you need to source the LOADME file inside of the rootinstall directory.\nThis path should look like this: \n~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh\n.\n\n\n    $> source ~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh\n\n\n\nThen we need to create the file that describe the software we want to install. In the home directory, create a file named \nswsets.yaml\n and make it match the following content:\n\n\n    core:\n      - bzip2-1.0.6.eb\n      - ABINIT-7.2.1-x86_64_linux_gnu4.5.eb\n\n\n\nWe include here only the bzip2 library and the ABINIT software as they are fast to deploy.\n\n\nWe now only need to build the given software:\n\n\n    $> resif build --swsets-config ~/swsets.yaml core\n\n\n\nThis will install the software listed in the \nswsets.yaml\n file. The software stack is now ready to be used.\n\n\nTo learn more about RESIF and how to control more parameters of its usage, please refer to the \ndocumentation\n.\n\n\nTo conclude this tutorial, here is a schema that summarizes the previous parts:",
            "title": "RESIF"
        },
        {
            "location": "/advanced/RESIF/#ul-hpc-tutorial-using-resif-to-manage-software-modules",
            "text": "/!\\ IMPORTANT :  This tutorial is obsolete as we are about to release the new RESIF code completely refactored .\nThus the below content has to be reworked with regards our new changes and is only kept for archives purposes.   The objective of this tutorial is to present how to interact with the software installed on the UL HPC platform, from using provided software to extending the software collection by adding new software on the platform, and also reproducing the software environment on a local workstation.  This course is divided in three chapters that are going through each of these use cases.  The  first chapter  details the architecture of the software collection and the basic tools to start using the software.\nThe  second chapter  defines the process of adding new software to the existing stack as a user using RESIF, the tool developed internally to manage the software stack on the UL HPC platform.\nThe  third chapter  explains the process of reproducing the software stack present on the platform, or part of it, in a local environment.  The latest version of this document can be found on  Github .",
            "title": "UL HPC Tutorial: Using RESIF to manage software modules"
        },
        {
            "location": "/advanced/RESIF/#using-the-software-environment-available-on-the-ul-hpc-platform",
            "text": "Before starting this tutorial, please make sure you are on a compute node of Gaia/Chaos and not on the access node. To get resources on a compute node, use the following command: (access)$> oarsub -I -l core=1,walltime=1:00:00 \n(for more details about this command and the node reservation process on the clusters, please referer to the  ULHPC documentation .)  Using the software available on the UL HPC platform is done through  Lmod  which provide a  module  command that we review in the following section.\nLmod allows us to provide a multitude of applications and libraries in multiple versions. These tools use special files named \"modules\" that define ways to manage environment variables such as PATH, LD_LIBRARY_PATH and MANPATH, enabling the easy loading and unloading of application/library profiles and their dependencies.",
            "title": "Using the software environment available on the UL HPC platform"
        },
        {
            "location": "/advanced/RESIF/#module-command-basics-and-workflow",
            "text": "Firstly, we activate the newest software stack:  source /opt/apps/resif/default_user.sh  By using  module available  (or the shorter forms  module avail  or  module av ) we can list all the software modules of the software stack:  (node)$> module avail\n------------------- /opt/apps/devel/v1.1-20150414/core/modules/bio ----------------\nbio/ABySS/1.3.4-goolf-1.4.10-Python-2.7.3        bio/Bowtie2/2.2.2-goolf-1.4.10   (D)\nbio/ABySS/1.3.4-ictce-5.3.0-Python-2.7.3  (D)    bio/Cufflinks/2.0.2-goolf-1.4.10\n[...]  Note that this command returns a lot of information since there is a lot of installed software. To reduce the output we can search for what we are interested in, for example:  (node)$> module avail gromacs\n----------- /opt/apps/devel/v1.1-20150414/core/modules/bio --------------\nbio/GROMACS/4.6.1-ictce-5.3.0-hybrid    bio/GROMACS/4.6.1-ictce-5.3.0-mt\nbio/GROMACS/4.6.5-goolf-1.4.10-mt (D)\n[...]  This will only output the software modules from the software stack that contain \"gromacs\" (case insensitive) in their name.  To start using an application in the version we require, for example  bio/GROMACS/4.6.5-goolf-1.4.10-mt , we are going to  load  its software module: (node)$> module load bio/GROMACS/4.6.5-goolf-1.4.10-mt  We can now use the software by running its commands (e.g. for Gromacs we can now use  mdrun ).  To check the currently loaded modules, we use the  module list  command:  (node)$> module list\nCurrently Loaded Modules:\n    1) compiler/GCC/4.7.2             4) toolchain/gompi/1.4.10                            7) numlib/ScaLAPACK/2.0.2-gompi-1.4.10-OpenBLAS-0.2.6-LAPACK-3.4.2\n    2) system/hwloc/1.6.2-GCC-4.7.2   5) numlib/OpenBLAS/0.2.6-gompi-1.4.10-LAPACK-3.4.2   8) toolchain/goolf/1.4.10\n    3) mpi/OpenMPI/1.6.4-GCC-4.7.2    6) numlib/FFTW/3.3.3-gompi-1.4.10                    9) bio/GROMACS/4.6.5-goolf-1.4.10-mt  When we've finished working with the application, we can remove its environment profile with  module unload : (node)$> module unload bio/GROMACS/4.6.5-goolf-1.4.10-mt  However, this will not remove its dependencies from the environment:  (node)$> module list\nCurrently Loaded Modules:\n    1) compiler/GCC/4.7.2             4) toolchain/gompi/1.4.10                            7) numlib/ScaLAPACK/2.0.2-gompi-1.4.10-OpenBLAS-0.2.6-LAPACK-3.4.2\n    2) system/hwloc/1.6.2-GCC-4.7.2   5) numlib/OpenBLAS/0.2.6-gompi-1.4.10-LAPACK-3.4.2   8) toolchain/goolf/1.4.10\n    3) mpi/OpenMPI/1.6.4-GCC-4.7.2    6) numlib/FFTW/3.3.3-gompi-1.4.10  To remove all the loaded modules at once we use the  module purge  command: (node)$> module purge  Next we are going to look at the hierarchical architecture of the modules.",
            "title": "module command basics and workflow"
        },
        {
            "location": "/advanced/RESIF/#software-stack-architecture",
            "text": "The upper layer of the architecture is what we call a  software set . It is a collection of software, for example we define a  core  set that only contains commonly-used and tested software and an  experimental  set that contains untested software.\nThe main goal of these categories is to provide information on the degree of support for the various software.  Inside these sets, software is named in regards to a  naming scheme  which classifies the software (e.g. compilers, physics) and allows for a better structuring of results with the  module avail  command.\nThe software named using this scheme has the following format:  software_class/software_name/software_complete_version  where   software_class  describes the category among the following classes: [base, bio, cae, chem, compiler, data, debugger, devel, lang, lib, math, mpi, numlib, phys, system, toolchain, tools, vis]  software_name  is the name of the software (e.g. GROMACS, MATLAB, R or ABySS)  software_complete_version  is the full version of the software: containing the version of the software itself followed by the type and version of the main dependencies it relies on (e.g. compiler) with the following format: software_version-dependencies_versions   The  module avail  command will thus have the output shown below, where we can note:   the core software set is shown first  application names are prefixed with the category (class)  full versions including tool dependencies are shown  the default module for each application is marked with a  (D) , thus by loading  compiler/GCC  the system would in effect load  compiler/GCC/4.8.2 ```\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/bio -------------------------------------------------------------------------------\n   bio/ABySS/1.3.4-goolf-1.4.10-Python-2.7.3        bio/Bowtie2/2.2.2-goolf-1.4.10   (D)    bio/GROMACS/4.6.1-ictce-5.3.0-hybrid     bio/GROMACS/4.6.5-goolf-1.4.10-mt (D)\n   bio/Bowtie2/2.0.2-ictce-5.3.0                    bio/Cufflinks/2.0.2-ictce-5.3.0  (D)    bio/GROMACS/4.6.5-goolf-1.4.10-hybrid    bio/SAMtools/0.1.18-ictce-5.3.0   (D)\n\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/cae -------------------------------------------------------------------------------\n   cae/ABAQUS/6.11.1    cae/OpenFOAM/2.3.0-goolf-1.4.10    cae/OpenFOAM/2.3.0-ictce-5.3.0 (D)\n\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/chem ------------------------------------------------------------------------------\n   chem/ABINIT/7.2.1-x86_64_linux_gnu4.5                chem/GPAW/0.9.0.8965-goolf-1.4.10-Python-2.7.3        chem/QuantumESPRESSO/5.0.2-goolf-1.4.10              chem/libctl/3.2.1-goolf-1.4.10\n   chem/ASE/3.6.0.2515-ictce-5.3.0-Python-2.7.3  (D)    chem/QuantumESPRESSO/5.0.2-goolf-1.4.10-hybrid        chem/QuantumESPRESSO/5.0.2-ictce-5.3.0        (D)\n\n---------------------------------------------------------------------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/compiler ----------------------------------------------------------------------------\n   compiler/GCC/4.7.2    compiler/GCC/4.8.2 (D)    compiler/icc/2013.3.163    compiler/ifort/2013.3.163\n\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/data ------------------------------------------------------------------------------\n   data/HDF5/1.8.7-goolf-1.4.10    data/HDF5/1.8.10-patch1-goolf-1.4.10        data/netCDF/4.2-goolf-1.4.10            data/netCDF-C++/4.2-goolf-1.4.10\n   data/HDF5/1.8.9-ictce-5.3.0     data/h5utils/1.12.1-ictce-5.3.0      (D)    data/netCDF/4.2.1.1-ictce-5.3.0  (D)    data/netCDF-Fortran/4.2-ictce-5.3.0  (D)\n```",
            "title": "Software stack architecture"
        },
        {
            "location": "/advanced/RESIF/#adding-a-software-to-an-existing-stack-on-ul-hpc",
            "text": "In this part, we are going to show the steps to add a software that is not already provided on the platform yet is available as a module in the  EasyBuild database . Using the RESIF tool is the preferred way to add new software.\nFirst of all we are going to install RESIF and then use it to add a software (bzip2 in this example).  Note: The following commands must be executed in an OAR job.",
            "title": "Adding a software to an existing stack on UL HPC"
        },
        {
            "location": "/advanced/RESIF/#installation-of-resif",
            "text": "RESIF requires the following prerequisites (already available on the platform):   Python 2.6  or above  pip (included in the latest Python)  git   RESIF installation:      (node)$> pip install --install-option=\"--prefix=$HOME/.local\" resif  Initially, we need to add the following paths to the environment:      (node)$> export PATH=$PATH:~/.local/bin\n    (node)$> export PYTHONPATH=$PYTHONPATH:~/.local/lib/python2.7/site-packages  Also,  RESIF initialization which will download the required module sources to build new software:      (node)$> resif init",
            "title": "Installation of RESIF"
        },
        {
            "location": "/advanced/RESIF/#installation-of-additional-software",
            "text": "First we need to create a file that lists the applications we want to install.\nTo search for the names of the application configuration files, we use RESIF as follow:      (node)$> resif search bzip2\n    [...]\n    * bzip2-1.0.6.eb\n    [...]  Create a file (we are assuming it is in the home directory), name it  swsets.yaml  and insert the following content:      mysoftware:\n      - bzip2-1.0.6.eb  This is a  YAML  format file that RESIF reads, the internal layout is the following:      software_set1_name:\n      - software1_configurationfile\n      - software2_configurationfile\n    software_set2_name:\n      - software3_configurationfile  It can list as many software, divided in as many software sets as we require to structure our installation.  Now we install the software using  resif build :      (node)$> resif build --installdir $HOME/.local/resif --swsets-config ~/swsets.yaml mysoftware  This will install the software using ~/.local/resif as the root of the installation.  To make the software modules available through the  module  command, we need to add their path:      (node)$> module use $HOME/.local/resif/mysoftware/modules/all  Now, we can see  bzip2  at the very beginning of the output of the list of the software modules:      (node)$> module avail\n    ----- /home/users/username/.local/resif/mysoftware/core/modules/all -----\n    tools/bzip2/1.0.6  The software is installed, and we can load its profile with  module load tools/bzip2/1.0.6 .  RESIF offers many more possibilities than this basic functionality, for more details check the  documentation .",
            "title": "Installation of additional software"
        },
        {
            "location": "/advanced/RESIF/#replicating-the-software-sets-in-a-local-environment",
            "text": "In this part, we are going to create a software stack from scratch. This is especially useful if we want to work on a local workstation (e.g. a laptop) with the same tools as those available on the platform.  We assume that RESIF is already installed as per the instructions described  above .\nNote: RESIF internally depends on  EasyBuild  which is compatible with Linux/OSX but not Windows. On Windows, you can configure a Linux Virtual Machine to use RESIF and the software built using it.  At this point, please make sure to unset the RESIF_ROOTINSTALL environment variable.  This is a necessary step since this variable would interfere with the creation of a new software stack (which defines a new rootinstall).\nTo do so, execute the following command:      unset RESIF_ROOTINSTALL",
            "title": "Replicating the software sets in a local environment"
        },
        {
            "location": "/advanced/RESIF/#direct-method",
            "text": "A  swsets.yaml  file that describes which software we want to install needs to be created.\nAs an example, create it in your home directory with the following content:      core:\n      - bzip2-1.0.6.eb\n      - ABINIT-7.2.1-x86_64_linux_gnu4.5.eb  We include here only the bzip2 library and the ABINIT software as they are fast to deploy.  We can now install this simple software stack with the following command:      $> resif cleaninstall --swsets-config ~/swsets.yaml core  This will install everything using  ~/.local/resif  as the root of the installation.  To use this newly installed software stack, you need to source the LOADME file inside of the rootinstall directory.\nThis path should look like this:  ~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh .      $> source ~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh  The software stack is now ready to be used with the  module  command.",
            "title": "Direct method"
        },
        {
            "location": "/advanced/RESIF/#indirect-method",
            "text": "In this method we are going to create the architecture and then add software to it.  We create the architecture using the  bootstrap  subcommand:      $> resif bootstrap  This will create the architecture using  ~/.local/resif  as the root of the installation.  We now need to make this architecture active: you need to source the LOADME file inside of the rootinstall directory.\nThis path should look like this:  ~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh .      $> source ~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh  Then we need to create the file that describe the software we want to install. In the home directory, create a file named  swsets.yaml  and make it match the following content:      core:\n      - bzip2-1.0.6.eb\n      - ABINIT-7.2.1-x86_64_linux_gnu4.5.eb  We include here only the bzip2 library and the ABINIT software as they are fast to deploy.  We now only need to build the given software:      $> resif build --swsets-config ~/swsets.yaml core  This will install the software listed in the  swsets.yaml  file. The software stack is now ready to be used.  To learn more about RESIF and how to control more parameters of its usage, please refer to the  documentation .  To conclude this tutorial, here is a schema that summarizes the previous parts:",
            "title": "Indirect method"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/",
            "text": "-\n- mode: markdown; mode: visual-line; fill-column: 80 -\n-\n\n\nCopyright (c) 2013-2017 UL HPC Team  \nhpc-sysadmins@uni.lu\n\n\n\n\nUL HPC MPI Tutorial: Building and Runnning OSU Micro-Benchmarks\n\n\n \n \n \n \n \n \n\n\n\n\nThe objective of this tutorial is to compile and run on of the \nOSU micro-benchmarks\n which permit to measure the performance of an MPI implementation.\n\nOn purpose\n, we won't check here if an Easybuild recipe is available for this software to conduct a full build \nby hand\n and check the difference in build options between the different MPI suits.\n\n\nYou can work in groups for this training, yet individual work is encouraged to ensure you understand and practice the usage of MPI programs on an HPC platform.\n\n\nIn all cases, ensure you are able to \nconnect to the UL HPC  clusters\n.\n\n\n# /!\\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A COMPUTING NODE\n# Have an interactive job\n(access)$> si -N 2 --ntasks-per-node=1                    # iris\n(access)$> srun -p interactive --qos qos-iteractive -N 2 --ntasks-per-node=1 --pty bash  # iris (long version)\n(access)$> oarsub -I -l enclosure=1/nodes=2,walltime=4   # chaos / gaia\n\n\n\n\nAdvanced users only\n: rely on \nscreen\n (see  \ntutorial\n or the \nUL HPC tutorial\n on the  frontend prior to running any \noarsub\n or \nsrun/sbatch\n command to be more resilient to disconnection.\n\n\nThe latest version of this tutorial is available on \nGithub\n.\nFinally, advanced MPI users might be interested to take a look at the \nIntel Math Kernel Library Link Line Advisor\n.\n\n\nObjectives\n\n\nThe \nOSU micro-benchmarks\n feature a series of MPI benchmarks that measure the performances of various MPI operations:\n\n\n\n\nPoint-to-Point MPI Benchmarks\n: Latency, multi-threaded latency, multi-pair latency, multiple bandwidth / message rate test bandwidth, bidirectional bandwidth\n\n\nCollective MPI Benchmarks\n: Collective latency tests for various MPI collective operations such as MPI_Allgather, MPI_Alltoall, MPI_Allreduce, MPI_Barrier, MPI_Bcast, MPI_Gather, MPI_Reduce, MPI_Reduce_Scatter, MPI_Scatter and vector collectives.\n\n\nOne-sided MPI Benchmarks\n: one-sided put latency (active/passive), one-sided put bandwidth (active/passive), one-sided put bidirectional bandwidth, one-sided get latency (active/passive), one-sided get bandwidth (active/passive), one-sided accumulate latency (active/passive), compare and swap latency (passive), and fetch and operate (passive) for MVAPICH2 (MPI-2 and MPI-3).\n\n\nSince the 4.3 version, the \nOSU micro-benchmarks\n also features OpenSHMEM benchmarks, a 1-sided communications library.\n\n\n\n\nIn this tutorial, we will build \nversion 5.3.2 of the \nOSU micro-benchmarks\n (the latest at the time of writing), and focus on two of the available tests:\n\n\n\n\nosu_get_latency\n - Latency Test\n\n\nosu_get_bw\n - Bandwidth Test\n\n\n\n\n\n\nThe latency tests are carried out in a ping-pong fashion. The sender sends a message with a certain data size to the receiver and waits for a reply from the receiver. The receiver receives the message from the sender and sends back a reply with the same data size. Many iterations of this ping-pong test are carried out and average one-way latency numbers are obtained. Blocking version of MPI functions (MPI_Send and MPI_Recv) are used in the tests.\n\n\nThe bandwidth tests were carried out by having the sender sending out a fixed number (equal to the window size) of back-to-back messages to the receiver and then waiting for a reply from the receiver. The receiver sends the reply only after receiving all these messages. This process is repeated for several iterations and the bandwidth is calculated based on the elapsed time (from the time sender sends the first message until the time it receives the reply back from the receiver) and the number of bytes sent by the sender. The objective of this bandwidth test is to determine the maximum sustained date rate that can be achieved at the network level. Thus, non-blocking version of MPI functions (MPI_Isend and MPI_Irecv) were used in the test.\n\n\n\n\nThe idea is to compare the different MPI implementations available on the \nUL HPC platform\n.:\n\n\n\n\nIntel MPI\n\n\nOpenMPI\n\n\nMVAPICH2\n (MPI-3 over OpenFabrics-IB, Omni-Path, OpenFabrics-iWARP, PSM, and TCP/IP)\n\n\n\n\nFor the sake of time and simplicity, we will focus on the first two suits. Eventually, the benchmarking campain will typically involves for each MPI suit:\n\n\n\n\ntwo nodes, belonging to the \nsame\n enclosure\n\n\ntwo nodes, belonging to \ndifferent\n enclosures\n\n\n\n\nPre-requisites\n\n\nOn the \naccess\n and a \ncomputing\n node of the cluster you're working on, clone the \nULHPC/tutorials\n  and \nULHPC/launcher-scripts\n repositories\n\n\n$> cd\n$> mkdir -p git/ULHPC && cd  git/ULHPC\n$> git clone https://github.com/ULHPC/launcher-scripts.git\n$> git clone https://github.com/ULHPC/tutorials.git         # If not yet done\n\n\n\n\nPrepare your working directory\n\n\n$> mkdir -p ~/tutorials/OSU-MicroBenchmarks\n$> cd ~/tutorials/OSU-MicroBenchmarks\n$> ln -s ~/git/ULHPC/tutorials/advanced/OSU_MicroBenchmarks ref.ulhpc.d   # Keep a symlink to the reference tutorial\n$> ln -s ref.ulhpc.d/Makefile .     # symlink to the root Makefile\n\n\n\n\nFetch and uncompress the latest version of the \nOSU micro-benchmarks\n\n\n$> cd ~/tutorials/OSU-MicroBenchmarks\n$> mkdir src\n$> cd src\n# Download the latest version\n$> export OSU_VERSION=5.3.2     # Just to abstract from the version to download\n$> wget --no-check-certificate http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-${OSU_VERSION}.tar.gz\n$> tar xvzf osu-micro-benchmarks-${OSU_VERSION}.tar.gz\n$> cd osu-micro-benchmarks-${OSU_VERSION}\n\n\n\n\nBuilding the OSU Micro-benchmarks\n\n\nWe will build the \nOSU micro-benchmarks\n for each considered MPI suit, thus in a separate directory \nbuild.<suit>\n -- that's a good habbit you're encouraged (as with \nCMake\n based projects)\nIn all cases, you \nshould\n now operate the compilation within an interactive job to be able to use the \nmodule\n command.\n\n\n# If not yet done\n(access)$> si -N 2 --ntasks-per-node=1                   # on iris (1 core on 2 nodes)\n(access)$> oarsub -I -l enclosure=1/nodes=2,walltime=4   # chaos / gaia\n\n\n\n\nCompilation based on the Intel MPI suit\n\n\nWe are first going to use the \nIntel Cluster Toolkit Compiler Edition\n,\nwhich provides Intel C/C++ and Fortran compilers, Intel MPI.\nWe will compile the \nOSU micro-benchmarks\n in a specific directory (that a good habbit)\n\n\n$> cd ~/tutorials/OSU-MicroBenchmarks/\n$> mkdir build.intel    # Prepare the specific building directory\n$> cd  build.intel\n# Load the appropriate module\n$> module spider MPI     # Search for available modules featuring MPI\n$> module load toolchain/intel   # On iris -- use 'module load toolchain/ictce' otherwise\n$> module list\nCurrently Loaded Modules:\n  1) compiler/GCCcore/6.3.0                   4) compiler/ifort/2017.1.132-GCC-6.3.0-2.27                 7) toolchain/iimpi/2017a\n  2) tools/binutils/2.27-GCCcore-6.3.0        5) toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27             8) numlib/imkl/2017.1.132-iimpi-2017a\n  3) compiler/icc/2017.1.132-GCC-6.3.0-2.27   6) mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27   9) toolchain/intel/2017a\n\n# Configure the Intel MPI-based build for installation in the current directory\n$> ../src/osu-micro-benchmarks-5.4/configure CC=mpiicc CXX=mpiicpc CFLAGS=$(pwd)/../src/osu-micro-benchmarks-5.4/util --prefix=$(pwd)\n$> make && make install\n\n\n\n\nIf everything goes fine, you shall have the \nOSU micro-benchmarks\n installed in the directory \nlibexec/osu-micro-benchmarks/mpi/\n.\n\n\nOnce compiled, ensure you are able to run it:\n\n\n$> cd libexec/osu-micro-benchmarks/mpi/one-sided/\n\n#### On iris\n$> srun -n $SLURM_NTASKS ./osu_get_latency\n$> srun -n $SLURM_NTASKS ./osu_get_bw\n\n#### On gaia, chaos\n$> mpirun -hostfile $OAR_NODEFILE -perhost 1 ./osu_get_latency\n$> mpirun -hostfile $OAR_NODEFILE -perhost 1 ./osu_get_bw\n\n\n\n\nCompilation based on the OpenMPI suit\n\n\nRepeat the procedure for the OpenMPI suit:\n\n\n$> cd ~/tutorials/OSU-MicroBenchmarks/\n$> mkdir build.openmpi    # Prepare the specific building directory\n$> cd  build.openmpi\n# Clean the previously loaded module and load the appropriate OpenMPI one\n$> module purge\n$> module spider OpenMPI\n$> module load mpi/OpenMPI\n$> module list\n\nCurrently Loaded Modules:\n  1) compiler/GCCcore/6.3.0              3) compiler/GCC/6.3.0-2.28              5) system/hwloc/1.11.7-GCC-6.3.0-2.28\n  2) tools/binutils/2.28-GCCcore-6.3.0   4) tools/numactl/2.0.11-GCCcore-6.3.0   6) mpi/OpenMPI/2.1.1-GCC-6.3.0-2.28\n\n# Configure the OpenMPI-based build for installation in the current directory\n$> ../src/osu-micro-benchmarks-5.4/configure CC=mpicc --prefix=$(pwd)\n$> make && make install\n\n\n\n\nOnce compiled, ensure you are able to run it:\n\n\n$> cd libexec/osu-micro-benchmarks/mpi/one-sided/\n\n#### On iris\n$> srun -n $SLURM_NTASKS ./osu_get_latency   # OR mpirun -npernode 1 --mca btl openib,self,sm  ./osu_get_latency\n$> srun -n $SLURM_NTASKS ./osu_get_bw        # OR mpirun -npernode 1 --mca btl openib,self,sm  ./osu_get_bw\n# Or, if you don't want to use PMI2\n$> mpirun -np $SLURM_NTASKS -perhost 1 --mca btl openib,self,sm ./osu_get_{latency,bw}\n\n#### On gaia, chaos\n$> mpirun -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -npernode 1 ./osu_get_latency\n$> mpirun -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -npernode 1 ./osu_get_bw\n\n\n\n\nPreparing batch runs\n\n\nWe are now going to prepare launcher scripts to permit passive runs (typically in the \n{default | batch}\n queue).\nWe will place them in a separate directory (\nruns/\n) as it will host the outcomes of the executions on the UL HPC platform .\n\n\n$> cd ~/tutorials/OSU-MicroBenchmarks/\n$> mkdir runs    # Prepare the specific run directory\n\n\n\n\nSlurm launcher (Intel MPI)\n\n\nCopy and adapt the \ndefault SLURM launcher\n you should have a copy in \n~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh\n\n\n$> cd ~/tutorials/OSU-MicroBenchmarks/runs\n# Prepare a laucnher for intel suit\n$> cp ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh launcher-OSU.intel.sh\n\n\n\n\nTake your favorite editor (\nvim\n, \nnano\n, etc.) to modify it according to your needs.\nHere is for instance a suggested difference for intel MPI:\n\n\n--- ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh  2017-06-11 23:40:34.007152000 +0200\n+++ launcher-OSU.intel.sh       2017-06-11 23:41:57.597055000 +0200\n@@ -10,8 +10,8 @@\n #\n #          Set number of resources\n #\n-#SBATCH -N 1\n-#SBATCH --ntasks-per-node=28\n+#SBATCH -N 2\n+#SBATCH --ntasks-per-node=1\n ### -c, --cpus-per-task=<ncpus>\n ###     (multithreading) Request that ncpus be allocated per process\n #SBATCH -c 1\n@@ -64,15 +64,15 @@\n module load toolchain/intel\n\n # Directory holding your built applications\n-APPDIR=\"$HOME\"\n+APPDIR=\"$HOME/tutorials/OSU-MicroBenchmarks/build.intel/libexec/osu-micro-benchmarks/mpi/one-sided\"\n # The task to be executed i.E. your favorite Java/C/C++/Ruby/Perl/Python/R/whatever program\n # to be invoked in parallel\n-TASK=\"${APPDIR}/app.exe\"\n+TASK=\"${APPDIR}/$1\"\n\n # The command to run\n-CMD=\"${TASK}\"\n+# CMD=\"${TASK}\"\n ### General MPI Case:\n-# CMD=\"srun -n $SLURM_NTASKS ${TASK}\"\n+CMD=\"srun -n $SLURM_NTASKS ${TASK}\"\n ### OpenMPI case if you wish to specialize the MCA parameters\n #CMD=\"mpirun -np $SLURM_NTASKS --mca btl openib,self,sm ${TASK}\"\n\n\n\n\nIf you apply the above changes, you can test the script in an interactive job as follows:\n\n\n$> cd ~/tutorials/OSU-MicroBenchmarks/runs\n$> si -N 2 --ntasks-per-node=1     # create an interactive job, 1 core on 2 nodes\n$> ./launcher-OSU.intel.sh osu_get_bw\n$> ./launcher-OSU.intel.sh osu_get_latency\n\n\n\n\nAnd then test it in batch mode:\n\n\n$> cd ~/tutorials/OSU-MicroBenchmarks/runs\n$> sbatch ./launcher-OSU.intel.sh osu_get_bw\n$> sbatch ./launcher-OSU.intel.sh osu_get_latency\n\n\n\n\nSlurm launcher (OpenMPI)\n\n\n$> cd ~/tutorials/OSU-MicroBenchmarks/runs\n$> cp launcher-OSU.intel.sh launcher-OSU.openmpi.sh\n\n\n\n\nTake again your favorite editor (\nvim\n, \nnano\n, etc.) to modify \nlauncher-OSU.openmpi.sh\n as follows:\n\n\n--- launcher-OSU.intel.sh       2017-06-11 23:41:57.597055000 +0200\n+++ launcher-OSU.openmpi.sh     2017-06-11 23:46:04.589924000 +0200\n@@ -61,10 +61,10 @@\n\n # Load the {intel | foss} toolchain and whatever module(s) you need\n module purge\n-module load toolchain/intel\n+module load mpi/OpenMPI\n\n # Directory holding your built applications\n-APPDIR=\"$HOME/tutorials/OSU-MicroBenchmarks/build.intel/libexec/osu-micro-benchmarks/mpi/one-sided\"\n+APPDIR=\"$HOME/tutorials/OSU-MicroBenchmarks/build.openmpi/libexec/osu-micro-benchmarks/mpi/one-sided\"\n # The task to be executed i.E. your favorite Java/C/C++/Ruby/Perl/Python/R/whatever program\n # to be invoked in parallel\n TASK=\"${APPDIR}/$1\"\n\n\n\n\nAnd then test it in (passive) batch mode:\n\n\n$> cd ~/tutorials/OSU-MicroBenchmarks/runs\n$> sbatch ./launcher-OSU.openmpi.sh osu_get_bw\n$> sbatch ./launcher-OSU.openmpi.sh osu_get_latency\n\n\n\n\nSlurm launcher (OpenMPI over Ethernet interface)\n\n\nBy default, the MPI communications are operated over the fast Infiniband interconnect.\nWith OpenMPI, we can \nforce\n them over the Ethernet network to highlight the performance impact of using such a slower network.\n\n\n$> cd ~/tutorials/OSU-MicroBenchmarks/runs\n$> cp launcher-OSU.openmpi.sh launcher-OSU.openmpi-eth.sh\n\n\n\n\nTake again your favorite editor (\nvim\n, \nnano\n, etc.) to modify \nlauncher-OSU.openmpi-eth.sh\n as follows:\n\n\n--- launcher-OSU.openmpi.sh     2017-06-11 23:46:04.589924000 +0200\n+++ launcher-OSU.openmpi-eth.sh 2017-06-11 23:55:02.239258000 +0200\n@@ -72,9 +72,9 @@\n # The command to run\n # CMD=\"${TASK}\"\n ### General MPI Case:\n-CMD=\"srun -n $SLURM_NTASKS ${TASK}\"\n+# CMD=\"srun -n $SLURM_NTASKS ${TASK}\"\n ### OpenMPI case if you wish to specialize the MCA parameters\n-#CMD=\"mpirun -np $SLURM_NTASKS --mca btl openib,self,sm ${TASK}\"\n+CMD=\"mpirun -np $SLURM_NTASKS -npernode 1 --mca btl tcp,self ${TASK}\"\n\n ### Prepare logfile\n LOGFILE=\"${RUNDIR}/$(date +%Y-%m-%d)_$(basename ${TASK})_${SLURM_JOBID}.log\"\n\n\n\n\nAnd then test it in (passive) batch mode:\n\n\n$> cd ~/tutorials/OSU-MicroBenchmarks/runs\n$> sbatch ./launcher-OSU.openmpi-eth.sh osu_get_bw\n$> sbatch ./launcher-OSU.openmpi-eth.sh osu_get_latency\n\n\n\n\nYou can find the obtained results on the \niris\n cluster:\n\n\n\n\n\n\nOAR launcher (Intel MPI)\n\n\nIn the case of OAR (\ni.e.\n on the \ngaia\n and \nchaos\n cluster), you can use the \nMPI generic launcher\n to run the code:\n\n\n$> ~/tutorials/OSU-MicroBenchmarks/runs\n$> ln -s ~/git/ULHPC/launcher-scripts/bash/MPI/mpi_launcher.sh launcher-OSU.intel.sh\n$> ./launcher-OSU.intel.sh \\\n     --basedir $HOME/tutorials/OSU-MicroBenchmarks/src/osu-micro-benchmarks-5.4/build.intel/libexec/osu-micro-benchmarks/mpi/one-sided \\\n     --npernode 1 --module toolchain/intel --exe osu_get_latency,osu_get_bw\n\n\n\n\nIf you want to avoid this long list of arguments, just create a file \nlauncher-OSU.intel.default.conf\n to contain:\n\n\n# Defaults settings for running the OSU Micro benchmarks compiled with Intel MPI\nNAME=OSU.intel\n\nMODULE_TO_LOADstr=toolchain/intel\nMPI_PROG_BASEDIR=$HOME/tutorials/OSU-MicroBenchmarks/src/osu-micro-benchmarks-5.4/build.intel/libexec/osu-micro-benchmarks/mpi/one-sided/\n\nMPI_PROGstr=osu_get_latency,osu_get_bw\nMPI_NPERNODE=1\n\n\n\n\nNow you can run the launcher script (interactively, or not):\n\n\n# IF within an interactive job\n$> ./launcher-OSU.intel.sh\n# You might want also to host the output files in the local directory (under the date)\n$> ./launcher-OSU.intel.sh --datadir data/$(date +%Y-%m-%d)\n\n# Submitting a passive job\n$> oarsub -S ./launcher-OSU.intel.sh\n\n\n\n\nOAR launcher (OpenMPI)\n\n\nAgain, we will rely on the \nMPI generic launcher\n to run the code:\n\n\n$> ~/tutorials/OSU-MicroBenchmarks/runs\n$> ln -s ~/git/ULHPC/launcher-scripts/bash/MPI/mpi_launcher.sh launcher-OSU.openmpi.sh\n$> vim launcher-OSU.openmpi.default.conf\n[...] # See below for content\n$> cat launcher-OSU.openmpi.default.conf\n# Defaults settings for running the OSU Micro benchmarks wompiled with OpenMPI\nNAME=OSU.openmpi\n\nMODULE_TO_LOADstr=mpi/OpenMPI\n    MPI_PROG_BASEDIR=$HOME/tutorials/OSU-MicroBenchmarks/src/osu-micro-benchmarks-5.4/build.openmpi/libexec/osu-micro-benchmarks/mpi/one-sided/\n\nMPI_PROGstr=osu_get_latency,osu_get_bw\nMPI_NPERNODE=1\n\n# Submit a passive job\n$> oarsub -S ./launcher-OSU.openmpi.sh\n\n\n\n\nBenchmarking on two nodes\n\n\nOperate the benchmarking campain (in the two cases) in the following context:\n\n\n\n\n\n\n2 nodes belonging to the same enclosure. Use for that:\n\n\n$> oarsub -l enclosure=1/nodes=2,walltime=8 [\u2026]\n\n\n\n\n\n\n\n2 nodes belonging to the different enclosures:\n\n\n$> oarsub -l enclosure=2/core=1,walltime=8 [\u2026]\n\n\n\n\n\n\n\nNow for Lazy / frustrated persons\n\n\nYou will find in the \nUL HPC tutorial\n\nrepository, under the \nadvanced/OSU_MicroBenchmarks\n directory, a set of tools / script that\nfacilitate the running and analysis of this tutorial that you can use/adapt to\nsuit your needs.\n\n\nIn particular, once in the \nadvanced/OSU_MicroBenchmarks\n directory:\n\n\n\n\nrunning \nmake fetch\n will automatically download the archives for the \nOSU micro-benchmarks\n in the \nsrc/\n directory\n\n\nThe  different launcher files in \nruns/\n\n\nSome sample output data in \nruns/data/\n\n\nrun \nmake build\n to build the different versions of the OSU Micro-benchmarks\n\n\nrun \nmake plot\n to invoke the \nGnuplot\n script\n  \nplots/benchmark_OSU.gnuplot\n and generate various plots from the sample\n  runs.\n\n\n\n\nIn particular, you'll probably want to see the comparison figure extracted from\nthe sample run in \nplots/benchmark_OSU_2H_latency.pdf\n and \nplots/benchmark_OSU_2H_bandwidth.pdf\n\n\nA PNG version of these plots is available on Github:\n\nOSU latency\n -- \nOSU Bandwidth",
            "title": "OSU Micro-benchmarks"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#ul-hpc-mpi-tutorial-building-and-runnning-osu-micro-benchmarks",
            "text": "The objective of this tutorial is to compile and run on of the  OSU micro-benchmarks  which permit to measure the performance of an MPI implementation. On purpose , we won't check here if an Easybuild recipe is available for this software to conduct a full build  by hand  and check the difference in build options between the different MPI suits.  You can work in groups for this training, yet individual work is encouraged to ensure you understand and practice the usage of MPI programs on an HPC platform.  In all cases, ensure you are able to  connect to the UL HPC  clusters .  # /!\\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A COMPUTING NODE\n# Have an interactive job\n(access)$> si -N 2 --ntasks-per-node=1                    # iris\n(access)$> srun -p interactive --qos qos-iteractive -N 2 --ntasks-per-node=1 --pty bash  # iris (long version)\n(access)$> oarsub -I -l enclosure=1/nodes=2,walltime=4   # chaos / gaia  Advanced users only : rely on  screen  (see   tutorial  or the  UL HPC tutorial  on the  frontend prior to running any  oarsub  or  srun/sbatch  command to be more resilient to disconnection.  The latest version of this tutorial is available on  Github .\nFinally, advanced MPI users might be interested to take a look at the  Intel Math Kernel Library Link Line Advisor .",
            "title": "UL HPC MPI Tutorial: Building and Runnning OSU Micro-Benchmarks"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#objectives",
            "text": "The  OSU micro-benchmarks  feature a series of MPI benchmarks that measure the performances of various MPI operations:   Point-to-Point MPI Benchmarks : Latency, multi-threaded latency, multi-pair latency, multiple bandwidth / message rate test bandwidth, bidirectional bandwidth  Collective MPI Benchmarks : Collective latency tests for various MPI collective operations such as MPI_Allgather, MPI_Alltoall, MPI_Allreduce, MPI_Barrier, MPI_Bcast, MPI_Gather, MPI_Reduce, MPI_Reduce_Scatter, MPI_Scatter and vector collectives.  One-sided MPI Benchmarks : one-sided put latency (active/passive), one-sided put bandwidth (active/passive), one-sided put bidirectional bandwidth, one-sided get latency (active/passive), one-sided get bandwidth (active/passive), one-sided accumulate latency (active/passive), compare and swap latency (passive), and fetch and operate (passive) for MVAPICH2 (MPI-2 and MPI-3).  Since the 4.3 version, the  OSU micro-benchmarks  also features OpenSHMEM benchmarks, a 1-sided communications library.   In this tutorial, we will build  version 5.3.2 of the  OSU micro-benchmarks  (the latest at the time of writing), and focus on two of the available tests:   osu_get_latency  - Latency Test  osu_get_bw  - Bandwidth Test    The latency tests are carried out in a ping-pong fashion. The sender sends a message with a certain data size to the receiver and waits for a reply from the receiver. The receiver receives the message from the sender and sends back a reply with the same data size. Many iterations of this ping-pong test are carried out and average one-way latency numbers are obtained. Blocking version of MPI functions (MPI_Send and MPI_Recv) are used in the tests.  The bandwidth tests were carried out by having the sender sending out a fixed number (equal to the window size) of back-to-back messages to the receiver and then waiting for a reply from the receiver. The receiver sends the reply only after receiving all these messages. This process is repeated for several iterations and the bandwidth is calculated based on the elapsed time (from the time sender sends the first message until the time it receives the reply back from the receiver) and the number of bytes sent by the sender. The objective of this bandwidth test is to determine the maximum sustained date rate that can be achieved at the network level. Thus, non-blocking version of MPI functions (MPI_Isend and MPI_Irecv) were used in the test.   The idea is to compare the different MPI implementations available on the  UL HPC platform .:   Intel MPI  OpenMPI  MVAPICH2  (MPI-3 over OpenFabrics-IB, Omni-Path, OpenFabrics-iWARP, PSM, and TCP/IP)   For the sake of time and simplicity, we will focus on the first two suits. Eventually, the benchmarking campain will typically involves for each MPI suit:   two nodes, belonging to the  same  enclosure  two nodes, belonging to  different  enclosures",
            "title": "Objectives"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#pre-requisites",
            "text": "On the  access  and a  computing  node of the cluster you're working on, clone the  ULHPC/tutorials   and  ULHPC/launcher-scripts  repositories  $> cd\n$> mkdir -p git/ULHPC && cd  git/ULHPC\n$> git clone https://github.com/ULHPC/launcher-scripts.git\n$> git clone https://github.com/ULHPC/tutorials.git         # If not yet done  Prepare your working directory  $> mkdir -p ~/tutorials/OSU-MicroBenchmarks\n$> cd ~/tutorials/OSU-MicroBenchmarks\n$> ln -s ~/git/ULHPC/tutorials/advanced/OSU_MicroBenchmarks ref.ulhpc.d   # Keep a symlink to the reference tutorial\n$> ln -s ref.ulhpc.d/Makefile .     # symlink to the root Makefile  Fetch and uncompress the latest version of the  OSU micro-benchmarks  $> cd ~/tutorials/OSU-MicroBenchmarks\n$> mkdir src\n$> cd src\n# Download the latest version\n$> export OSU_VERSION=5.3.2     # Just to abstract from the version to download\n$> wget --no-check-certificate http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-${OSU_VERSION}.tar.gz\n$> tar xvzf osu-micro-benchmarks-${OSU_VERSION}.tar.gz\n$> cd osu-micro-benchmarks-${OSU_VERSION}",
            "title": "Pre-requisites"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#building-the-osu-micro-benchmarks",
            "text": "We will build the  OSU micro-benchmarks  for each considered MPI suit, thus in a separate directory  build.<suit>  -- that's a good habbit you're encouraged (as with  CMake  based projects)\nIn all cases, you  should  now operate the compilation within an interactive job to be able to use the  module  command.  # If not yet done\n(access)$> si -N 2 --ntasks-per-node=1                   # on iris (1 core on 2 nodes)\n(access)$> oarsub -I -l enclosure=1/nodes=2,walltime=4   # chaos / gaia",
            "title": "Building the OSU Micro-benchmarks"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#compilation-based-on-the-intel-mpi-suit",
            "text": "We are first going to use the  Intel Cluster Toolkit Compiler Edition ,\nwhich provides Intel C/C++ and Fortran compilers, Intel MPI.\nWe will compile the  OSU micro-benchmarks  in a specific directory (that a good habbit)  $> cd ~/tutorials/OSU-MicroBenchmarks/\n$> mkdir build.intel    # Prepare the specific building directory\n$> cd  build.intel\n# Load the appropriate module\n$> module spider MPI     # Search for available modules featuring MPI\n$> module load toolchain/intel   # On iris -- use 'module load toolchain/ictce' otherwise\n$> module list\nCurrently Loaded Modules:\n  1) compiler/GCCcore/6.3.0                   4) compiler/ifort/2017.1.132-GCC-6.3.0-2.27                 7) toolchain/iimpi/2017a\n  2) tools/binutils/2.27-GCCcore-6.3.0        5) toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27             8) numlib/imkl/2017.1.132-iimpi-2017a\n  3) compiler/icc/2017.1.132-GCC-6.3.0-2.27   6) mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27   9) toolchain/intel/2017a\n\n# Configure the Intel MPI-based build for installation in the current directory\n$> ../src/osu-micro-benchmarks-5.4/configure CC=mpiicc CXX=mpiicpc CFLAGS=$(pwd)/../src/osu-micro-benchmarks-5.4/util --prefix=$(pwd)\n$> make && make install  If everything goes fine, you shall have the  OSU micro-benchmarks  installed in the directory  libexec/osu-micro-benchmarks/mpi/ .  Once compiled, ensure you are able to run it:  $> cd libexec/osu-micro-benchmarks/mpi/one-sided/\n\n#### On iris\n$> srun -n $SLURM_NTASKS ./osu_get_latency\n$> srun -n $SLURM_NTASKS ./osu_get_bw\n\n#### On gaia, chaos\n$> mpirun -hostfile $OAR_NODEFILE -perhost 1 ./osu_get_latency\n$> mpirun -hostfile $OAR_NODEFILE -perhost 1 ./osu_get_bw",
            "title": "Compilation based on the Intel MPI suit"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#compilation-based-on-the-openmpi-suit",
            "text": "Repeat the procedure for the OpenMPI suit:  $> cd ~/tutorials/OSU-MicroBenchmarks/\n$> mkdir build.openmpi    # Prepare the specific building directory\n$> cd  build.openmpi\n# Clean the previously loaded module and load the appropriate OpenMPI one\n$> module purge\n$> module spider OpenMPI\n$> module load mpi/OpenMPI\n$> module list\n\nCurrently Loaded Modules:\n  1) compiler/GCCcore/6.3.0              3) compiler/GCC/6.3.0-2.28              5) system/hwloc/1.11.7-GCC-6.3.0-2.28\n  2) tools/binutils/2.28-GCCcore-6.3.0   4) tools/numactl/2.0.11-GCCcore-6.3.0   6) mpi/OpenMPI/2.1.1-GCC-6.3.0-2.28\n\n# Configure the OpenMPI-based build for installation in the current directory\n$> ../src/osu-micro-benchmarks-5.4/configure CC=mpicc --prefix=$(pwd)\n$> make && make install  Once compiled, ensure you are able to run it:  $> cd libexec/osu-micro-benchmarks/mpi/one-sided/\n\n#### On iris\n$> srun -n $SLURM_NTASKS ./osu_get_latency   # OR mpirun -npernode 1 --mca btl openib,self,sm  ./osu_get_latency\n$> srun -n $SLURM_NTASKS ./osu_get_bw        # OR mpirun -npernode 1 --mca btl openib,self,sm  ./osu_get_bw\n# Or, if you don't want to use PMI2\n$> mpirun -np $SLURM_NTASKS -perhost 1 --mca btl openib,self,sm ./osu_get_{latency,bw}\n\n#### On gaia, chaos\n$> mpirun -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -npernode 1 ./osu_get_latency\n$> mpirun -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -npernode 1 ./osu_get_bw",
            "title": "Compilation based on the OpenMPI suit"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#preparing-batch-runs",
            "text": "We are now going to prepare launcher scripts to permit passive runs (typically in the  {default | batch}  queue).\nWe will place them in a separate directory ( runs/ ) as it will host the outcomes of the executions on the UL HPC platform .  $> cd ~/tutorials/OSU-MicroBenchmarks/\n$> mkdir runs    # Prepare the specific run directory",
            "title": "Preparing batch runs"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#slurm-launcher-intel-mpi",
            "text": "Copy and adapt the  default SLURM launcher  you should have a copy in  ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh  $> cd ~/tutorials/OSU-MicroBenchmarks/runs\n# Prepare a laucnher for intel suit\n$> cp ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh launcher-OSU.intel.sh  Take your favorite editor ( vim ,  nano , etc.) to modify it according to your needs.\nHere is for instance a suggested difference for intel MPI:  --- ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh  2017-06-11 23:40:34.007152000 +0200\n+++ launcher-OSU.intel.sh       2017-06-11 23:41:57.597055000 +0200\n@@ -10,8 +10,8 @@\n #\n #          Set number of resources\n #\n-#SBATCH -N 1\n-#SBATCH --ntasks-per-node=28\n+#SBATCH -N 2\n+#SBATCH --ntasks-per-node=1\n ### -c, --cpus-per-task=<ncpus>\n ###     (multithreading) Request that ncpus be allocated per process\n #SBATCH -c 1\n@@ -64,15 +64,15 @@\n module load toolchain/intel\n\n # Directory holding your built applications\n-APPDIR=\"$HOME\"\n+APPDIR=\"$HOME/tutorials/OSU-MicroBenchmarks/build.intel/libexec/osu-micro-benchmarks/mpi/one-sided\"\n # The task to be executed i.E. your favorite Java/C/C++/Ruby/Perl/Python/R/whatever program\n # to be invoked in parallel\n-TASK=\"${APPDIR}/app.exe\"\n+TASK=\"${APPDIR}/$1\"\n\n # The command to run\n-CMD=\"${TASK}\"\n+# CMD=\"${TASK}\"\n ### General MPI Case:\n-# CMD=\"srun -n $SLURM_NTASKS ${TASK}\"\n+CMD=\"srun -n $SLURM_NTASKS ${TASK}\"\n ### OpenMPI case if you wish to specialize the MCA parameters\n #CMD=\"mpirun -np $SLURM_NTASKS --mca btl openib,self,sm ${TASK}\"  If you apply the above changes, you can test the script in an interactive job as follows:  $> cd ~/tutorials/OSU-MicroBenchmarks/runs\n$> si -N 2 --ntasks-per-node=1     # create an interactive job, 1 core on 2 nodes\n$> ./launcher-OSU.intel.sh osu_get_bw\n$> ./launcher-OSU.intel.sh osu_get_latency  And then test it in batch mode:  $> cd ~/tutorials/OSU-MicroBenchmarks/runs\n$> sbatch ./launcher-OSU.intel.sh osu_get_bw\n$> sbatch ./launcher-OSU.intel.sh osu_get_latency",
            "title": "Slurm launcher (Intel MPI)"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#slurm-launcher-openmpi",
            "text": "$> cd ~/tutorials/OSU-MicroBenchmarks/runs\n$> cp launcher-OSU.intel.sh launcher-OSU.openmpi.sh  Take again your favorite editor ( vim ,  nano , etc.) to modify  launcher-OSU.openmpi.sh  as follows:  --- launcher-OSU.intel.sh       2017-06-11 23:41:57.597055000 +0200\n+++ launcher-OSU.openmpi.sh     2017-06-11 23:46:04.589924000 +0200\n@@ -61,10 +61,10 @@\n\n # Load the {intel | foss} toolchain and whatever module(s) you need\n module purge\n-module load toolchain/intel\n+module load mpi/OpenMPI\n\n # Directory holding your built applications\n-APPDIR=\"$HOME/tutorials/OSU-MicroBenchmarks/build.intel/libexec/osu-micro-benchmarks/mpi/one-sided\"\n+APPDIR=\"$HOME/tutorials/OSU-MicroBenchmarks/build.openmpi/libexec/osu-micro-benchmarks/mpi/one-sided\"\n # The task to be executed i.E. your favorite Java/C/C++/Ruby/Perl/Python/R/whatever program\n # to be invoked in parallel\n TASK=\"${APPDIR}/$1\"  And then test it in (passive) batch mode:  $> cd ~/tutorials/OSU-MicroBenchmarks/runs\n$> sbatch ./launcher-OSU.openmpi.sh osu_get_bw\n$> sbatch ./launcher-OSU.openmpi.sh osu_get_latency",
            "title": "Slurm launcher (OpenMPI)"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#slurm-launcher-openmpi-over-ethernet-interface",
            "text": "By default, the MPI communications are operated over the fast Infiniband interconnect.\nWith OpenMPI, we can  force  them over the Ethernet network to highlight the performance impact of using such a slower network.  $> cd ~/tutorials/OSU-MicroBenchmarks/runs\n$> cp launcher-OSU.openmpi.sh launcher-OSU.openmpi-eth.sh  Take again your favorite editor ( vim ,  nano , etc.) to modify  launcher-OSU.openmpi-eth.sh  as follows:  --- launcher-OSU.openmpi.sh     2017-06-11 23:46:04.589924000 +0200\n+++ launcher-OSU.openmpi-eth.sh 2017-06-11 23:55:02.239258000 +0200\n@@ -72,9 +72,9 @@\n # The command to run\n # CMD=\"${TASK}\"\n ### General MPI Case:\n-CMD=\"srun -n $SLURM_NTASKS ${TASK}\"\n+# CMD=\"srun -n $SLURM_NTASKS ${TASK}\"\n ### OpenMPI case if you wish to specialize the MCA parameters\n-#CMD=\"mpirun -np $SLURM_NTASKS --mca btl openib,self,sm ${TASK}\"\n+CMD=\"mpirun -np $SLURM_NTASKS -npernode 1 --mca btl tcp,self ${TASK}\"\n\n ### Prepare logfile\n LOGFILE=\"${RUNDIR}/$(date +%Y-%m-%d)_$(basename ${TASK})_${SLURM_JOBID}.log\"  And then test it in (passive) batch mode:  $> cd ~/tutorials/OSU-MicroBenchmarks/runs\n$> sbatch ./launcher-OSU.openmpi-eth.sh osu_get_bw\n$> sbatch ./launcher-OSU.openmpi-eth.sh osu_get_latency  You can find the obtained results on the  iris  cluster:",
            "title": "Slurm launcher (OpenMPI over Ethernet interface)"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#oar-launcher-intel-mpi",
            "text": "In the case of OAR ( i.e.  on the  gaia  and  chaos  cluster), you can use the  MPI generic launcher  to run the code:  $> ~/tutorials/OSU-MicroBenchmarks/runs\n$> ln -s ~/git/ULHPC/launcher-scripts/bash/MPI/mpi_launcher.sh launcher-OSU.intel.sh\n$> ./launcher-OSU.intel.sh \\\n     --basedir $HOME/tutorials/OSU-MicroBenchmarks/src/osu-micro-benchmarks-5.4/build.intel/libexec/osu-micro-benchmarks/mpi/one-sided \\\n     --npernode 1 --module toolchain/intel --exe osu_get_latency,osu_get_bw  If you want to avoid this long list of arguments, just create a file  launcher-OSU.intel.default.conf  to contain:  # Defaults settings for running the OSU Micro benchmarks compiled with Intel MPI\nNAME=OSU.intel\n\nMODULE_TO_LOADstr=toolchain/intel\nMPI_PROG_BASEDIR=$HOME/tutorials/OSU-MicroBenchmarks/src/osu-micro-benchmarks-5.4/build.intel/libexec/osu-micro-benchmarks/mpi/one-sided/\n\nMPI_PROGstr=osu_get_latency,osu_get_bw\nMPI_NPERNODE=1  Now you can run the launcher script (interactively, or not):  # IF within an interactive job\n$> ./launcher-OSU.intel.sh\n# You might want also to host the output files in the local directory (under the date)\n$> ./launcher-OSU.intel.sh --datadir data/$(date +%Y-%m-%d)\n\n# Submitting a passive job\n$> oarsub -S ./launcher-OSU.intel.sh",
            "title": "OAR launcher (Intel MPI)"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#oar-launcher-openmpi",
            "text": "Again, we will rely on the  MPI generic launcher  to run the code:  $> ~/tutorials/OSU-MicroBenchmarks/runs\n$> ln -s ~/git/ULHPC/launcher-scripts/bash/MPI/mpi_launcher.sh launcher-OSU.openmpi.sh\n$> vim launcher-OSU.openmpi.default.conf\n[...] # See below for content\n$> cat launcher-OSU.openmpi.default.conf\n# Defaults settings for running the OSU Micro benchmarks wompiled with OpenMPI\nNAME=OSU.openmpi\n\nMODULE_TO_LOADstr=mpi/OpenMPI\n    MPI_PROG_BASEDIR=$HOME/tutorials/OSU-MicroBenchmarks/src/osu-micro-benchmarks-5.4/build.openmpi/libexec/osu-micro-benchmarks/mpi/one-sided/\n\nMPI_PROGstr=osu_get_latency,osu_get_bw\nMPI_NPERNODE=1\n\n# Submit a passive job\n$> oarsub -S ./launcher-OSU.openmpi.sh",
            "title": "OAR launcher (OpenMPI)"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#benchmarking-on-two-nodes",
            "text": "Operate the benchmarking campain (in the two cases) in the following context:    2 nodes belonging to the same enclosure. Use for that:  $> oarsub -l enclosure=1/nodes=2,walltime=8 [\u2026]    2 nodes belonging to the different enclosures:  $> oarsub -l enclosure=2/core=1,walltime=8 [\u2026]",
            "title": "Benchmarking on two nodes"
        },
        {
            "location": "/advanced/OSU_MicroBenchmarks/#now-for-lazy-frustrated-persons",
            "text": "You will find in the  UL HPC tutorial \nrepository, under the  advanced/OSU_MicroBenchmarks  directory, a set of tools / script that\nfacilitate the running and analysis of this tutorial that you can use/adapt to\nsuit your needs.  In particular, once in the  advanced/OSU_MicroBenchmarks  directory:   running  make fetch  will automatically download the archives for the  OSU micro-benchmarks  in the  src/  directory  The  different launcher files in  runs/  Some sample output data in  runs/data/  run  make build  to build the different versions of the OSU Micro-benchmarks  run  make plot  to invoke the  Gnuplot  script\n   plots/benchmark_OSU.gnuplot  and generate various plots from the sample\n  runs.   In particular, you'll probably want to see the comparison figure extracted from\nthe sample run in  plots/benchmark_OSU_2H_latency.pdf  and  plots/benchmark_OSU_2H_bandwidth.pdf  A PNG version of these plots is available on Github: OSU latency  --  OSU Bandwidth",
            "title": "Now for Lazy / frustrated persons"
        },
        {
            "location": "/advanced/HPL/",
            "text": "-\n- mode: markdown; mode: visual-line; fill-column: 80 -\n-\n\n\nCopyright (c) 2013-2017 UL HPC Team  \nhpc-sysadmins@uni.lu\n\n\n\n\nUL HPC MPI Tutorial: High-Performance Linpack (HPL) benchmarking on UL HPC platform\n\n\n \n \n \n \n \n \n\n\n\n\nThe objective of this tutorial is to compile and run on of the reference HPC benchmarks, \nHPL\n, on top of the \nUL HPC\n platform.\n\n\nYou can work in groups for this training, yet individual work is encouraged to ensure you understand and practice the usage of MPI programs on an HPC platform.\nIf not yet done, you should consider completing the \nOSU Micro-benchmark\n tutorial as it introduces the effective usage of the different MPI suits available on the UL HPC platform.\n\n\nIn all cases, ensure you are able to \nconnect to the UL HPC  clusters\n.\n\n\n# /!\\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A (at least half) COMPUTING NODE\n# Have an interactive job\n(access)$> si -n 14                                      # iris\n(access)$> srun -p interactive --qos qos-iteractive -n 14 --pty bash  # iris (long version)\n(access)$> oarsub -I -l enclosure=1/nodes=1,walltime=4   # chaos / gaia\n\n\n\n\nAdvanced users only\n: rely on \nscreen\n (see  \ntutorial\n or the \nUL HPC tutorial\n on the  frontend prior to running any \noarsub\n or \nsrun/sbatch\n command to be more resilient to disconnection.\n\n\nThe latest version of this tutorial is available on \nGithub\n\nFinally, advanced MPI users might be interested to take a look at the \nIntel Math Kernel Library Link Line Advisor\n.\n\n\nObjectives\n\n\nHPL\n is a  portable implementation of the High-Performance Linpack (HPL) Benchmark for Distributed-Memory Computers. It is used as reference benchmark to provide data for the \nTop500\n list and thus rank to supercomputers worldwide.\nHPL rely on an efficient implementation of the Basic Linear Algebra Subprograms (BLAS). You have several choices at this level:\n\n\n\n\nIntel MKL\n\n\nATLAS\n\n\nGotoBlas\n\n\n\n\nThe idea is to compare the different MPI and BLAS implementations available on the \nUL HPC platform\n:\n\n\n\n\nIntel MPI\n and the Intel MKL\n\n\nOpenMPI\n\n\nMVAPICH2\n (MPI-3 over OpenFabrics-IB, Omni-Path, OpenFabrics-iWARP, PSM, and TCP/IP)\n\n\nATLAS\n\n\nGotoBlas\n\n\n\n\nFor the sake of time and simplicity, we will focus on the combination expected to lead to the best performant runs, \ni.e.\n Intel MKL and Intel MPI suite.\n\n\nPre-requisites\n\n\nOn the \naccess\n and a \ncomputing\n node of the cluster you're working on, clone the \nULHPC/tutorials\n  and \nULHPC/launcher-scripts\n repositories\n\n\n$> cd\n$> mkdir -p git/ULHPC && cd  git/ULHPC\n$> git clone https://github.com/ULHPC/launcher-scripts.git\n$> git clone https://github.com/ULHPC/tutorials.git         # If not yet done\n\n\n\n\nPrepare your working directory\n\n\n$> mkdir -p ~/tutorials/HPL\n$> cd ~/tutorials/HPL\n$> ln -s ~/git/ULHPC/tutorials/advanced/HPL ref.ulhpc.d   # Keep a symlink to the reference tutorial\n$> ln -s ref.ulhpc.d/Makefile .     # symlink to the root Makefile\n\n\n\n\nFetch and uncompress the latest version of the \nHPL\n benchmark (\ni.e.\n \nversion 2.2\n at the time of writing).\n\n\n$> cd ~/tutorials/HPL\n$> mkdir src\n$> cd src\n# Download the latest version\n$> export HPL_VERSION=2.2\n$> wget --no-check-certificate http://www.netlib.org/benchmark/hpl/hpl-${HPL_VERSION}.tar.gz\n$> tar xvzf hpl-${HPL_VERSION}.tar.gz\n$> cd  hpl-${HPL_VERSION}\n\n\n\n\nBuilding the HPL benchmark\n\n\nWe are first going to use the \nIntel Cluster Toolkit Compiler Edition\n, which provides Intel C/C++ and Fortran compilers, Intel MPI.\n\n\n$> cd ~/tutorials/HPL\n# Load the appropriate module\n$> module spider MPI     # Search for available modules featuring MPI\n$> module load toolchain/intel   # On iris -- use 'module load toolchain/ictce' otherwise\n$> module list\nCurrently Loaded Modules:\n  1) compiler/GCCcore/6.3.0                   4) compiler/ifort/2017.1.132-GCC-6.3.0-2.27                 7) toolchain/iimpi/2017a\n  2) tools/binutils/2.27-GCCcore-6.3.0        5) toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27             8) numlib/imkl/2017.1.132-iimpi-2017a\n  3) compiler/icc/2017.1.132-GCC-6.3.0-2.27   6) mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27   9) toolchain/intel/2017a\n\n\n\n\nYou notice that Intel MKL is now loaded.\n\n\nRead the \nINSTALL\n file. In particular, you'll have to edit and adapt a new makefile \nMake.intel64\n\n(inspired from \nsetup/Make.Linux_PII_CBLAS\n typically)\n\n\n$> cd ~/tutorials/HPL/src/hpl-2.2\n$> cp setup/Make.Linux_Intel64 Make.intel64\n\n\n\n\nOnce tweaked, run the compilation by:\n\n\n$> make arch=intel64 clean_arch_all\n$> make arch=intel64\n\n\n\n\nBut \nfirst\n, you will need to configure correctly the file \nMake.intel64\n.\nTake your favorite editor (\nvim\n, \nnano\n, etc.) to modify it. In particular, you should adapt:\n\n\n\n\nTOPdir\n to point to the directory holding the HPL sources (\ni.e.\n where you uncompress them: \n$(HOME)/tutorials/HPL/src/hpl-2.2\n)\n\n\nAdapt the \nMP*\n variables to point to the appropriate MPI libraries path.\n\n\n(eventually) adapt the \nCCFLAGS\n\n\n\n\nHere is for instance a suggested difference for intel MPI:\n\n\n--- setup/Make.Linux_Intel64    2016-02-24 02:10:50.000000000 +0100\n+++ Make.intel64        2017-06-12 13:48:31.016524323 +0200\n@@ -61,13 +61,13 @@\n # - Platform identifier ------------------------------------------------\n # ----------------------------------------------------------------------\n #\n-ARCH         = Linux_Intel64\n+ARCH         = $(arch)\n #\n # ----------------------------------------------------------------------\n # - HPL Directory Structure / HPL library ------------------------------\n # ----------------------------------------------------------------------\n #\n-TOPdir       = $(HOME)/hpl\n+TOPdir       = $(HOME)/tutorials/HPL/src/hpl-2.2\n INCdir       = $(TOPdir)/include\n BINdir       = $(TOPdir)/bin/$(ARCH)\n LIBdir       = $(TOPdir)/lib/$(ARCH)\n@@ -81,9 +81,9 @@\n # header files,  MPlib  is defined  to be the name of  the library to be\n # used. The variable MPdir is only used for defining MPinc and MPlib.\n #\n-# MPdir        = /opt/intel/mpi/4.1.0\n-# MPinc        = -I$(MPdir)/include64\n-# MPlib        = $(MPdir)/lib64/libmpi.a\n+MPdir        = $(I_MPI_ROOT)/intel64\n+MPinc        = -I$(MPdir)/include\n+MPlib        = $(MPdir)/lib/libmpi.a\n #\n # ----------------------------------------------------------------------\n # - Linear Algebra library (BLAS or VSIPL) -----------------------------\n@@ -178,7 +178,7 @@\n CC       = mpiicc\n CCNOOPT  = $(HPL_DEFS)\n OMP_DEFS = -openmp\n-CCFLAGS  = $(HPL_DEFS) -O3 -w -ansi-alias -i-static -z noexecstack -z relro -z now -nocompchk -Wall\n+CCFLAGS  = $(HPL_DEFS) -O3 -w -ansi-alias -i-static -z noexecstack -z relro -z now -nocompchk -Wall -xHost\n #\n # On some platforms,  it is necessary  to use the Fortran linker to find\n # the Fortran internals used in the BLAS library.\n\n\n\n\nIf you don't succeed by yourself, use the following \nmakefile\n:\n\n\n$> cd ~/tutorials/HPL\n$> cp ref.ulhpc.d/src/hpl-2.2/Make.intel64 src/hpl-2.2/Make.intel64\n\n\n\n\nOnce compiled, ensure you are able to run it:\n\n\n$> cd ~/tutorials/HPL/src/hpl-2.2/bin/intel64\n$> cat HPL.dat      # Default (dummy) HPL.dat  input file\n\n# On Slurm cluster (iris)\n$> srun -n $SLURM_NTASKS ./xhpl\n\n# On OAR clusters (gaia, chaos)\n$> mpirun -hostfile $OAR_NODEFILE ./xhpl\n\n\n\n\nPreparing batch runs\n\n\nWe are now going to prepare launcher scripts to permit passive runs (typically in the \n{default | batch}\n queue).\nWe will place them in a separate directory (\nruns/\n) as it will host the outcomes of the executions on the UL HPC platform .\n\n\n$> cd ~/tutorials/HPL\n$> mkdir runs    # Prepare the specific run directory\n\n\n\n\nNow you'll have to find the optimal set of parameters for using a single\nnode. You can use the following site:\n\nHPL Calculator\n to find good parameters\nand expected performances and adapt \nbin/intel64/HPL.dat\n accordingly.\nHere we are going to use reasonable choices as outline from \nthis website\n\n\nSlurm launcher (Intel MPI)\n\n\nCopy and adapt the \ndefault SLURM launcher\n you should have a copy in \n~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh\n\n\n$> cd ~/tutorials/HPL/runs\n# Prepare a laucnher for intel suit\n$> cp ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh launcher-HPL.intel.sh\n\n\n\n\nTake your favorite editor (\nvim\n, \nnano\n, etc.) to modify it according to your needs.\n\n\nHere is for instance a suggested difference for intel MPI:\n\n\n--- ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh  2017-06-11 23:40:34.007152000 +0200\n+++ launcher-HPL.intel.sh       2017-06-11 23:41:57.597055000 +0200\n@@ -10,8 +10,8 @@\n #\n #          Set number of resources\n #\n-#SBATCH -N 1\n+#SBATCH -N 2\n #SBATCH --ntasks-per-node=28\n ### -c, --cpus-per-task=<ncpus>\n ###     (multithreading) Request that ncpus be allocated per process\n #SBATCH -c 1\n@@ -64,15 +64,15 @@\n module load toolchain/intel\n\n # Directory holding your built applications\n-APPDIR=\"$HOME\"\n+APPDIR=\"$HOME/tutorials/HPL/src/hpl-2.2/bin/intel64\"\n # The task to be executed i.E. your favorite Java/C/C++/Ruby/Perl/Python/R/whatever program\n # to be invoked in parallel\n-TASK=\"${APPDIR}/app.exe\"\n+TASK=\"${APPDIR}/xhpl\"\n\n # The command to run\n-CMD=\"${TASK}\"\n+# CMD=\"${TASK}\"\n ### General MPI Case:\n-# CMD=\"srun -n $SLURM_NTASKS ${TASK}\"\n+CMD=\"srun -n $SLURM_NTASKS ${TASK}\"\n ### OpenMPI case if you wish to specialize the MCA parameters\n #CMD=\"mpirun -np $SLURM_NTASKS --mca btl openib,self,sm ${TASK}\"\n\n\n\n\nNow you should create an input \nHPL.dat\n file within the \nruns/\n.\n\n\n$> cd ~/tutorials/HPL/runs\n$> cp ../ref.ulhpc.d/HPL.dat .\n$> ll\ntotal 0\n-rw-r--r--. 1 svarrette clusterusers 1.5K Jun 12 15:38 HPL.dat\n-rwxr-xr-x. 1 svarrette clusterusers 2.7K Jun 12 15:25 launcher-HPL.intel.sh\n\n\n\n\nYou are ready for testing a batch job:\n\n\n$> cd ~/tutorials/HPL/runs\n$> sbatch ./launcher-HPL.intel.sh\n$> sq     # OR (long version) squeue -u $USER\n\n\n\n\n(bonus)\n Connect to one of the allocated nodes and run \nhtop\n (followed by \nu\n to select process run under your username, and \nF5\n to enable the tree-view.\n\n\nNow you can check the output of the HPL runs:\n\n\n$> grep WR slurm-<jobid>.out    # /!\\ ADAPT <jobid> appropriately.\n\n\n\n\nOf course, we made here a small test and optimizing the HPL parameters to get the best performances and efficiency out of a given HPC platform is not easy.\nBelow are some plots obtained when benchmarking the \niris\n cluster and seeking the best set of parameters across increasing number of nodes (see \nthis blog post\n)",
            "title": "High Performance Linpack (HPL)"
        },
        {
            "location": "/advanced/HPL/#ul-hpc-mpi-tutorial-high-performance-linpack-hpl-benchmarking-on-ul-hpc-platform",
            "text": "The objective of this tutorial is to compile and run on of the reference HPC benchmarks,  HPL , on top of the  UL HPC  platform.  You can work in groups for this training, yet individual work is encouraged to ensure you understand and practice the usage of MPI programs on an HPC platform.\nIf not yet done, you should consider completing the  OSU Micro-benchmark  tutorial as it introduces the effective usage of the different MPI suits available on the UL HPC platform.  In all cases, ensure you are able to  connect to the UL HPC  clusters .  # /!\\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A (at least half) COMPUTING NODE\n# Have an interactive job\n(access)$> si -n 14                                      # iris\n(access)$> srun -p interactive --qos qos-iteractive -n 14 --pty bash  # iris (long version)\n(access)$> oarsub -I -l enclosure=1/nodes=1,walltime=4   # chaos / gaia  Advanced users only : rely on  screen  (see   tutorial  or the  UL HPC tutorial  on the  frontend prior to running any  oarsub  or  srun/sbatch  command to be more resilient to disconnection.  The latest version of this tutorial is available on  Github \nFinally, advanced MPI users might be interested to take a look at the  Intel Math Kernel Library Link Line Advisor .",
            "title": "UL HPC MPI Tutorial: High-Performance Linpack (HPL) benchmarking on UL HPC platform"
        },
        {
            "location": "/advanced/HPL/#objectives",
            "text": "HPL  is a  portable implementation of the High-Performance Linpack (HPL) Benchmark for Distributed-Memory Computers. It is used as reference benchmark to provide data for the  Top500  list and thus rank to supercomputers worldwide.\nHPL rely on an efficient implementation of the Basic Linear Algebra Subprograms (BLAS). You have several choices at this level:   Intel MKL  ATLAS  GotoBlas   The idea is to compare the different MPI and BLAS implementations available on the  UL HPC platform :   Intel MPI  and the Intel MKL  OpenMPI  MVAPICH2  (MPI-3 over OpenFabrics-IB, Omni-Path, OpenFabrics-iWARP, PSM, and TCP/IP)  ATLAS  GotoBlas   For the sake of time and simplicity, we will focus on the combination expected to lead to the best performant runs,  i.e.  Intel MKL and Intel MPI suite.",
            "title": "Objectives"
        },
        {
            "location": "/advanced/HPL/#pre-requisites",
            "text": "On the  access  and a  computing  node of the cluster you're working on, clone the  ULHPC/tutorials   and  ULHPC/launcher-scripts  repositories  $> cd\n$> mkdir -p git/ULHPC && cd  git/ULHPC\n$> git clone https://github.com/ULHPC/launcher-scripts.git\n$> git clone https://github.com/ULHPC/tutorials.git         # If not yet done  Prepare your working directory  $> mkdir -p ~/tutorials/HPL\n$> cd ~/tutorials/HPL\n$> ln -s ~/git/ULHPC/tutorials/advanced/HPL ref.ulhpc.d   # Keep a symlink to the reference tutorial\n$> ln -s ref.ulhpc.d/Makefile .     # symlink to the root Makefile  Fetch and uncompress the latest version of the  HPL  benchmark ( i.e.   version 2.2  at the time of writing).  $> cd ~/tutorials/HPL\n$> mkdir src\n$> cd src\n# Download the latest version\n$> export HPL_VERSION=2.2\n$> wget --no-check-certificate http://www.netlib.org/benchmark/hpl/hpl-${HPL_VERSION}.tar.gz\n$> tar xvzf hpl-${HPL_VERSION}.tar.gz\n$> cd  hpl-${HPL_VERSION}",
            "title": "Pre-requisites"
        },
        {
            "location": "/advanced/HPL/#building-the-hpl-benchmark",
            "text": "We are first going to use the  Intel Cluster Toolkit Compiler Edition , which provides Intel C/C++ and Fortran compilers, Intel MPI.  $> cd ~/tutorials/HPL\n# Load the appropriate module\n$> module spider MPI     # Search for available modules featuring MPI\n$> module load toolchain/intel   # On iris -- use 'module load toolchain/ictce' otherwise\n$> module list\nCurrently Loaded Modules:\n  1) compiler/GCCcore/6.3.0                   4) compiler/ifort/2017.1.132-GCC-6.3.0-2.27                 7) toolchain/iimpi/2017a\n  2) tools/binutils/2.27-GCCcore-6.3.0        5) toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27             8) numlib/imkl/2017.1.132-iimpi-2017a\n  3) compiler/icc/2017.1.132-GCC-6.3.0-2.27   6) mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27   9) toolchain/intel/2017a  You notice that Intel MKL is now loaded.  Read the  INSTALL  file. In particular, you'll have to edit and adapt a new makefile  Make.intel64 \n(inspired from  setup/Make.Linux_PII_CBLAS  typically)  $> cd ~/tutorials/HPL/src/hpl-2.2\n$> cp setup/Make.Linux_Intel64 Make.intel64  Once tweaked, run the compilation by:  $> make arch=intel64 clean_arch_all\n$> make arch=intel64  But  first , you will need to configure correctly the file  Make.intel64 .\nTake your favorite editor ( vim ,  nano , etc.) to modify it. In particular, you should adapt:   TOPdir  to point to the directory holding the HPL sources ( i.e.  where you uncompress them:  $(HOME)/tutorials/HPL/src/hpl-2.2 )  Adapt the  MP*  variables to point to the appropriate MPI libraries path.  (eventually) adapt the  CCFLAGS   Here is for instance a suggested difference for intel MPI:  --- setup/Make.Linux_Intel64    2016-02-24 02:10:50.000000000 +0100\n+++ Make.intel64        2017-06-12 13:48:31.016524323 +0200\n@@ -61,13 +61,13 @@\n # - Platform identifier ------------------------------------------------\n # ----------------------------------------------------------------------\n #\n-ARCH         = Linux_Intel64\n+ARCH         = $(arch)\n #\n # ----------------------------------------------------------------------\n # - HPL Directory Structure / HPL library ------------------------------\n # ----------------------------------------------------------------------\n #\n-TOPdir       = $(HOME)/hpl\n+TOPdir       = $(HOME)/tutorials/HPL/src/hpl-2.2\n INCdir       = $(TOPdir)/include\n BINdir       = $(TOPdir)/bin/$(ARCH)\n LIBdir       = $(TOPdir)/lib/$(ARCH)\n@@ -81,9 +81,9 @@\n # header files,  MPlib  is defined  to be the name of  the library to be\n # used. The variable MPdir is only used for defining MPinc and MPlib.\n #\n-# MPdir        = /opt/intel/mpi/4.1.0\n-# MPinc        = -I$(MPdir)/include64\n-# MPlib        = $(MPdir)/lib64/libmpi.a\n+MPdir        = $(I_MPI_ROOT)/intel64\n+MPinc        = -I$(MPdir)/include\n+MPlib        = $(MPdir)/lib/libmpi.a\n #\n # ----------------------------------------------------------------------\n # - Linear Algebra library (BLAS or VSIPL) -----------------------------\n@@ -178,7 +178,7 @@\n CC       = mpiicc\n CCNOOPT  = $(HPL_DEFS)\n OMP_DEFS = -openmp\n-CCFLAGS  = $(HPL_DEFS) -O3 -w -ansi-alias -i-static -z noexecstack -z relro -z now -nocompchk -Wall\n+CCFLAGS  = $(HPL_DEFS) -O3 -w -ansi-alias -i-static -z noexecstack -z relro -z now -nocompchk -Wall -xHost\n #\n # On some platforms,  it is necessary  to use the Fortran linker to find\n # the Fortran internals used in the BLAS library.  If you don't succeed by yourself, use the following  makefile :  $> cd ~/tutorials/HPL\n$> cp ref.ulhpc.d/src/hpl-2.2/Make.intel64 src/hpl-2.2/Make.intel64  Once compiled, ensure you are able to run it:  $> cd ~/tutorials/HPL/src/hpl-2.2/bin/intel64\n$> cat HPL.dat      # Default (dummy) HPL.dat  input file\n\n# On Slurm cluster (iris)\n$> srun -n $SLURM_NTASKS ./xhpl\n\n# On OAR clusters (gaia, chaos)\n$> mpirun -hostfile $OAR_NODEFILE ./xhpl",
            "title": "Building the HPL benchmark"
        },
        {
            "location": "/advanced/HPL/#preparing-batch-runs",
            "text": "We are now going to prepare launcher scripts to permit passive runs (typically in the  {default | batch}  queue).\nWe will place them in a separate directory ( runs/ ) as it will host the outcomes of the executions on the UL HPC platform .  $> cd ~/tutorials/HPL\n$> mkdir runs    # Prepare the specific run directory  Now you'll have to find the optimal set of parameters for using a single\nnode. You can use the following site: HPL Calculator  to find good parameters\nand expected performances and adapt  bin/intel64/HPL.dat  accordingly.\nHere we are going to use reasonable choices as outline from  this website",
            "title": "Preparing batch runs"
        },
        {
            "location": "/advanced/HPL/#slurm-launcher-intel-mpi",
            "text": "Copy and adapt the  default SLURM launcher  you should have a copy in  ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh  $> cd ~/tutorials/HPL/runs\n# Prepare a laucnher for intel suit\n$> cp ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh launcher-HPL.intel.sh  Take your favorite editor ( vim ,  nano , etc.) to modify it according to your needs.  Here is for instance a suggested difference for intel MPI:  --- ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh  2017-06-11 23:40:34.007152000 +0200\n+++ launcher-HPL.intel.sh       2017-06-11 23:41:57.597055000 +0200\n@@ -10,8 +10,8 @@\n #\n #          Set number of resources\n #\n-#SBATCH -N 1\n+#SBATCH -N 2\n #SBATCH --ntasks-per-node=28\n ### -c, --cpus-per-task=<ncpus>\n ###     (multithreading) Request that ncpus be allocated per process\n #SBATCH -c 1\n@@ -64,15 +64,15 @@\n module load toolchain/intel\n\n # Directory holding your built applications\n-APPDIR=\"$HOME\"\n+APPDIR=\"$HOME/tutorials/HPL/src/hpl-2.2/bin/intel64\"\n # The task to be executed i.E. your favorite Java/C/C++/Ruby/Perl/Python/R/whatever program\n # to be invoked in parallel\n-TASK=\"${APPDIR}/app.exe\"\n+TASK=\"${APPDIR}/xhpl\"\n\n # The command to run\n-CMD=\"${TASK}\"\n+# CMD=\"${TASK}\"\n ### General MPI Case:\n-# CMD=\"srun -n $SLURM_NTASKS ${TASK}\"\n+CMD=\"srun -n $SLURM_NTASKS ${TASK}\"\n ### OpenMPI case if you wish to specialize the MCA parameters\n #CMD=\"mpirun -np $SLURM_NTASKS --mca btl openib,self,sm ${TASK}\"  Now you should create an input  HPL.dat  file within the  runs/ .  $> cd ~/tutorials/HPL/runs\n$> cp ../ref.ulhpc.d/HPL.dat .\n$> ll\ntotal 0\n-rw-r--r--. 1 svarrette clusterusers 1.5K Jun 12 15:38 HPL.dat\n-rwxr-xr-x. 1 svarrette clusterusers 2.7K Jun 12 15:25 launcher-HPL.intel.sh  You are ready for testing a batch job:  $> cd ~/tutorials/HPL/runs\n$> sbatch ./launcher-HPL.intel.sh\n$> sq     # OR (long version) squeue -u $USER  (bonus)  Connect to one of the allocated nodes and run  htop  (followed by  u  to select process run under your username, and  F5  to enable the tree-view.  Now you can check the output of the HPL runs:  $> grep WR slurm-<jobid>.out    # /!\\ ADAPT <jobid> appropriately.  Of course, we made here a small test and optimizing the HPL parameters to get the best performances and efficiency out of a given HPC platform is not easy.\nBelow are some plots obtained when benchmarking the  iris  cluster and seeking the best set of parameters across increasing number of nodes (see  this blog post )",
            "title": "Slurm launcher (Intel MPI)"
        },
        {
            "location": "/advanced/HPCG/",
            "text": "-\n- mode: markdown; mode: visual-line; fill-column: 80 -\n-\n\n\nCopyright (c) 2013-2017 UL HPC Team  \nhpc-sysadmins@uni.lu\n\n\n\n\nUL HPC MPI Tutorial: High Performance Conjugate Gradients (HPCG) benchmarking on UL HPC platform\n\n\n \n \n \n \n \n\n\nThe objective of this tutorial is to compile and run one of the newest HPC benchmarks, \nHigh Performance Conjugate Gradients (HPCG)\n, on top of the\n\nUL HPC\n platform.\n\n\nYou can work in groups for this training, yet individual work is encouraged to ensure you understand and practice the usage of MPI programs on an HPC platform.\nIf not yet done, you should consider completing the \nOSU Micro-benchmark\n and \nHPL\n tutorials.\n\n\nIn all cases, ensure you are able to \nconnect to the UL HPC  clusters\n.\n\n\n# /!\\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A (at least half) COMPUTING NODE\n# Have an interactive job\n(access)$> si -n 14                                      # iris\n(access)$> srun -p interactive --qos qos-iteractive -n 14 --pty bash  # iris (long version)\n(access)$> oarsub -I -l enclosure=1/nodes=1,walltime=4   # chaos / gaia\n\n\n\n\nAdvanced users only\n: rely on \nscreen\n (see  \ntutorial\n or the \nUL HPC tutorial\n on the  frontend prior to running any \noarsub\n or \nsrun/sbatch\n command to be more resilient to disconnection.\n\n\nThe latest version of this tutorial is available on \nGithub\n.\nFinally, advanced MPI users might be interested to take a look at the \nIntel Math Kernel Library Link Line Advisor\n.\n\n\nObjectives\n\n\nThe High Performance Conjugate Gradient \nHPCG\n project is an effort to create a more relevant metric for ranking HPC systems than the High Performance LINPACK (HPL) benchmark, which is currently used in\nthe \nTop500\n ranking.\n\n\nHPCG exhibits the following patterns:\n\n Dense and sparse computations\n\n Dense and sparse collective operations\n* Data-driven parallelism (unstructured sparse triangular solves)\n\n\nFor more details, check out:\n\n \nToward a New Metric for Ranking High Performance Computing Systems\n\n\n \nTechnical specification\n\n\nHPCG is written in C++, with OpenMP and MPI parallelization capabilities, thus requires a C++ compiler with OpenMP support, and/or a MPI library.\n\n\nThe objective of this practical session is to compare the performance obtained by running HPCG\ncompiled with different compilers and options:\n\n\n\n\nHPCG + Intel C++ + Intel MPI\n\n\narchitecture native build, using the most recent supported instruction set (AVX2/FMA3)\n\n\nSSE4.1 instruction set build\n\n\n\n\n\n\nHPCG + GNU C++ + Open MPI\n\n\narchitecture native build, using the most recent supported instruction set (AVX2/FMA3)\n\n\nSSE4.1 instruction set build\n\n\n\n\n\n\n\n\nThe benchmarking tests should be performed on:\n\n\n\n\na single node\n\n\ntwo nodes, ideally belonging to the same enclosure\n\n\ntwo nodes, belonging to different enclosures\n\n\n\n\nExecutions on a single node\n\n\nHigh Performance Conjugate Gradient (HPCG) with the Intel Suite\n\n\nWe are first going to use the\n\nIntel Cluster Toolkit Compiler Edition\n,\nwhich provides Intel C/C++ and Fortran compilers, Intel MPI & Intel MKL.\n\n\nResources:\n\n\n\n\nHPCG project\n\n\n\n\nGet the latest release:\n\n\n$> mkdir ~/TP && cd ~/TP\n$> wget https://software.sandia.gov/hpcg/downloads/hpcg-2.4.tar.gz\n$> tar xvzf hpcg-2.4.tar.gz\n$> cd hpcg-2.4\n$> module avail MPI\n$> module load toolchain/ictce/7.3.5\n$> module list\nCurrently Loaded Modules:\n  1) compiler/icc/2015.3.187     3) toolchain/iccifort/2015.3.187            5) toolchain/iimpi/7.3.5                7) toolchain/ictce/7.3.5\n  2) compiler/ifort/2015.3.187   4) mpi/impi/5.0.3.048-iccifort-2015.3.187   6) numlib/imkl/11.2.3.187-iimpi-7.3.50\n$> module show mpi/impi/5.0.3.048-iccifort-2015.3.187\n\n\n\nRead the \nINSTALL\n file.\n\n\nIn particular, you'll have to edit a new makefile \nMake.intel64\n\n(inspired from \nsetup/Make.MPI_ICPC\n typically), adapting:\n\n\n\n\nthe CXX variable specifying the C++ compiler (use \nmpiicpc\n for the MPI Intel C++ wrapper)\n\n\nthe CXXFLAGS variable\nwith architecture-specific compilation flags (see \nthis Intel article\n)\n\n\n\n\nOnce the configuration file is prepared, run the compilation with:\n    $> make arch=intel64\n\n\nOnce compiled, ensure that you are able to run it:\n\n\n$> cd bin\n$> cat hpcg.\n$> mkdir intel64-optimized\n$> mv xhpcg intel64-optimized\n$> cd intel64-optimized\n$> ln -s ../hpcg.dat .\n$> mpirun -hostfile $OAR_NODEFILE ./xhpcg\n\n\n\nAs configured in the default \nhpcg.dat\n, HPCG generates a synthetic discretized three-dimensional partial differential equation model problem with Nx=Ny=Nz=104 local subgrid dimensions. NPx, NPy, NPz are a factoring of the MPI process space, giving a global domain dimension of (Nx * NPx ) * (Ny * NPy ) * (Nz * NPz).\n\n\nYou can tune Nx, Ny, Nz to increase/decrease the problem size, yet take care not to generate a problem whose local node grid representation exceeds computing node memory.\n\n\nThe result of your experiments will be stored in the directory HPCG was started in, in a \nHPCG-Benchmark-2.4_$(date).yaml\n file. Check out the benchmark result (GFLOP/s) in the final summary section:\n\n\n$> grep \"HPCG result is\" $file.yaml\n\n\n\nIn addition to the architecture optimized build, re-generate xhpcg to with the compiler options to support only the SSE4.1 instruction set (common across all UL HPC computing nodes) and perform the same experiment, in a new \nintel64-generic\n directory.\n\n\nHPCG with GNU C++ and Open MPI\n\n\nRe-compile HPCG with GNU C++, adapting the setup file  \nMake.gcc\n from \nMake.Linux_MPI\n to use the \nmpicxx\n wrapper and the GCC specific \narchitecture options\n.\n\n\n$> cd ~/TP\n$> make clean\n$> module purge\n$> module load mpi/OpenMPI/1.8.4-GCC-4.9.2\n$> make arch=gcc\n\n\n\nOnce compiled, ensure you are able to run it:\n\n\n$> cd bin\n$> cat hpcg.dat\n$> mkdir gnu-optimized\n$> mv xhpcg gnu-optimized\n$> cd gnu-optimized\n$> ln -s ../hpcg.dat .\n$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE ./xhpcg\n\n\n\nBenchmarking on two nodes\n\n\nRestart the benchmarking campaign (for both the Intel and GCC) in the following context:\n\n\n\n\n\n\n2 nodes belonging to the same enclosure. Use for that:\n\n\n$> oarsub -l enclosure=1/nodes=2,walltime=1 [\u2026]\n\n\n\n\n\n\n2 nodes belonging to the different enclosures:\n\n\n$> oarsub -l enclosure=2/core=1,walltime=1 [\u2026]\n\n\n\n\n\n\nBenchmarking with OpenMP active\n\n\nFinally, activate OpenMP support when building HPCG, adapting for the Intel and GCC suites \nMake.MPI_ICPC_OMP\n and \nMake.MPI_GCC_OMP\n respectively.\nAs before, perform single and multiple node benchmarks.\n\n\nHow is the performance result for the OpenMP+MPI vs MPI-only executions?",
            "title": "High Performance Conjugate Gradient (HPCG)"
        },
        {
            "location": "/advanced/HPCG/#ul-hpc-mpi-tutorial-high-performance-conjugate-gradients-hpcg-benchmarking-on-ul-hpc-platform",
            "text": "The objective of this tutorial is to compile and run one of the newest HPC benchmarks,  High Performance Conjugate Gradients (HPCG) , on top of the UL HPC  platform.  You can work in groups for this training, yet individual work is encouraged to ensure you understand and practice the usage of MPI programs on an HPC platform.\nIf not yet done, you should consider completing the  OSU Micro-benchmark  and  HPL  tutorials.  In all cases, ensure you are able to  connect to the UL HPC  clusters .  # /!\\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A (at least half) COMPUTING NODE\n# Have an interactive job\n(access)$> si -n 14                                      # iris\n(access)$> srun -p interactive --qos qos-iteractive -n 14 --pty bash  # iris (long version)\n(access)$> oarsub -I -l enclosure=1/nodes=1,walltime=4   # chaos / gaia  Advanced users only : rely on  screen  (see   tutorial  or the  UL HPC tutorial  on the  frontend prior to running any  oarsub  or  srun/sbatch  command to be more resilient to disconnection.  The latest version of this tutorial is available on  Github .\nFinally, advanced MPI users might be interested to take a look at the  Intel Math Kernel Library Link Line Advisor .",
            "title": "UL HPC MPI Tutorial: High Performance Conjugate Gradients (HPCG) benchmarking on UL HPC platform"
        },
        {
            "location": "/advanced/HPCG/#objectives",
            "text": "The High Performance Conjugate Gradient  HPCG  project is an effort to create a more relevant metric for ranking HPC systems than the High Performance LINPACK (HPL) benchmark, which is currently used in\nthe  Top500  ranking.  HPCG exhibits the following patterns:  Dense and sparse computations  Dense and sparse collective operations\n* Data-driven parallelism (unstructured sparse triangular solves)  For more details, check out:   Toward a New Metric for Ranking High Performance Computing Systems    Technical specification  HPCG is written in C++, with OpenMP and MPI parallelization capabilities, thus requires a C++ compiler with OpenMP support, and/or a MPI library.  The objective of this practical session is to compare the performance obtained by running HPCG\ncompiled with different compilers and options:   HPCG + Intel C++ + Intel MPI  architecture native build, using the most recent supported instruction set (AVX2/FMA3)  SSE4.1 instruction set build    HPCG + GNU C++ + Open MPI  architecture native build, using the most recent supported instruction set (AVX2/FMA3)  SSE4.1 instruction set build     The benchmarking tests should be performed on:   a single node  two nodes, ideally belonging to the same enclosure  two nodes, belonging to different enclosures",
            "title": "Objectives"
        },
        {
            "location": "/advanced/HPCG/#executions-on-a-single-node",
            "text": "",
            "title": "Executions on a single node"
        },
        {
            "location": "/advanced/HPCG/#high-performance-conjugate-gradient-hpcg-with-the-intel-suite",
            "text": "We are first going to use the Intel Cluster Toolkit Compiler Edition ,\nwhich provides Intel C/C++ and Fortran compilers, Intel MPI & Intel MKL.  Resources:   HPCG project   Get the latest release:  $> mkdir ~/TP && cd ~/TP\n$> wget https://software.sandia.gov/hpcg/downloads/hpcg-2.4.tar.gz\n$> tar xvzf hpcg-2.4.tar.gz\n$> cd hpcg-2.4\n$> module avail MPI\n$> module load toolchain/ictce/7.3.5\n$> module list\nCurrently Loaded Modules:\n  1) compiler/icc/2015.3.187     3) toolchain/iccifort/2015.3.187            5) toolchain/iimpi/7.3.5                7) toolchain/ictce/7.3.5\n  2) compiler/ifort/2015.3.187   4) mpi/impi/5.0.3.048-iccifort-2015.3.187   6) numlib/imkl/11.2.3.187-iimpi-7.3.50\n$> module show mpi/impi/5.0.3.048-iccifort-2015.3.187  Read the  INSTALL  file.  In particular, you'll have to edit a new makefile  Make.intel64 \n(inspired from  setup/Make.MPI_ICPC  typically), adapting:   the CXX variable specifying the C++ compiler (use  mpiicpc  for the MPI Intel C++ wrapper)  the CXXFLAGS variable\nwith architecture-specific compilation flags (see  this Intel article )   Once the configuration file is prepared, run the compilation with:\n    $> make arch=intel64  Once compiled, ensure that you are able to run it:  $> cd bin\n$> cat hpcg.\n$> mkdir intel64-optimized\n$> mv xhpcg intel64-optimized\n$> cd intel64-optimized\n$> ln -s ../hpcg.dat .\n$> mpirun -hostfile $OAR_NODEFILE ./xhpcg  As configured in the default  hpcg.dat , HPCG generates a synthetic discretized three-dimensional partial differential equation model problem with Nx=Ny=Nz=104 local subgrid dimensions. NPx, NPy, NPz are a factoring of the MPI process space, giving a global domain dimension of (Nx * NPx ) * (Ny * NPy ) * (Nz * NPz).  You can tune Nx, Ny, Nz to increase/decrease the problem size, yet take care not to generate a problem whose local node grid representation exceeds computing node memory.  The result of your experiments will be stored in the directory HPCG was started in, in a  HPCG-Benchmark-2.4_$(date).yaml  file. Check out the benchmark result (GFLOP/s) in the final summary section:  $> grep \"HPCG result is\" $file.yaml  In addition to the architecture optimized build, re-generate xhpcg to with the compiler options to support only the SSE4.1 instruction set (common across all UL HPC computing nodes) and perform the same experiment, in a new  intel64-generic  directory.",
            "title": "High Performance Conjugate Gradient (HPCG) with the Intel Suite"
        },
        {
            "location": "/advanced/HPCG/#hpcg-with-gnu-c-and-open-mpi",
            "text": "Re-compile HPCG with GNU C++, adapting the setup file   Make.gcc  from  Make.Linux_MPI  to use the  mpicxx  wrapper and the GCC specific  architecture options .  $> cd ~/TP\n$> make clean\n$> module purge\n$> module load mpi/OpenMPI/1.8.4-GCC-4.9.2\n$> make arch=gcc  Once compiled, ensure you are able to run it:  $> cd bin\n$> cat hpcg.dat\n$> mkdir gnu-optimized\n$> mv xhpcg gnu-optimized\n$> cd gnu-optimized\n$> ln -s ../hpcg.dat .\n$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE ./xhpcg",
            "title": "HPCG with GNU C++ and Open MPI"
        },
        {
            "location": "/advanced/HPCG/#benchmarking-on-two-nodes",
            "text": "Restart the benchmarking campaign (for both the Intel and GCC) in the following context:    2 nodes belonging to the same enclosure. Use for that:  $> oarsub -l enclosure=1/nodes=2,walltime=1 [\u2026]    2 nodes belonging to the different enclosures:  $> oarsub -l enclosure=2/core=1,walltime=1 [\u2026]",
            "title": "Benchmarking on two nodes"
        },
        {
            "location": "/advanced/HPCG/#benchmarking-with-openmp-active",
            "text": "Finally, activate OpenMP support when building HPCG, adapting for the Intel and GCC suites  Make.MPI_ICPC_OMP  and  Make.MPI_GCC_OMP  respectively.\nAs before, perform single and multiple node benchmarks.  How is the performance result for the OpenMP+MPI vs MPI-only executions?",
            "title": "Benchmarking with OpenMP active"
        },
        {
            "location": "/advanced/MATLAB1/README/",
            "text": "-\n- mode: markdown; mode: visual-line; fill-column: 80 -\n-\n\n\nAuthor: Valentin Plugaru \nValentin.Plugaru@uni.lu\n\nCopyright (c) 2013-2017 UL HPC Team  \nhpc-sysadmins@uni.lu\n\n\n\n\nMATLAB (interactive, passive and sequential jobs, checkpointing and parallel jobs) execution on the UL HPC platform\n\n\n \n \n \n \n \n \n\n\n\n\n\n\nThe objective of this tutorial is to exemplify the execution of \nMATLAB\n -\na high-level language and interactive environment for numerical computation,\nvisualization and programming, on top of the \nUL HPC\n platform.\n\n\nThe tutorial will show you:\n\n\n\n\nhow to run MATLAB in interactive mode, with either the full graphical interface or the text-mode interface\n\n\nhow to check the available toolboxes and licenses used\n\n\nhow to run MATLAB in passive (batch) mode, enabling unattended execution on the clusters\n\n\nhow to use MATLAB script (.m) files\n\n\nhow to plot data, saving the plots to file\n\n\nhow to take advantage of some of the paralelization capabilities of MATLAB to speed up your tasks\n\n\n\n\nFor the tutorial we will use the UL HPC \nGaia\n cluster that includes nodes with GPU accelerators.\n\n\nPrerequisites\n\n\nAs part of this tutorial two Matlab example scripts have been developed and you will need to download them,\nalong with their dependencies, before following the instructions in the next sections:\n\n\n    (gaia-frontend)$> mkdir -p ~/matlab-tutorial/code\n    (gaia-frontend)$> cd ~/matlab-tutorial/code\n    (gaia-frontend)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/MATLAB1/code/example1.m\n    (gaia-frontend)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/MATLAB1/code/example2.m\n    (gaia-frontend)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/MATLAB1/code/google_finance_data.m\n\n\n\nOr simply clone the full tutorials repository and make a link to the MATLAB tutorial:\n\n\n    (gaia-frontend)$> git clone https://github.com/ULHPC/tutorials.git\n    (gaia-frontend)$> ln -s tutorials/advanced/MATLAB1/ ~/matlab-tutorial\n\n\n\nMatlab execution in interactive mode\n\n\nLaunching the full graphical environment\n\n\nRunning the full MATLAB environment (e.g. on the Gaia cluster) requires an \ninteractive OAR session\n. When connecting to the clusters you will\nneed to enable X11 forwarding in order for the graphical environment to be shown on your\nlocal machine:\n\n\n\n\n\n\non Linux simply follow the commands below\n\n\n\n\n\n\non OS X (depending on version) you may not have the X Window System installed,\n  and thus will need to install \nXQuartz\n if\n  the first command below returns an 'X11 forwarding request failed on channel 0' error\n\n\n\n\n\n\non Windows you will need to run \nVcXsrv\n first\n  then to configure Putty (Connection -> SSH -> X11 -> Enable X11 forwarding) before\n  logging in to the clusters.\n\n\n# Connect to Gaia with X11 forwarding enabled (Linux/OS X):\n(yourmachine)$> ssh access-gaia.uni.lu -X\n\n# Request an interactive job (the default parameters get you 1 core for 2 hours):\n(gaia-frontend)$> oarsub -I\n\n# Check the Matlab versions installed on the clusters:\n(node)$> module spider matlab\n\n# Load a specific MATLAB version:\n(node)$> module load base/MATLAB/2017a\n\n# Check that its profile has been loaded and thus we can start to use it:\n(node)$> module list\n\n# Launch MATLAB\n(node)$> matlab\n\n\n\n\n\n\n\nAfter a delay, the full Matlab interface will be displayed on your machine and you will be able to run commands, load and edit\nscripts and generate plots. An alternative to the graphical interface is the command-line (text-mode) interface, which is\nenabled through specific parameters, described in the following section.\n\n\nLaunching the command-line environment\n\n\nRunning the text-mode MATLAB interface in an interactive session, is much faster than\nusing the full graphical environment through the network and is useful for commands/scripts testing and\nquick executions:\n\n\n    # First, connect to an UL cluster (e.g. Gaia):\n\n    (yourmachine)$> ssh access-gaia.uni.lu\n    (gaia-frontend)$> oarsub -I\n    (node)$> module load base/MATLAB/2017a\n\n    # Launch MATLAB with the graphical display mode disabled (critical parameters):\n    (node)$> matlab -nodisplay -nosplash\n    Opening log file:  /home/users/vplugaru/java.log.23247\n\n                                                                   < M A T L A B (R) >\n                                                         Copyright 1984-2017 The MathWorks, Inc.\n                                                          R2017a (9.2.0.538062) 64-bit (glnxa64)\n                                                                    February 23, 2017\n    To get started, type one of these: helpwin, helpdesk, or demo.\n    For product information, visit www.mathworks.com.\n    >> version()\n    ans =\n        '9.2.0.538062 (R2017a)\n\n\n\nIn this command line you are now able to run Matlab commands, load and edit scripts, but cannot display plots - they can\nhowever be generated and exported to file, which you will need to transfer to your own machine for visualisation.\nWhile the text mode interface is spartan, you still benefit from tab-completion (type the first few letters of\na command then press TAB twice to see possible completions) and can run the integrated help with \nhelp command_name\n\n(e.g. help plot3).\n\n\nExample usage of Matlab in interactive mode\n\n\nAt this point you should have downloaded the example scripts and started Matlab either with the graphical or the text-mode\ninterface. We will now test some Matlab commands by using the google_finance_data function defined in \ngoogle_finance_data.m\n.\nThis function downloads stock market data through the Google Finance API, and we will use it to get 1 month worth of stock data\nfor IBM (whose stock symbol is 'IBM'):\n\n\n     >> cd('~/matlab-tutorial/code/')\n     >> [hist_date, hist_high, hist_low, hist_open, hist_close, hist_vol] = google_finance_data('IBM', '2017-05-01', '2017-06-02');\n     >> size(hist_date)\n     ans =\n         24     1\n     >> [hist_date{1} ' ' hist_date{end}]\n     ans =\n         '1-May-17 2-Jun-17'\n     >> min(hist_low)\n     ans =\n       149.7900\n     >> max(hist_high)\n     ans =\n       160.4200\n     >> mean(hist_close)\n     ans =\n       153.2879\n     >> std(hist_close)\n     ans =\n         2.7618\n\n\n\nThrough these commands we have seen that the function returns column vectors, we were able to get 24 days' worth of information and\nwe used simple statistic functions to get an idea of how the stock varied in the given period.\n\n\nNow we will use the example1.m script that shows:\n  - how to use different plotting methods on the data retrieved with the google_finance_data function\n  - how to export the plots in different graphic formats instead of displaying them (which is only available when running the\n  full graphical environment and also allows the user to visually interact with the plot)\n\n\n     >> example1\n     Elapsed time is 1.709865 seconds.\n     >> quit\n     (node)$>\n     (node)$> ls *pdf *eps\n     example1-2dplot.eps  example1-2dplot.pdf  example1-scatter.eps\n\n\n\nWe have run the example1.m script which has downloaded Apple ('AAPL' ticker) stock data for the year 2016 and generated three plots:\n\n\n\n\nexample1-2dplot.pdf : color PDF generated with the saveas function, plotting dates (x-axis) vs closing stock price (y-axis)\n\n\nexample1-2dplot.eps : high quality black and white Enhanced PostScript (EPS) generated with the print function, same data as above\n\n\nexample1-scatter.eps : high quality color EPS generated with the print function, showing also the trading volume (z-axis) and\nusing different color datapoints (red) where the closing share price was above 100\n\n\n\n\nThe script has also used the tic/toc Matlab commands to time it's execution and we can see it took less than 2 seconds to download\nand process data from the Google Finance API and generate the plots.\n\n\nFinally, we have closed our Matlab session and were returned to the cluster's command line prompt where we found the generated plots.\n\n\nA PNG version of the latter two plots is shown below:\n\n\n\n\n\nFurther examples showing serial and parallel executions are given below in the 'Example usage of Matlab in passive mode' section.\n\n\nChecking available toolboxes and license status\n\n\nIn order to be able to run MATLAB and specific features provided through the various MATLAB toolboxes, sufficient licenses need to\nbe available. The state of the licenses can be checked with the \nlmstat\n utility.\n\n\nFirst, we will check that the license server is running (an \nUP\n status should be shown in the output of lmutil):\n\n\n     (node)$> module load base/MATLAB\n     (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic\n\n\n\nNext, we can check the total number of MATLAB licenses available (issued) and how many are used:\n\n\n     (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic -f MATLAB\n\n\n\nTo check for a specific feature and its usage (e.g. the financial toolbox if we know its name):\n\n\n     (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic -f Financial_toolbox\n\n\n\nTo see all available toolboxes:\n\n\n     (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic -a\n\n\n\nChecking the availability of statistics toolboxes (if we don't know the exact name, but that 'stat' is in the name):\n\n\n     (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic -a | grep -i stat\n\n\n\nFinally, checking the available toolboxes (but with no information on the specific # of available/used licenses), can be done directly from MATLAB, e.g.:\n\n\n     (node)$> module load base/MATLAB/2017a\n     (node)$> matlab -nodesktop -nodisplay\n     Opening log file:  /home/users/vplugaru/java.log.6343\n                                                                 < M A T L A B (R) >\n                                                       Copyright 1984-2017 The MathWorks, Inc.\n                                                        R2017a (9.2.0.538062) 64-bit (glnxa64)\n                                                                  February 23, 2017\n\n\n     To get started, type one of these: helpwin, helpdesk, or demo.\n     For product information, visit www.mathworks.com.\n\n     >> ver\n     ----------------------------------------------------------------------------------------------------\n     MATLAB Version: 9.2.0.538062 (R2017a)\n     MATLAB License Number: 886910\n     Operating System: Linux 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2+deb8u3 (2017-08-15) x86_64\n     Java Version: Java 1.7.0_60-b19 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode\n     ----------------------------------------------------------------------------------------------------\n     MATLAB                                                Version 9.2         (R2017a)\n     Simulink                                              Version 8.9         (R2017a)\n     Aerospace Blockset                                    Version 3.19        (R2017a)\n     Aerospace Toolbox                                     Version 2.19        (R2017a)\n     Antenna Toolbox                                       Version 2.2         (R2017a)\n     Bioinformatics Toolbox                                Version 4.8         (R2017a)\n     Communications System Toolbox                         Version 6.4         (R2017a)\n     Computer Vision System Toolbox                        Version 7.3         (R2017a)\n     Control System Toolbox                                Version 10.2        (R2017a)\n     Curve Fitting Toolbox                                 Version 3.5.5       (R2017a)\n     DSP System Toolbox                                    Version 9.4         (R2017a)\n     Database Toolbox                                      Version 7.1         (R2017a)\n     Datafeed Toolbox                                      Version 5.5         (R2017a)\n     Econometrics Toolbox                                  Version 4.0         (R2017a)\n     Embedded Coder                                        Version 6.12        (R2017a)\n     Filter Design HDL Coder                               Version 3.1.1       (R2017a)\n     Financial Instruments Toolbox                         Version 2.5         (R2017a)\n     Financial Toolbox                                     Version 5.9         (R2017a)\n     Fixed-Point Designer                                  Version 5.4         (R2017a)\n     Fuzzy Logic Toolbox                                   Version 2.2.25      (R2017a)\n     Global Optimization Toolbox                           Version 3.4.2       (R2017a)\n     HDL Coder                                             Version 3.10        (R2017a)\n     HDL Verifier                                          Version 5.2         (R2017a)\n     Image Acquisition Toolbox                             Version 5.2         (R2017a)\n     Image Processing Toolbox                              Version 10.0        (R2017a)\n     Instrument Control Toolbox                            Version 3.11        (R2017a)\n     LTE System Toolbox                                    Version 2.4         (R2017a)\n     MATLAB Coder                                          Version 3.3         (R2017a)\n     MATLAB Compiler                                       Version 6.4         (R2017a)\n     MATLAB Compiler SDK                                   Version 6.3.1       (R2017a)\n     MATLAB Report Generator                               Version 5.2         (R2017a)\n     Mapping Toolbox                                       Version 4.5         (R2017a)\n     Model Predictive Control Toolbox                      Version 5.2.2       (R2017a)\n     Neural Network Toolbox                                Version 10.0        (R2017a)\n     Optimization Toolbox                                  Version 7.6         (R2017a)\n     Parallel Computing Toolbox                            Version 6.10        (R2017a)\n     Partial Differential Equation Toolbox                 Version 2.4         (R2017a)\n     Phased Array System Toolbox                           Version 3.4         (R2017a)\n     RF Blockset                                           Version 6.0         (R2017a)\n     RF Blockset                                           Version 6.0         (R2017a)\n     RF Toolbox                                            Version 3.2         (R2017a)\n     Robotics System Toolbox                               Version 1.4         (R2017a)\n     Robust Control Toolbox                                Version 6.3         (R2017a)\n     Signal Processing Toolbox                             Version 7.4         (R2017a)\n     SimBiology                                            Version 5.6         (R2017a)\n     SimEvents                                             Version 5.2         (R2017a)\n     Simscape                                              Version 4.2         (R2017a)\n     Simscape Driveline                                    Version 2.12        (R2017a)\n     Simscape Electronics                                  Version 2.11        (R2017a)\n     Simscape Fluids                                       Version 2.2         (R2017a)\n     Simscape Multibody                                    Version 5.0         (R2017a)\n     Simscape Power Systems                                Version 6.7         (R2017a)\n     Simulink 3D Animation                                 Version 7.7         (R2017a)\n     Simulink Code Inspector                               Version 3.0         (R2017a)\n     Simulink Coder                                        Version 8.12        (R2017a)\n     Simulink Control Design                               Version 4.5         (R2017a)\n     Simulink Design Optimization                          Version 3.2         (R2017a)\n     Simulink Design Verifier                              Version 3.3         (R2017a)\n     Simulink Report Generator                             Version 5.2         (R2017a)\n     Simulink Test                                         Version 2.2         (R2017a)\n     Simulink Verification and Validation                  Version 3.13        (R2017a)\n     Stateflow                                             Version 8.9         (R2017a)\n     Statistics and Machine Learning Toolbox               Version 11.1        (R2017a)\n     Symbolic Math Toolbox                                 Version 7.2         (R2017a)\n     System Identification Toolbox                         Version 9.6         (R2017a)\n     Trading Toolbox                                       Version 3.2         (R2017a)\n     Vision HDL Toolbox                                    Version 1.4         (R2017a)\n     Wavelet Toolbox                                       Version 4.18        (R2017a)\n\n\n\nMatlab execution in passive mode\n\n\nFor non-interactive or long executions, MATLAB can be ran in passive mode, reading all commands from\nan input file you provide (e.g. named INPUTFILE.m) and saving the results in an output file (e.g. named OUTPUTFILE.out),\nby either:\n\n\n\n\n\n\nusing redirection operators:\n\n\n$> matlab -nodisplay -nosplash < INPUTFILE.m > OUTPUTFILE.out\n\n\n\n\n\n\n\nrunning the input file as a command (notice the missing '.m' extension) and copying output\n(as a log) to the output file:\n\n\n$> matlab -nodisplay -nosplash -r INPUTFILE -logfile OUTPUTFILE.out\n\n\n\n\n\n\n\nThe second usage mode is recommended as it corresponds to the batch-mode execution. In the first case your\noutput file will contain the '>>' characters generated by Matlab as if ran interactively, along with the\nresults of your own commands.\n\n\nHowever as the second usage mode runs your script as a command, it \nmust\n contain the \nquit\n command at\nthe end in order to close Matlab, otherwise after the script has executed Matlab will stay open,\nwaiting for further input until the end of the walltime you set for the passive job, tying up compute\nresources needlessly.\n\n\nThe following minimal example shows how to run a serial (1 core) MATLAB script for 24 hours in passive mode:\n\n\n    (gaia-frontend)$> oarsub -l walltime=24:00:00 \"source /etc/profile; module load base/MATLAB; matlab -nodisplay -nosplash < INPUTFILE.m > OUTPUTFILE.out\"\n\n\n\nIdeally you \nwould not\n run MATLAB jobs like this but instead \ncreate/adapt a launcher script\n to contain those instructions. A minimal shell script (e.g. named 'your_matlab_launcher.sh') could be:\n\n\n    #!/bin/bash\n    source /etc/profile\n    # REMEMBER to change the following to the correct paths of the input/output files:\n    INPUTFILE=your_input_file_name_without_extension\n    OUTPUTFILE=your_output_file_name_with_extension.out\n    # Load a specific version of MATLAB and run the input script:\n    module load base/MATLAB/2017a\n    matlab -nodisplay -nosplash -r $INPUTFILE -logfile $OUTPUTFILE\n\n\n\nthen launch it in a job (e.g. requesting 6 cores on 1 node for 10 hours - assuming your input file takes advantage of the parallel cores):\n\n\n    (gaia-frontend)$> oarsub -l nodes=1/core=6,walltime=10:00:00 your_matlab_launcher.sh\n\n\n\nRemember! that the Matlab script you run with the '-r' parameter must contain the \nquit\n command at the end\nin order to close Matlab properly when the script finishes.\n\n\nExample usage of Matlab in passive mode\n\n\nIn this section we will use the \nexample2.m\n script which shows:\n  - the serial execution of time consuming operations; 1 core on 1 node\n  - the parallel execution (based on the \nparfor\n command) and relative speedup vs serial execution, setting\n    the maximum number of parallel threads through environment variables; up to 1 full node\n  - GPU-based parallel execution; available only on \nGPU-enabled nodes\n\n\nBy default the parallel section of the script uses up to 4 threads, thus for a first test we will:\n\n\n\n\ncreate a \nlauncher script\n called matlab-minlauncher.sh (paste in your terminal from \ncat\n to \nEOF\n - \nsee here how this is done\n, \nor create the file yourself with the respective content)\n\n\nmake it executable by setting the corresponding filesystem permission (\nsee here for more details\n)\n\n\nrequest from the OAR scheduler 4 cores on 1 compute node for 5 minutes\n\n\n\n\nwait until the job completes its execution (see its status with \noarstat -j $JOBID\n and full details with \noarstat -f -j $JOBID\n):\n\n\n(gaia-frontend)$> cd ~/matlab-tutorial/code\n  (gaia-frontend)$> cat << EOF > matlab-minlauncher.sh\n  #!/bin/bash\n  source /etc/profile\n  module load base/MATLAB/2017a\n  cd ~/matlab-tutorial/code\n  matlab -nodisplay -nosplash -r example2 -logfile example2.out\n  EOF\n  (gaia-frontend)$> chmod +x matlab-minlauncher.sh\n  (gaia-frontend)$> oarsub -l nodes=1/core=4,walltime=00:05:00 ~/matlab-tutorial/code/matlab-minlauncher.sh\n  (gaia-frontend)$> cat example2.out\n                         < M A T L A B (R) >\n               Copyright 1984-2017 The MathWorks, Inc.\n                R2017a (9.2.0.538062) 64-bit (glnxa64)\n                          February 23, 2017\n\n\nTo get started, type one of these: helpwin, helpdesk, or demo.\n  For product information, visit www.mathworks.com.\n\n\n-- Will perform 24 iterations on a 1000x1000 matrix\n\n\n-- Serial test\n  -- Execution time: 15.143097s.\n  -- Parallel tests with up to 4 cores\n\n\n-- Parallel test using 2 cores\n  Starting parallel pool (parpool) using the 'local' profile ...\n  connected to 2 workers.\n  Parallel pool using the 'local' profile is shutting down.\n  -- Execution time: 10.588108s.\n  -- Execution time with overhead: 51.693702s.\n\n\n-- Parallel test using 3 cores\n  Starting parallel pool (parpool) using the 'local' profile ...\n  connected to 3 workers.\n  Parallel pool using the 'local' profile is shutting down.\n  -- Execution time: 6.646897s.\n  -- Execution time with overhead: 21.025083s.\n\n\n-- Parallel test using 4 cores\n  Starting parallel pool (parpool) using the 'local' profile ...\n  connected to 4 workers.\n  Parallel pool using the 'local' profile is shutting down.\n  -- Execution time: 5.196755s.\n  -- Execution time with overhead: 19.772026s.\n\n\n-- Number of processes, parallel execution time (s), parallel execution time with overhead(s), speedup, speedup with overhead:\n    1.0000   15.1431   15.1431    1.0000    1.0000\n    2.0000   10.5881   51.6937    1.4302    0.2929\n    3.0000    6.6469   21.0251    2.2782    0.7202\n    4.0000    5.1968   19.7720    2.9140    0.7659\n\n\n-- GPU-Parallel test not available on this system.\n\n\n\n\n\n\nThe next launcher script is also able to read an environment variable \nMATLABMP\n and create as many parallel threads as specified in this variable.\nWe will now generate another launcher which will set this variable to the number of cores we specified to OAR.\n\n\n(gaia-frontend)$> cd ~/matlab-tutorial/code\n(gaia-frontend)$> cat << EOF > matlab-minlauncher2.sh\n#!/bin/bash\nsource /etc/profile\nmodule load base/MATLAB/2017a\ncd ~/matlab-tutorial/code\nexport MATLABMP=\\$(cat $OAR_NODEFILE | wc -l)\nmatlab -nodisplay -nosplash -r example2 -logfile example2b.out\nEOF\n(gaia-frontend)$> chmod +x matlab-minlauncher2.sh\n(gaia-frontend)$> oarsub -l nodes=1/core=6,walltime=00:05:00 ~/matlab-tutorial/code/matlab-minlauncher2.sh\n# we now wait for the job to complete execution - check its progress yourself with oarstat\n(gaia-frontend)$> head -n 17 example2b.out\n                       < M A T L A B (R) >\n             Copyright 1984-2017 The MathWorks, Inc.\n              R2017a (9.2.0.538062) 64-bit (glnxa64)\n                        February 23, 2017\n\nTo get started, type one of these: helpwin, helpdesk, or demo.\nFor product information, visit www.mathworks.com.\n\n-- Will perform 24 iterations on a 1000x1000 matrix\n\n-- Serial test\n-- Execution time: 15.227198s.\n\n-- Found environment variable MATLABMP=6.\n-- Parallel tests with up to 6 cores\n(gaia-frontend)$>\n\n\n\nWe have submitted an OAR job requesting 6 cores for 5 minutes and used the second launcher. It can be seen that the example2.m script has read the MATLABMP environment variable and has used in its execution. Take a look at the MATLAB code to see how this is accomplished.\n\n\nAs shown previously, the jobs we have submitted did not run on GPU-enabled nodes, thus in this last example we will specifically target GPU nodes and see that the last test of example2.m will also be executed.\nBefore testing the following commands, edit the \nmatlab-minlauncher2.sh\n script and make MATLAB store its output in a \nexample2c.out\n\nfile.\n\n\n  (gaia-frontend)$> cd ~/matlab-tutorial/code\n  (gaia-frontend)$> oarsub -l nodes=1/core=6,walltime=00:05:00 -p \"gpu='YES'\" ~/matlab-tutorial/code/matlab-minlauncher2.sh\n  # now wait for the job to complete execution, then check the output file\n  (gaia-frontend)$> tail -n 5 example2c.out\n  -- GPU test\n  -- GPU Execution time: 19.809493s.\n  -- GPU Execution time with overhead: 21.399610s.\n  -- GPU vs Serial speedup: 1.249748.\n  -- GPU with overhead vs Serial speedup: 1.156884.\n\n\n\nThe following plot shows a sample speedup obtained by using parfor on Gaia, with up to 12 parallel threads:\n\n\n\nRelative to the fast execution of the inner instruction (which calculates the eigenvalues of a matrix)\nthe overhead given by the creation of the parallel pool and the task assignations is quite high in this example,\nwhere for 12 cores the speedup is 5.26x but taking the overhead into account it is only 4x.\n\n\nUseful references\n\n\n\n\nGetting Started with Parallel Computing Toolbox\n\n\nParallel for-Loops (parfor) documentation\n\n\nGPU Computing documentation",
            "title": "MATLAB"
        },
        {
            "location": "/advanced/MATLAB1/README/#matlab-interactive-passive-and-sequential-jobs-checkpointing-and-parallel-jobs-execution-on-the-ul-hpc-platform",
            "text": "The objective of this tutorial is to exemplify the execution of  MATLAB  -\na high-level language and interactive environment for numerical computation,\nvisualization and programming, on top of the  UL HPC  platform.  The tutorial will show you:   how to run MATLAB in interactive mode, with either the full graphical interface or the text-mode interface  how to check the available toolboxes and licenses used  how to run MATLAB in passive (batch) mode, enabling unattended execution on the clusters  how to use MATLAB script (.m) files  how to plot data, saving the plots to file  how to take advantage of some of the paralelization capabilities of MATLAB to speed up your tasks   For the tutorial we will use the UL HPC  Gaia  cluster that includes nodes with GPU accelerators.",
            "title": "MATLAB (interactive, passive and sequential jobs, checkpointing and parallel jobs) execution on the UL HPC platform"
        },
        {
            "location": "/advanced/MATLAB1/README/#prerequisites",
            "text": "As part of this tutorial two Matlab example scripts have been developed and you will need to download them,\nalong with their dependencies, before following the instructions in the next sections:      (gaia-frontend)$> mkdir -p ~/matlab-tutorial/code\n    (gaia-frontend)$> cd ~/matlab-tutorial/code\n    (gaia-frontend)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/MATLAB1/code/example1.m\n    (gaia-frontend)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/MATLAB1/code/example2.m\n    (gaia-frontend)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/MATLAB1/code/google_finance_data.m  Or simply clone the full tutorials repository and make a link to the MATLAB tutorial:      (gaia-frontend)$> git clone https://github.com/ULHPC/tutorials.git\n    (gaia-frontend)$> ln -s tutorials/advanced/MATLAB1/ ~/matlab-tutorial",
            "title": "Prerequisites"
        },
        {
            "location": "/advanced/MATLAB1/README/#matlab-execution-in-interactive-mode",
            "text": "",
            "title": "Matlab execution in interactive mode"
        },
        {
            "location": "/advanced/MATLAB1/README/#launching-the-full-graphical-environment",
            "text": "Running the full MATLAB environment (e.g. on the Gaia cluster) requires an  interactive OAR session . When connecting to the clusters you will\nneed to enable X11 forwarding in order for the graphical environment to be shown on your\nlocal machine:    on Linux simply follow the commands below    on OS X (depending on version) you may not have the X Window System installed,\n  and thus will need to install  XQuartz  if\n  the first command below returns an 'X11 forwarding request failed on channel 0' error    on Windows you will need to run  VcXsrv  first\n  then to configure Putty (Connection -> SSH -> X11 -> Enable X11 forwarding) before\n  logging in to the clusters.  # Connect to Gaia with X11 forwarding enabled (Linux/OS X):\n(yourmachine)$> ssh access-gaia.uni.lu -X\n\n# Request an interactive job (the default parameters get you 1 core for 2 hours):\n(gaia-frontend)$> oarsub -I\n\n# Check the Matlab versions installed on the clusters:\n(node)$> module spider matlab\n\n# Load a specific MATLAB version:\n(node)$> module load base/MATLAB/2017a\n\n# Check that its profile has been loaded and thus we can start to use it:\n(node)$> module list\n\n# Launch MATLAB\n(node)$> matlab    After a delay, the full Matlab interface will be displayed on your machine and you will be able to run commands, load and edit\nscripts and generate plots. An alternative to the graphical interface is the command-line (text-mode) interface, which is\nenabled through specific parameters, described in the following section.",
            "title": "Launching the full graphical environment"
        },
        {
            "location": "/advanced/MATLAB1/README/#launching-the-command-line-environment",
            "text": "Running the text-mode MATLAB interface in an interactive session, is much faster than\nusing the full graphical environment through the network and is useful for commands/scripts testing and\nquick executions:      # First, connect to an UL cluster (e.g. Gaia):\n\n    (yourmachine)$> ssh access-gaia.uni.lu\n    (gaia-frontend)$> oarsub -I\n    (node)$> module load base/MATLAB/2017a\n\n    # Launch MATLAB with the graphical display mode disabled (critical parameters):\n    (node)$> matlab -nodisplay -nosplash\n    Opening log file:  /home/users/vplugaru/java.log.23247\n\n                                                                   < M A T L A B (R) >\n                                                         Copyright 1984-2017 The MathWorks, Inc.\n                                                          R2017a (9.2.0.538062) 64-bit (glnxa64)\n                                                                    February 23, 2017\n    To get started, type one of these: helpwin, helpdesk, or demo.\n    For product information, visit www.mathworks.com.\n    >> version()\n    ans =\n        '9.2.0.538062 (R2017a)  In this command line you are now able to run Matlab commands, load and edit scripts, but cannot display plots - they can\nhowever be generated and exported to file, which you will need to transfer to your own machine for visualisation.\nWhile the text mode interface is spartan, you still benefit from tab-completion (type the first few letters of\na command then press TAB twice to see possible completions) and can run the integrated help with  help command_name \n(e.g. help plot3).",
            "title": "Launching the command-line environment"
        },
        {
            "location": "/advanced/MATLAB1/README/#example-usage-of-matlab-in-interactive-mode",
            "text": "At this point you should have downloaded the example scripts and started Matlab either with the graphical or the text-mode\ninterface. We will now test some Matlab commands by using the google_finance_data function defined in  google_finance_data.m .\nThis function downloads stock market data through the Google Finance API, and we will use it to get 1 month worth of stock data\nfor IBM (whose stock symbol is 'IBM'):       >> cd('~/matlab-tutorial/code/')\n     >> [hist_date, hist_high, hist_low, hist_open, hist_close, hist_vol] = google_finance_data('IBM', '2017-05-01', '2017-06-02');\n     >> size(hist_date)\n     ans =\n         24     1\n     >> [hist_date{1} ' ' hist_date{end}]\n     ans =\n         '1-May-17 2-Jun-17'\n     >> min(hist_low)\n     ans =\n       149.7900\n     >> max(hist_high)\n     ans =\n       160.4200\n     >> mean(hist_close)\n     ans =\n       153.2879\n     >> std(hist_close)\n     ans =\n         2.7618  Through these commands we have seen that the function returns column vectors, we were able to get 24 days' worth of information and\nwe used simple statistic functions to get an idea of how the stock varied in the given period.  Now we will use the example1.m script that shows:\n  - how to use different plotting methods on the data retrieved with the google_finance_data function\n  - how to export the plots in different graphic formats instead of displaying them (which is only available when running the\n  full graphical environment and also allows the user to visually interact with the plot)       >> example1\n     Elapsed time is 1.709865 seconds.\n     >> quit\n     (node)$>\n     (node)$> ls *pdf *eps\n     example1-2dplot.eps  example1-2dplot.pdf  example1-scatter.eps  We have run the example1.m script which has downloaded Apple ('AAPL' ticker) stock data for the year 2016 and generated three plots:   example1-2dplot.pdf : color PDF generated with the saveas function, plotting dates (x-axis) vs closing stock price (y-axis)  example1-2dplot.eps : high quality black and white Enhanced PostScript (EPS) generated with the print function, same data as above  example1-scatter.eps : high quality color EPS generated with the print function, showing also the trading volume (z-axis) and\nusing different color datapoints (red) where the closing share price was above 100   The script has also used the tic/toc Matlab commands to time it's execution and we can see it took less than 2 seconds to download\nand process data from the Google Finance API and generate the plots.  Finally, we have closed our Matlab session and were returned to the cluster's command line prompt where we found the generated plots.  A PNG version of the latter two plots is shown below:   Further examples showing serial and parallel executions are given below in the 'Example usage of Matlab in passive mode' section.",
            "title": "Example usage of Matlab in interactive mode"
        },
        {
            "location": "/advanced/MATLAB1/README/#checking-available-toolboxes-and-license-status",
            "text": "In order to be able to run MATLAB and specific features provided through the various MATLAB toolboxes, sufficient licenses need to\nbe available. The state of the licenses can be checked with the  lmstat  utility.  First, we will check that the license server is running (an  UP  status should be shown in the output of lmutil):       (node)$> module load base/MATLAB\n     (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic  Next, we can check the total number of MATLAB licenses available (issued) and how many are used:       (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic -f MATLAB  To check for a specific feature and its usage (e.g. the financial toolbox if we know its name):       (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic -f Financial_toolbox  To see all available toolboxes:       (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic -a  Checking the availability of statistics toolboxes (if we don't know the exact name, but that 'stat' is in the name):       (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic -a | grep -i stat  Finally, checking the available toolboxes (but with no information on the specific # of available/used licenses), can be done directly from MATLAB, e.g.:       (node)$> module load base/MATLAB/2017a\n     (node)$> matlab -nodesktop -nodisplay\n     Opening log file:  /home/users/vplugaru/java.log.6343\n                                                                 < M A T L A B (R) >\n                                                       Copyright 1984-2017 The MathWorks, Inc.\n                                                        R2017a (9.2.0.538062) 64-bit (glnxa64)\n                                                                  February 23, 2017\n\n\n     To get started, type one of these: helpwin, helpdesk, or demo.\n     For product information, visit www.mathworks.com.\n\n     >> ver\n     ----------------------------------------------------------------------------------------------------\n     MATLAB Version: 9.2.0.538062 (R2017a)\n     MATLAB License Number: 886910\n     Operating System: Linux 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2+deb8u3 (2017-08-15) x86_64\n     Java Version: Java 1.7.0_60-b19 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode\n     ----------------------------------------------------------------------------------------------------\n     MATLAB                                                Version 9.2         (R2017a)\n     Simulink                                              Version 8.9         (R2017a)\n     Aerospace Blockset                                    Version 3.19        (R2017a)\n     Aerospace Toolbox                                     Version 2.19        (R2017a)\n     Antenna Toolbox                                       Version 2.2         (R2017a)\n     Bioinformatics Toolbox                                Version 4.8         (R2017a)\n     Communications System Toolbox                         Version 6.4         (R2017a)\n     Computer Vision System Toolbox                        Version 7.3         (R2017a)\n     Control System Toolbox                                Version 10.2        (R2017a)\n     Curve Fitting Toolbox                                 Version 3.5.5       (R2017a)\n     DSP System Toolbox                                    Version 9.4         (R2017a)\n     Database Toolbox                                      Version 7.1         (R2017a)\n     Datafeed Toolbox                                      Version 5.5         (R2017a)\n     Econometrics Toolbox                                  Version 4.0         (R2017a)\n     Embedded Coder                                        Version 6.12        (R2017a)\n     Filter Design HDL Coder                               Version 3.1.1       (R2017a)\n     Financial Instruments Toolbox                         Version 2.5         (R2017a)\n     Financial Toolbox                                     Version 5.9         (R2017a)\n     Fixed-Point Designer                                  Version 5.4         (R2017a)\n     Fuzzy Logic Toolbox                                   Version 2.2.25      (R2017a)\n     Global Optimization Toolbox                           Version 3.4.2       (R2017a)\n     HDL Coder                                             Version 3.10        (R2017a)\n     HDL Verifier                                          Version 5.2         (R2017a)\n     Image Acquisition Toolbox                             Version 5.2         (R2017a)\n     Image Processing Toolbox                              Version 10.0        (R2017a)\n     Instrument Control Toolbox                            Version 3.11        (R2017a)\n     LTE System Toolbox                                    Version 2.4         (R2017a)\n     MATLAB Coder                                          Version 3.3         (R2017a)\n     MATLAB Compiler                                       Version 6.4         (R2017a)\n     MATLAB Compiler SDK                                   Version 6.3.1       (R2017a)\n     MATLAB Report Generator                               Version 5.2         (R2017a)\n     Mapping Toolbox                                       Version 4.5         (R2017a)\n     Model Predictive Control Toolbox                      Version 5.2.2       (R2017a)\n     Neural Network Toolbox                                Version 10.0        (R2017a)\n     Optimization Toolbox                                  Version 7.6         (R2017a)\n     Parallel Computing Toolbox                            Version 6.10        (R2017a)\n     Partial Differential Equation Toolbox                 Version 2.4         (R2017a)\n     Phased Array System Toolbox                           Version 3.4         (R2017a)\n     RF Blockset                                           Version 6.0         (R2017a)\n     RF Blockset                                           Version 6.0         (R2017a)\n     RF Toolbox                                            Version 3.2         (R2017a)\n     Robotics System Toolbox                               Version 1.4         (R2017a)\n     Robust Control Toolbox                                Version 6.3         (R2017a)\n     Signal Processing Toolbox                             Version 7.4         (R2017a)\n     SimBiology                                            Version 5.6         (R2017a)\n     SimEvents                                             Version 5.2         (R2017a)\n     Simscape                                              Version 4.2         (R2017a)\n     Simscape Driveline                                    Version 2.12        (R2017a)\n     Simscape Electronics                                  Version 2.11        (R2017a)\n     Simscape Fluids                                       Version 2.2         (R2017a)\n     Simscape Multibody                                    Version 5.0         (R2017a)\n     Simscape Power Systems                                Version 6.7         (R2017a)\n     Simulink 3D Animation                                 Version 7.7         (R2017a)\n     Simulink Code Inspector                               Version 3.0         (R2017a)\n     Simulink Coder                                        Version 8.12        (R2017a)\n     Simulink Control Design                               Version 4.5         (R2017a)\n     Simulink Design Optimization                          Version 3.2         (R2017a)\n     Simulink Design Verifier                              Version 3.3         (R2017a)\n     Simulink Report Generator                             Version 5.2         (R2017a)\n     Simulink Test                                         Version 2.2         (R2017a)\n     Simulink Verification and Validation                  Version 3.13        (R2017a)\n     Stateflow                                             Version 8.9         (R2017a)\n     Statistics and Machine Learning Toolbox               Version 11.1        (R2017a)\n     Symbolic Math Toolbox                                 Version 7.2         (R2017a)\n     System Identification Toolbox                         Version 9.6         (R2017a)\n     Trading Toolbox                                       Version 3.2         (R2017a)\n     Vision HDL Toolbox                                    Version 1.4         (R2017a)\n     Wavelet Toolbox                                       Version 4.18        (R2017a)",
            "title": "Checking available toolboxes and license status"
        },
        {
            "location": "/advanced/MATLAB1/README/#matlab-execution-in-passive-mode",
            "text": "For non-interactive or long executions, MATLAB can be ran in passive mode, reading all commands from\nan input file you provide (e.g. named INPUTFILE.m) and saving the results in an output file (e.g. named OUTPUTFILE.out),\nby either:    using redirection operators:  $> matlab -nodisplay -nosplash < INPUTFILE.m > OUTPUTFILE.out    running the input file as a command (notice the missing '.m' extension) and copying output\n(as a log) to the output file:  $> matlab -nodisplay -nosplash -r INPUTFILE -logfile OUTPUTFILE.out    The second usage mode is recommended as it corresponds to the batch-mode execution. In the first case your\noutput file will contain the '>>' characters generated by Matlab as if ran interactively, along with the\nresults of your own commands.  However as the second usage mode runs your script as a command, it  must  contain the  quit  command at\nthe end in order to close Matlab, otherwise after the script has executed Matlab will stay open,\nwaiting for further input until the end of the walltime you set for the passive job, tying up compute\nresources needlessly.  The following minimal example shows how to run a serial (1 core) MATLAB script for 24 hours in passive mode:      (gaia-frontend)$> oarsub -l walltime=24:00:00 \"source /etc/profile; module load base/MATLAB; matlab -nodisplay -nosplash < INPUTFILE.m > OUTPUTFILE.out\"  Ideally you  would not  run MATLAB jobs like this but instead  create/adapt a launcher script  to contain those instructions. A minimal shell script (e.g. named 'your_matlab_launcher.sh') could be:      #!/bin/bash\n    source /etc/profile\n    # REMEMBER to change the following to the correct paths of the input/output files:\n    INPUTFILE=your_input_file_name_without_extension\n    OUTPUTFILE=your_output_file_name_with_extension.out\n    # Load a specific version of MATLAB and run the input script:\n    module load base/MATLAB/2017a\n    matlab -nodisplay -nosplash -r $INPUTFILE -logfile $OUTPUTFILE  then launch it in a job (e.g. requesting 6 cores on 1 node for 10 hours - assuming your input file takes advantage of the parallel cores):      (gaia-frontend)$> oarsub -l nodes=1/core=6,walltime=10:00:00 your_matlab_launcher.sh  Remember! that the Matlab script you run with the '-r' parameter must contain the  quit  command at the end\nin order to close Matlab properly when the script finishes.",
            "title": "Matlab execution in passive mode"
        },
        {
            "location": "/advanced/MATLAB1/README/#example-usage-of-matlab-in-passive-mode",
            "text": "In this section we will use the  example2.m  script which shows:\n  - the serial execution of time consuming operations; 1 core on 1 node\n  - the parallel execution (based on the  parfor  command) and relative speedup vs serial execution, setting\n    the maximum number of parallel threads through environment variables; up to 1 full node\n  - GPU-based parallel execution; available only on  GPU-enabled nodes  By default the parallel section of the script uses up to 4 threads, thus for a first test we will:   create a  launcher script  called matlab-minlauncher.sh (paste in your terminal from  cat  to  EOF  -  see here how this is done , \nor create the file yourself with the respective content)  make it executable by setting the corresponding filesystem permission ( see here for more details )  request from the OAR scheduler 4 cores on 1 compute node for 5 minutes   wait until the job completes its execution (see its status with  oarstat -j $JOBID  and full details with  oarstat -f -j $JOBID ):  (gaia-frontend)$> cd ~/matlab-tutorial/code\n  (gaia-frontend)$> cat << EOF > matlab-minlauncher.sh\n  #!/bin/bash\n  source /etc/profile\n  module load base/MATLAB/2017a\n  cd ~/matlab-tutorial/code\n  matlab -nodisplay -nosplash -r example2 -logfile example2.out\n  EOF\n  (gaia-frontend)$> chmod +x matlab-minlauncher.sh\n  (gaia-frontend)$> oarsub -l nodes=1/core=4,walltime=00:05:00 ~/matlab-tutorial/code/matlab-minlauncher.sh\n  (gaia-frontend)$> cat example2.out\n                         < M A T L A B (R) >\n               Copyright 1984-2017 The MathWorks, Inc.\n                R2017a (9.2.0.538062) 64-bit (glnxa64)\n                          February 23, 2017  To get started, type one of these: helpwin, helpdesk, or demo.\n  For product information, visit www.mathworks.com.  -- Will perform 24 iterations on a 1000x1000 matrix  -- Serial test\n  -- Execution time: 15.143097s.\n  -- Parallel tests with up to 4 cores  -- Parallel test using 2 cores\n  Starting parallel pool (parpool) using the 'local' profile ...\n  connected to 2 workers.\n  Parallel pool using the 'local' profile is shutting down.\n  -- Execution time: 10.588108s.\n  -- Execution time with overhead: 51.693702s.  -- Parallel test using 3 cores\n  Starting parallel pool (parpool) using the 'local' profile ...\n  connected to 3 workers.\n  Parallel pool using the 'local' profile is shutting down.\n  -- Execution time: 6.646897s.\n  -- Execution time with overhead: 21.025083s.  -- Parallel test using 4 cores\n  Starting parallel pool (parpool) using the 'local' profile ...\n  connected to 4 workers.\n  Parallel pool using the 'local' profile is shutting down.\n  -- Execution time: 5.196755s.\n  -- Execution time with overhead: 19.772026s.  -- Number of processes, parallel execution time (s), parallel execution time with overhead(s), speedup, speedup with overhead:\n    1.0000   15.1431   15.1431    1.0000    1.0000\n    2.0000   10.5881   51.6937    1.4302    0.2929\n    3.0000    6.6469   21.0251    2.2782    0.7202\n    4.0000    5.1968   19.7720    2.9140    0.7659  -- GPU-Parallel test not available on this system.    The next launcher script is also able to read an environment variable  MATLABMP  and create as many parallel threads as specified in this variable.\nWe will now generate another launcher which will set this variable to the number of cores we specified to OAR.  (gaia-frontend)$> cd ~/matlab-tutorial/code\n(gaia-frontend)$> cat << EOF > matlab-minlauncher2.sh\n#!/bin/bash\nsource /etc/profile\nmodule load base/MATLAB/2017a\ncd ~/matlab-tutorial/code\nexport MATLABMP=\\$(cat $OAR_NODEFILE | wc -l)\nmatlab -nodisplay -nosplash -r example2 -logfile example2b.out\nEOF\n(gaia-frontend)$> chmod +x matlab-minlauncher2.sh\n(gaia-frontend)$> oarsub -l nodes=1/core=6,walltime=00:05:00 ~/matlab-tutorial/code/matlab-minlauncher2.sh\n# we now wait for the job to complete execution - check its progress yourself with oarstat\n(gaia-frontend)$> head -n 17 example2b.out\n                       < M A T L A B (R) >\n             Copyright 1984-2017 The MathWorks, Inc.\n              R2017a (9.2.0.538062) 64-bit (glnxa64)\n                        February 23, 2017\n\nTo get started, type one of these: helpwin, helpdesk, or demo.\nFor product information, visit www.mathworks.com.\n\n-- Will perform 24 iterations on a 1000x1000 matrix\n\n-- Serial test\n-- Execution time: 15.227198s.\n\n-- Found environment variable MATLABMP=6.\n-- Parallel tests with up to 6 cores\n(gaia-frontend)$>  We have submitted an OAR job requesting 6 cores for 5 minutes and used the second launcher. It can be seen that the example2.m script has read the MATLABMP environment variable and has used in its execution. Take a look at the MATLAB code to see how this is accomplished.  As shown previously, the jobs we have submitted did not run on GPU-enabled nodes, thus in this last example we will specifically target GPU nodes and see that the last test of example2.m will also be executed.\nBefore testing the following commands, edit the  matlab-minlauncher2.sh  script and make MATLAB store its output in a  example2c.out \nfile.    (gaia-frontend)$> cd ~/matlab-tutorial/code\n  (gaia-frontend)$> oarsub -l nodes=1/core=6,walltime=00:05:00 -p \"gpu='YES'\" ~/matlab-tutorial/code/matlab-minlauncher2.sh\n  # now wait for the job to complete execution, then check the output file\n  (gaia-frontend)$> tail -n 5 example2c.out\n  -- GPU test\n  -- GPU Execution time: 19.809493s.\n  -- GPU Execution time with overhead: 21.399610s.\n  -- GPU vs Serial speedup: 1.249748.\n  -- GPU with overhead vs Serial speedup: 1.156884.  The following plot shows a sample speedup obtained by using parfor on Gaia, with up to 12 parallel threads:  Relative to the fast execution of the inner instruction (which calculates the eigenvalues of a matrix)\nthe overhead given by the creation of the parallel pool and the task assignations is quite high in this example,\nwhere for 12 cores the speedup is 5.26x but taking the overhead into account it is only 4x.",
            "title": "Example usage of Matlab in passive mode"
        },
        {
            "location": "/advanced/MATLAB1/README/#useful-references",
            "text": "Getting Started with Parallel Computing Toolbox  Parallel for-Loops (parfor) documentation  GPU Computing documentation",
            "title": "Useful references"
        },
        {
            "location": "/advanced/R/",
            "text": "Introduction to R\n\n\nCopyright (c) 2014 Joseph Emeras \njoseph.emeras@uni.lu\n\n\nCopyright (c) 2017 Aur\u00e9lien Ginolhac \naurelien.ginolhac@uni.lu\n\n\n\n\nR Tutorial\n\n\nThrough this tutorial you will learn how to use R from your local machine or from one of the \nUL HPC platform\n clusters. We will also use the \nggplot\n library to generate nice graphics and export them as pdf files. Then, we will see how to organize and group data. Finally we will illustrate how R can benefit from multicore and cluster parallelization.\n\n\nWarning: this tutorial does not focus on the learning of R language but aims at showing you nice startup tips.\nIf you're also looking for a good tutorial on R's data structures you can take a look at: \nHadley Wickham's page\n. Another \nbookdown\n's book is available for free: \nR for Data Science\n by Garrett Grolemund & Hadley Wickham\n\n\nConventions used in this tutorial:\n\n\n\n\ncommands that have to be typed on the cluster start with a prompt like this: \njdoe@access:~$\n\n\ncommands that have to be typed on your local machine start with a prompt like this: \njdoe@localhost:~$\n\n\ncode blocks containing one or several \n>\n should not be pasted \"as it\", they are meant for you to observe the output of each function; others can be pasted in R terminal \"as it\".\n\n\n\n\nlecture: introduction to R\n\n\nsee slides:\n\n\n\n\nhere as html\n\n\nhere as pdf\n\n\n\n\nPre-requisites\n\n\nOptional: On your local machine\n\n\nFirst of all, let's install R. You will find releases for various distributions available at \nCRAN Archive\n. Once installed, to use R interactive session interface, simply open a terminal and type:\n\n\njdoe@localhost:~$ R\n\n\n\nYou will also find handy to use the \nR-Studio\n graphical IDE. R-Studio embeds a R shell where you can call R functions as in the interactive session interface. Thus you can use whether R interactive shell or R-Studio embedded shell.\n\n\nOn the cluster\n\n\nR is already available in \nchaos\n, \ngaia\n and \niris\n clusters as a module. Only \niris\n has the latest version \n3.4.0\n. The first step is the reservation of a resource. Connect to your favorite cluster frontend (here: \niris\n). We assume you have already configured your \n.ssh/config\n.\n\n\niris\n\n\njdoe@localhost:~$ ssh iris-cluster\n\n\n\nOnce connected to the user frontend, book 1 core for half an hour (as we will use R in single-threaded mode, we will need only one core).\n\n\n[jdoe@access2 ~]$ srun -p interactive --qos qos-interactive --time=0:30:0 --pty bash\n\n\n\nWhen the job is running and you are connected load \nR\n module. For a complete list of availbale modules see: \nSoftware page\n.\n\n\n[jdoe@access2 ~]$ module load lang/R\n\n\n\nNow you should be able to invoke R and see something like this:\n\n\n[jdoe@iris-081 ~]$  R\n\nR version 3.4.0 (2017-04-21) -- \"You Stupid Darkness\"\nCopyright (C) 2017 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n>\n\n\n\nsessionInfo()\n function gives information about R version, loaded libraries etc.\n\n\n> sessionInfo()\nR version 3.4.0 (2017-04-21)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: CentOS Linux 7 (Core)\n\nMatrix products: default\nBLAS: /mnt/irisgpfs/apps/resif/data/production/v0.1-20170602/default/software/lang/R/3.4.0-intel-2017a-X11-20170314-bare/lib64/R/lib/libR.so\nLAPACK: /mnt/irisgpfs/apps/resif/data/production/v0.1-20170602/default/software/lang/R/3.4.0-intel-2017a-X11-20170314-bare/lib64/R/modules/lapack.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base\n\nloaded via a namespace (and not attached):\n[1] compiler_3.4.0\n>\n\n\n\ngaia\n\n\njdoe@localhost:~$ ssh gaia-cluster\n\n\n\nOnce connected to the user frontend, book 1 core for half an hour (as we will use R in single-threaded mode, we will need only one core).\n\n\njdoe@access(gaia-cluster) ~ $ oarsub -I -l core=1,walltime=0:30\n\n\n\nWhen the job is running and you are connected load \nR\n module. For a complete list of availbale modules see: \nSoftware page\n.\n\n\njdoe@access2:~$ module load lang/R\n\n\n\nNow you should be able to invoke R and see something like this:\n\n\njdoe@gaia-59(gaia-cluster)[OAR4159732->29] ~ $ R\n\nR version 3.3.0 (2016-05-03) -- \"Supposedly Educational\"\nCopyright (C) 2016 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n>\n\n\n\nsessionInfo()\n function gives information about R version, loaded libraries etc.\n\n\n> sessionInfo()\nR version 3.3.0 (2016-05-03)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Debian GNU/Linux 7 (wheezy)\n\nlocale:\n [1] LC_CTYPE=en_US.ISO-8859-15       LC_NUMERIC=C                    \n [3] LC_TIME=en_US.ISO-8859-15        LC_COLLATE=en_US.ISO-8859-15    \n [5] LC_MONETARY=en_US.ISO-8859-15    LC_MESSAGES=en_US.ISO-8859-15   \n [7] LC_PAPER=en_US.ISO-8859-15       LC_NAME=C                       \n [9] LC_ADDRESS=C                     LC_TELEPHONE=C                  \n[11] LC_MEASUREMENT=en_US.ISO-8859-15 LC_IDENTIFICATION=C\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base\n>\n\n\n\nInstalling R Packages\n\n\nTo install libraries you can use the \ninstall.packages()\n function. \ne.g\n\n\ninstall.packages(\"ggplot2\")\n\n\nThis will install the \nggplot2\n library.\n\n\nNote: on the first run, R might ask you various questions during the installation. e.g. selecting a CRAN mirror to use for downloading packages. Select a mirror close to your location. For other questions, using default values is ok.\n\n\nNow, to load this library call:\n\n\nlibrary(ggplot2)\n\n\nA call to \nsessionInfo()\n function will return \nggplot2\n version as it is now attached to the current session.\n\n\nWarm-up Session -- Simple Plotting\n\n\nFrom Single Dataset\n\n\nMovies dataset, derived from data provided by \nIMDB\n is a sample dataset available as a pacakge in \nggplot2movies\n for testing purpose. Its data description can be found \nhere\n. Thus, when loading \nggplot2movies\n library, this dataset is available under the name: \nmovies\n.\n\n\n(OPTIONAL) An other way to get the dataset would be to download, extract and read it by using the \nreadr\n library (part of the \ntidyverse\n) that can uncompress by itself:\n\n\nlibrary(readr)\nmovies <- read_csv(\"https://github.com/hadley/ggplot2movies/raw/master/data-raw/movies.csv\")\n\n\n\n\n## Parsed with column specification:\n## cols(\n##   .default = col_double(),\n##   title = col_character(),\n##   year = col_integer(),\n##   length = col_integer(),\n##   budget = col_integer(),\n##   votes = col_integer(),\n##   mpaa = col_character(),\n##   Action = col_integer(),\n##   Animation = col_integer(),\n##   Comedy = col_integer(),\n##   Drama = col_integer(),\n##   Documentary = col_integer(),\n##   Romance = col_integer(),\n##   Short = col_integer()\n## )\n\n## See spec(...) for full column specifications.\n\n\n\nNow let's take a (reproducible) sample of 1000 movies and plot their distribution regarding their rating.\n\n\nlibrary(ggplot2)            # load ggplot2 library to use packages functions\nset.seed(5689)              # set the seed for random selection used in `sample()` function\n# movies is the data.frame name, from this data.frame, randomly select 1000 rows\nmovies_sample <- movies[sample(nrow(movies), 1000), ]   \n# construct the graph -- movies_sample will be used as data, we will plot an histogram where x=movies_sample$rating and with a bin size=0.5\ngraph <- ggplot(movies_sample) + geom_histogram(aes(x = rating), binwidth = 0.5)        \nggsave(graph, file = \"movies_hist.pdf\", width = 8, height = 4)                              # save the graph in a pdf file\n\n\n\n\nNow you retrieve the generated pdf on your local workstation for visualization:\n\n\njdoe@localhost:~$ scp iris-cluster:movies_hist.pdf .\n\n\n\nggplot2\n proposes many functions to plot data according to your needs. Do not hesitate to wander in the \nggplot2 documentation\n and to read at provided examples to better understand how to use it. The \nggsave()\n function is convenient to export ggplot graphics as \n.pdf\n, \n.jpg\n or \n.png\n files\n\n\nyou should see:\n\n\n\n\nFrom Several Datasets\n\n\nNow, let's say we have two different datasets: \ndiamonds_fair\n and \ndiamonds_good\n that are both extracts from the \ndiamonds\n dataset (provided in the \nggplot2\n dataset). In this example we will consider that these two datasets come from different sources, so do not try to understand the next lines, they are just here to setup the example (simply copy-paste these in your R prompt).\n\n\nset.seed(2109)  \ndiamonds_fair <- data.frame(carat = diamonds$carat[which(diamonds$cut == 'Fair')],\n                            price = diamonds$price[which(diamonds$cut == 'Fair')])\ndiamonds_fair <- diamonds_fair[sample(nrow(diamonds_fair), 20), ]\ndiamonds_good <- data.frame(carat = diamonds$carat[which(diamonds$cut == 'Good')],\n                            price = diamonds$price[which(diamonds$cut == 'Good')])\ndiamonds_good <- diamonds_good[sample(nrow(diamonds_good), 20), ]\n\n\n\n\nTo know the class of an R object you can use the \nclass()\n function\n\n\nclass(diamonds_fair)\n\n\n\n\n## [1] \"data.frame\"\n\n\n\nSo we have these two datasets, being of class dataframe. In R, a \ndata.frame\n is one kind of data structure whose columns have names and that can contain several rows. Basically it looks like a matrix with columns identified by an index \nand\n a name, and with rows identified by an index. Let's check how they are organized with the \nnames()\n function that gives a dataset column names.\n\n\nnames(diamonds_fair)\n## [1] \"carat\" \"price\"\nnames(diamonds_good)\n## [1] \"carat\" \"price\"\n\n\n\n\nThus for each dataset row we have the price and the carat value for a given diamond. We want to add a column to datasets that will describe from which one it comes from, then we will merge these into one single dataset.\n\n\ndiamonds_fair <- cbind(diamonds_fair, cut_class = \"Fair\")                               # add a column named cut_class with all values being \"Fair\" to data.frame diamonds_fair\ndiamonds_good <- cbind(diamonds_good, cut_class = \"Good\")                               # same with \"Good\"\ndiamonds_merge <- rbind(diamonds_fair, diamonds_good)                                   # combine the 2 data.frame with `rbind()` as they both have the same structure\n\n\n\n\ncbind()\n function is used to add a column to a dataframe, \nrbind()\n to combine rows of two dataframes (c is for column, r is for row). Now we have all data merged in a dataframe and a column that describes the origin of data (the column \ncut_class\n), let's plot data.\n\n\nNote: To visualize an extract of your data you can do:\n\n\ndiamonds_merge[1:10,]       # returns rows 1 to 10\n\n\n\n\n##      carat price cut_class\n## 297   1.01  3801      Fair\n## 1104  0.46  1035      Fair\n## 623   1.01  5538      Fair\n## 35    0.91  2854      Fair\n## 290   1.51  3765      Fair\n## 1560  0.96  2540      Fair\n## 590   1.01  5226      Fair\n## 192   0.90  3332      Fair\n## 563   1.00  5027      Fair\n## 1213  0.89  1334      Fair\n\n\n\ndiamonds_merge[, 3]             # returns column no.3\n\n\n\n\n##  [1] Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair\n## [15] Fair Fair Fair Fair Fair Fair Good Good Good Good Good Good Good Good\n## [29] Good Good Good Good Good Good Good Good Good Good Good Good\n## Levels: Fair Good\n\n\n\ndiamonds_merge$cut_class        # returns column named cut_class\n\n\n\n\n##  [1] Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair\n## [15] Fair Fair Fair Fair Fair Fair Good Good Good Good Good Good Good Good\n## [29] Good Good Good Good Good Good Good Good Good Good Good Good\n## Levels: Fair Good\n\n\n\nThen we construct and save the graph.\n\n\nthis time we use ggplot's function \ngeom_point()\n to plot data points. colour=cut_class aestetics option will plot the points according to cut_class values\n\n\ngraph <- ggplot(data = diamonds_merge) + \n  geom_point(aes(x = carat, y = price, colour = cut_class))\nggsave(graph, file = \"diamonds_plot.pdf\", width=8, height=4)\n\n\n\n\nRemember, to get help about a particular function you can type \n?function_name\n. e.g.\n\n\n?cbind\n\n\n\n\nTo get package and meta information on a function you can type \n??function_name\n. e.g.\n\n\n??ggsave\n\n\n\n\nOrganizing your Data\n\n\nLet's say we are working with the full \ndiamonds\n dataset and we want to have the \naverage\n price for a given diamond \ncut\n.\n\n\nnames(diamonds)\n\n\n\n\n##  [1] \"carat\"   \"cut\"     \"color\"   \"clarity\" \"depth\"   \"table\"   \"price\"  \n##  [8] \"x\"       \"y\"       \"z\"\n\n\n\nWe could do a for loop to aggregate the data per cuts and manually compute the average price, but in R loops are generally a bad idea. For large datasets it is very long to compute. Thus instead of looping around the dataset, we will use a function from the \ndplyr\n package part of the \ntidyverse\n idiom\n\n\ndplyr\n from the tidyverse\n\n\nYou will first need to install and load \ndplyr\n.\n\n\ninstall.packages(\"dplyr\")\n\n\n\n\nOf note, all core packages of the \ntidyverse\n could be installed at once\n\n\ninstall.packages(\"tidyverse\")\n\n\n\n\nFirst of all, we chain the commands using the \npipe\n \n%>%\n. That avoids many parenthesis and keep the natural reading from left to right.\n\n\nThe first parameter will be the dataset, the second will be the column of the dataset we want to group the results on, third parameter will be the call to the \nsummarise()\n function that will enable to aggregate data on the \ncut\n column.\n\n\nlibrary(dplyr)\ndiamonds %>%\n  group_by(cut) %>%\n  summarise(avg_price = mean(price))\n\n\n\n\n## # A tibble: 5 x 2\n##         cut avg_price\n##       <ord>     <dbl>\n## 1      Fair  4358.758\n## 2      Good  3928.864\n## 3 Very Good  3981.760\n## 4   Premium  4584.258\n## 5     Ideal  3457.542\n\n\n\nNote: \nsummarise()\n from the \ndplyr\n package is similar to \naggregate()\n from base package, you can use indifferently one or the other, \ndplyr\n functions simply provide a more consistent naming convention together with better performance\n\n\naggregate\n from base\n\n\naggregate(price ~ cut,\n          FUN = mean,\n          data = diamonds)\n\n\n\n\n##         cut    price\n## 1      Fair 4358.758\n## 2      Good 3928.864\n## 3 Very Good 3981.760\n## 4   Premium 4584.258\n## 5     Ideal 3457.542\n\n\n\nlapply\n from base\n\n\nIn the previous example we used \naggregate\n for the \naggregation\n, we could also have used \nlapply\n (but in a slightlier more complicated way):\n\n\nas.data.frame(cbind(cut = as.character(unique(diamonds$cut)), avg_price = lapply(unique(diamonds$cut), function(x) mean(diamonds$price[which(diamonds$cut == x)]))))\n\n\n\n\n##         cut avg_price\n## 1     Ideal  3457.542\n## 2   Premium  4584.258\n## 3      Good  3928.864\n## 4 Very Good   3981.76\n## 5      Fair  4358.758\n\n\n\ndata.table\n\n\ndata.table\n is a package without dependencies that is super fast. Although the syntax is harder to learn compare to \ndplyr\n. See this long thread \nat stackoverflow\n for more details.\n\n\n# install.package(\"data.table\")\nlibrary(data.table)\n\n\n\n\n## \n## Attaching package: 'data.table'\n\n## The following objects are masked from 'package:dplyr':\n## \n##     between, first, last\n\n## The following object is masked from 'package:purrr':\n## \n##     transpose\n\n\n\nDT <- data.table(diamonds)\nDT[, mean(price), by = cut]\n\n\n\n\n##          cut       V1\n## 1:     Ideal 3457.542\n## 2:   Premium 4584.258\n## 3:      Good 3928.864\n## 4: Very Good 3981.760\n## 5:      Fair 4358.758\n\n\n\nplyr\n\n\nFor completeness, we could add \nddply\n from \nplyr\n the first attempt of Hadley Wickham to make data manipulation easier.\n\n\nlibrary(plyr)\n\n\n\n\n## -------------------------------------------------------------------------\n\n## You have loaded plyr after dplyr - this is likely to cause problems.\n## If you need functions from both plyr and dplyr, please load plyr first, then dplyr:\n## library(plyr); library(dplyr)\n\n## -------------------------------------------------------------------------\n\n## \n## Attaching package: 'plyr'\n\n## The following objects are masked from 'package:dplyr':\n## \n##     arrange, count, desc, failwith, id, mutate, rename, summarise,\n##     summarize\n\n## The following object is masked from 'package:purrr':\n## \n##     compact\n\n\n\nddply(diamonds, .(cut), summarize, avg_price = mean(price))\n\n\n\n\n##         cut avg_price\n## 1      Fair  4358.758\n## 2      Good  3928.864\n## 3 Very Good  3981.760\n## 4   Premium  4584.258\n## 5     Ideal  3457.542\n\n\n\nPerfomance Considerations\n\n\nSo, we want to know which one of the two versions is the most efficient, for that purpose, the library \nmicrobenchmark\n is handy.\n\n\ninstall.packages(\"microbenchmark\")\n\n\n\n\nWe can use the \nmicrobenchmark()\n function on several expressions, with a given repetition number to compare them:\n\n\nlibrary(dplyr) # dplyr needs to be loaded after plyr when both are used\nlibrary(microbenchmark)\nm <- microbenchmark(LAPPLY    = as.data.frame(cbind(cut = as.character(unique(diamonds$cut)), avg_price = lapply(unique(diamonds$cut), function(x) mean(diamonds$price[which(diamonds$cut == x)])))),\n                   AGGREGATE = aggregate(price ~ cut, FUN = mean, data = diamonds),\n                   DDPLY     = ddply(diamonds, .(cut), summarize, avg_price = mean(price)),\n                   DPLYR     = diamonds %>% group_by(cut) %>% summarise(avg_price = mean(price)),\n                   DATATABLE = DT[, .(mean(price)), by = cut],\n                   times     = 1000)\nm\n\n\n\n\n## Unit: microseconds\n##       expr       min        lq      mean    median        uq       max\n##     LAPPLY  9728.227 11430.066 14327.419 14451.795 15378.028  94718.08\n##  AGGREGATE 34857.006 39880.578 43258.622 41155.558 42766.251 127360.33\n##      DDPLY  9868.561 11775.282 15316.425 14737.079 15663.672  93199.60\n##      DPLYR  2724.228  2964.753  3223.255  3064.992  3199.719  29266.28\n##  DATATABLE   946.433  1204.498  1460.516  1303.794  1398.279   6723.17\n##  neval\n##   1000\n##   1000\n##   1000\n##   1000\n##   1000\n\n\n\nPlotting the benchmark\n\n\nresult gives us a boxplot graph if you use the base function \nplot\n\n\nplot(m)\n\n\n\n\n\n\nif you want to save as a \npng\n file\n\n\npng(\"benchmark_boxplot.png\")\n## plot the graph\nplot(m)\n## flush the output device to save the graph\ndev.off()                   \n\n\n\n\nOf note, you can use \nggplot2\n via the \nautoplot\n function\n\n\nautoplot(m)\n\n\n\n\n\n\nUsing the \ndata.table\n package\n\n\nAccording to the \ndata.table documentation\n \ndata.table\n inherits from \ndata.frame\n to offer fast subset, fast grouping, fast update, fast ordered joins and list columns in a short and flexible syntax, for faster development.\n\n\nIt uses binary search instead of vector scan to perform its operations and thus is scalable. We can convert easily a \ndata.frame\n to a \ndata.table\n.\n\n\nload the \ndata.table\n package then convert the \ndata.frame\n to a \ndata.table\n:\n\n\nMOVIES <- data.table(movies)\n\n\n\n\nAs \ndata.table\n uses binary search, we have to define manually the keys that will be used for this search, this is done with \nsetkey()\n function.\n\n\nLet's now create a new \ndata.frame\n. We will make it large enough (10 million rows, 676 groups) to demonstrate the difference between a vector scan and a binary search.\n\n\ngrpsize <- ceiling(1e7 / 26^2) \nsystem.time(DF <- data.frame(x = rep(LETTERS, each = 26 * grpsize),\n                             y = rep(letters, each = grpsize),\n                             v = runif(grpsize * 26^2),\n                             stringsAsFactors = FALSE)\n)   \n\n\n\n\n##    user  system elapsed \n##   0.910   0.067   0.982\n\n\n\nThis generated a data.frame named DF with 3 columns.\n\n\n\n\nColumn \nx\n is a repetition of uppercase letters from A to Z\n\n\ncolumn \ny\n is minorcase letters\n\n\nColumn \nv\n is a random uniform value.\n\n\n\n\nTo illustrate the difference, we take as example the selection in this large dataset of rows where \nx == \"R\"\n and \ny == \"h\"\n.\n\n\nvector scan\n\n\nwe select rows where x=\"R\" and y=\"h\". For this we have to scan the full data.frame twice.\n\n\nsystem.time(ans1 <- DF[DF$x==\"R\" & DF$y==\"h\",])         \n\n\n\n\n##    user  system elapsed \n##   0.316   0.074   0.411\n\n\n\nbinary search\n\n\nWe select rows that match the join between DT and the data.table row: data.table(\"R\",\"h\"). This will return the same result as before but much faster.\n\n\nDT <- data.table(DF)                                    # convert the data.frame to a data.table\nsetkey(DT, x, y)                                          # set column x and y as data.table keys.\nsystem.time(ans2 <- DT[J(\"R\", \"h\")])                    \n\n\n\n\n##    user  system elapsed \n##   0.002   0.001   0.003\n\n\n\nIn the first case, we scan the full table twice (once for selecting x's that are equal to \"R\", then y's that are equal to \"h\"), then do the selection. In the second case, we are joining DT to the 1 row, 2 column table returned by data.table(\"R\",\"h\"). We use the alias for joining data.tables called J(), short for join. As we defined x and y as keys, this works like a database join. You can see that vector scan is very long compared to binary search.\n\n\nGrouping\n\n\ndata.table\n also provides faster operations for reading files and grouping data.\n\n\nNow you can compare the same aggregation operation with \ndata.frame\n and \ndata.table\n. In both examples we aggregate on x and apply the function \nsum()\n to corresponding v.\n\n\ndata.frame\n and \nbase\n style:\n\n\nsystem.time(tapply(DT$v, DT$x, sum))\n\n\n\n\n##    user  system elapsed \n##   0.500   0.164   0.686\n\n\n\ndata.table\n style, using \nby\n:\n\n\nsystem.time(DT[, sum(v), by = x])\n\n\n\n\n##    user  system elapsed \n##   0.150   0.014   0.177\n\n\n\nQuestion: use \ndplyr\n instead of \ntapply()\n in the first example.\n\n\nQuestion: return the min and max instead of the sum using data.table\n\n\nParallel R\n\n\nThe first part of the tutorial is now over, you can connect to \ngaia\n cluster and submit an other job requesting several machines.\n\n\n  jdoe@localhost:~$ ssh gaia-cluster\njdoe@access(gaia-cluster) ~$ oarsub -I -l nodes=1,walltime=1\n\n\n\n\n\n\nWhen the job is running and you are connected load R module (version compiled with Intel Compiler), then run R.\n\n\njdoe@access:~$ module load lang/R\n  jdoe@access:~$ R\n\n\n\nWe will use a large dataset (400K+ rows) to illustrate the effect of parallelization in R (as dataset is large, the following line may take time to complete depending on your network speed).\n\n\n> air <- read.csv(url(\"http://packages.revolutionanalytics.com/datasets/AirOnTimeCSV2012/airOT201201.csv\"))\n\n\n\nNOTE\n: If downloading the air dataset (above line) takes too much time you can load it from a file on the cluster:\n\n\n> load(\"~jemeras/data/air.rda\")\n\n\n\n(For information you can save your \nair\n object just as showed above with: \nsave(air,file= \"~/data/air.rda\")\n)\n\n\nIf we want to have the number of flights for each destination \nDEST\n we can do the following:\n\n\ndests <- as.character(unique(air$DEST))\n  count_flights <- function(x) length(which(air$DEST == x))\n  as.data.frame(cbind(dest = dests, nb = lapply(dests, count_flights)))\n\n\n\nAs the dataframe is large it takes some time to compute\n\n\n> microbenchmark(LAPPLY = lapply(dests, count_flights), times = 10)\nUnit: seconds\n   expr      min       lq   median       uq      max neval\n LAPPLY 1.607961 1.609036 1.609638 1.610269 2.023961    10\n\n\n\nSingle Machine Parallelization\n\n\nTo parallelize the lapply function we can use \nmclapply()\n from \nparallel\n package and give it the number of cores to use. \nmclapply()\n uses the underlying operating system fork() functionality to achieve parallelization. Using several cores makes the process shorter.\n\n\nlibrary(parallel)\nas.data.frame(cbind(dest = dests, nb = mclapply(dests, count_flights, mc.cores = 12)))\n\nlibrary(microbenchmark)\nmicrobenchmark(MCLAPPLY = mclapply(dests, count_flights, mc.cores = 12), times = 20)  # or use `detectCores()` from `parallel` package instead of giving cores value.\n\n\n\n\n  Unit: milliseconds\n       expr      min       lq   median       uq     max neval\n   MCLAPPLY 233.8035 235.1089 235.9138 236.6393 263.934    10\n\n\n\nIt is nice to visualize all your cores working on your node with \nhtop\n for example. You can connect to the same node from another terminal by typing:\n\n\n  jdoe@access:~$ oarstat -u\n  Job id     Name           User           Submission Date     S Queue\n  ---------- -------------- -------------- ------------------- - ----------\n  6664321                   jdoe           2015-06-03 14:24:23 R default\n\n\n\nNote the \nJob id\n field. Put this job id in the next command:\n\n\n  jdoe@access:~$ oarsub -C 6664321\n\n\n\nThen \nhtop\n will show you your cores working if you call again the \nmclapply()\n function.\n\n\n\n\nFinally you can save the \nair\n R object to reuse it in an other R session.\n\n\n> save(air, file = \"./air.rda\")\n\n\n\nThen quit your current R session but \ndo not\n end your current \noar\n job.\n\n\ndplyr\n version\n\n\nlibrary(dplyr)\ncount(air, DEST)\nmicrobenchmark(DPLYR = count(air, DEST), times = 20)\n\n\n\n\nUnit: milliseconds\n  expr      min       lq     mean   median       uq     max neval\n DPLYR 25.66517 25.86669 26.40936 26.11667 26.83215 28.1973    20\n\n\n\nOptimised code may be better than parallelisation. Of note, parallel \ndplyr\n exists: \nmultidplyr\n.\n\n\nCluster Parallelization\n\n\nThe \nparLapply()\n function will create a cluster of processes, which could even reside on different machines on the network, and they communicate via TCP/IP or MPI in order to pass the tasks and results between each other. Thus you have to load necessary packages and export necessary data and functions to the global environment of the cluster workers.\n\n\nFirst, add the module loading at bash login too for enabling it on the nodes. To do so, within a shell type:\n\n\necho \"module load lang/R\" >> ~/.bash_login      # /!\\ TO REMOVE AT THE END OF PS /!\\\n  module purge\n  module load lang/R\n\n\n\nWarning: do not forget to clean your ~/.bash_login file after the PS (remove the 'module load lang/R/3.2.0-ictce-7.3.5-bare' line).\n\n\nSocket Communications\n\n\nFirst, let's load data and initialize variables.\n\n\nlibrary(parallel)\n\n  # air = read.csv(url(\"http://packages.revolutionanalytics.com/datasets/AirOnTimeCSV2012/airOT201201.csv\"))  # load again the air data.table from http\nload(\"./air.rda\")   # read saved R object from file \"air.rda\"\n  dests = as.character(unique(air$DEST))\n  count_flights = function(x){length(which(air$DEST == x))}\n\n  ## set cluster characteristics -- get OAR nodes to use, type of communication\n  nodes = scan(Sys.getenv(\"OAR_NODE_FILE\"), what=character(0))\n  oar_job_id = as.numeric(Sys.getenv(\"OAR_JOB_ID\"))\n  connector = paste0(\"OAR_JOB_ID=\", oar_job_id)\n  connector = paste0(connector, \" ~jemeras/bin/roarsh\")\n  comm_type = \"PSOCK\"\n\n\n\nThen, setup the cluster.\n\n\n  ## set up the cluster\n  cl = makeCluster(nodes, type = comm_type, rshcmd = connector) \n  ## If a particular library <LIB> is needed, load it on the nodes with\n  # clusterEvalQ(cl, { library(<LIB>) })\n  ## or give the full environment with\n  # clusterEvalQ(cl, sessionInfo())\n  ## export air dataset on all the nodes\n  clusterExport(cl, varlist=c(\"air\"))\n\n\n\nDo the parallel computation.\n\n\n  ## compute in parallel through sockets\n  as.data.frame(cbind(dest=dests, nb=parLapply(cl, dests, count_flights)))\n\n\n\nFinalize and cleanup things.\n\n\n  stopCluster(cl)\n\n\n\nExercise: Plot a speedup graph with different number of cores and/or machines used.\n\n\nMPI Communications\n\n\nIn this section we will use the same example as previously but with MPI connections. For that purpose we need to install two libraries: \nrmpi\n and \nsnow\n.\n\n\nAs we are using R compiled with Intel compiler we will have to specify manually some paths and which version of MPI we are using when installing \nrmpi\n.\n\n\n> install.packages(\"Rmpi\",\n                   configure.args =\n                   c(paste(\"--with-Rmpi-include=\",Sys.getenv(\"EBROOTIMPI\"),\"/include64\",sep=\"\"),\n                     paste(\"--with-Rmpi-libpath=\",Sys.getenv(\"EBROOTIMPI\"),\"/lib64\",sep=\"\"),\n                     paste(\"--with-mpi=\",Sys.getenv(\"EBROOTIMPI\"),sep=\"\"),\n                     \"--with-Rmpi-type=OPENMPI\"))\n\n\n\nNOTE\n: if the installation fails you can try using the module with \nRmpi\n already installed:\n\n\n  module purge\n  module load lang/R/3.2.0-ictce-7.3.5-Rmpi\n\n\n\nThen, outside of R shell write a file named \nparallelAirDests.R\n with the following code.\n\n\nWarning\n: when using parallelization in R, please keep in mind that communication is much slower than computation. Thus if your problem is involving a long computation then you are right to use it, otherwise, if your problem is a large quantity of data you must use \ndata.table\n or \ndplyr\n.\n\n\n  library(Rmpi)\n  library(snow)\n\n  # Initiate the cluster\n  cluster <- makeMPIcluster(length(readLines(Sys.getenv(\"OAR_NODE_FILE\"))))\n\n  # Function that prints the hostname of the caller, just for fun\n  sayhello <- function()\n  {\n      info <- Sys.info()[c(\"nodename\", \"machine\")]\n      paste(\"Hello from\", info[1])\n  }\n\n  # Call the 'sayhello' function on each node of the cluster\n  names <- clusterCall(cluster, sayhello)\n  print(unlist(names))\n\n\n  # Compute the number of flights for a given destination. \n  # Same as previous examples but with MPI communications.\n  load(\"~jemeras/data/air.rda\")\n  dests = as.character(unique(air$DEST))\n  count_flights = function(x){length(which(air$DEST == x))}\n  #as.data.frame(cbind(dest=dests, nb=lapply(dests, count_flights)))\n  clusterExport(cluster, \"air\")\n\n  print(as.data.frame(cbind(dest=dests, nb=parLapply(cluster, dests, count_flights))))\n\n  # Terminate the cluster\n  stopCluster(cluster)\n\n\n\nThen, still outside of R and on your job head node run:\n\n\n  mpirun -np 1 -machinefile $OAR_NODE_FILE Rscript parallelAirDests.R\n\n\n\nYou may find strange the \n-np 1\n, in fact this is because it is \nsnow\n that manages the processes spawning.\n\n\n\n\n\n\n\n\nUseful links\n\n\n\n\n\n\nCRAN Archive\n\n\n\n\n\n\nCRAN HPC Packages\n\n\n\n\n\n\nggplot2 Documentation\n\n\n\n\n\n\nAdvanced R programming by Hadley Wickham",
            "title": "R - statistical computing"
        },
        {
            "location": "/advanced/R/#introduction-to-r",
            "text": "Copyright (c) 2014 Joseph Emeras  joseph.emeras@uni.lu  Copyright (c) 2017 Aur\u00e9lien Ginolhac  aurelien.ginolhac@uni.lu",
            "title": "Introduction to R"
        },
        {
            "location": "/advanced/R/#r-tutorial",
            "text": "Through this tutorial you will learn how to use R from your local machine or from one of the  UL HPC platform  clusters. We will also use the  ggplot  library to generate nice graphics and export them as pdf files. Then, we will see how to organize and group data. Finally we will illustrate how R can benefit from multicore and cluster parallelization.  Warning: this tutorial does not focus on the learning of R language but aims at showing you nice startup tips.\nIf you're also looking for a good tutorial on R's data structures you can take a look at:  Hadley Wickham's page . Another  bookdown 's book is available for free:  R for Data Science  by Garrett Grolemund & Hadley Wickham  Conventions used in this tutorial:   commands that have to be typed on the cluster start with a prompt like this:  jdoe@access:~$  commands that have to be typed on your local machine start with a prompt like this:  jdoe@localhost:~$  code blocks containing one or several  >  should not be pasted \"as it\", they are meant for you to observe the output of each function; others can be pasted in R terminal \"as it\".",
            "title": "R Tutorial"
        },
        {
            "location": "/advanced/R/#lecture-introduction-to-r",
            "text": "see slides:   here as html  here as pdf",
            "title": "lecture: introduction to R"
        },
        {
            "location": "/advanced/R/#pre-requisites",
            "text": "",
            "title": "Pre-requisites"
        },
        {
            "location": "/advanced/R/#optional-on-your-local-machine",
            "text": "First of all, let's install R. You will find releases for various distributions available at  CRAN Archive . Once installed, to use R interactive session interface, simply open a terminal and type:  jdoe@localhost:~$ R  You will also find handy to use the  R-Studio  graphical IDE. R-Studio embeds a R shell where you can call R functions as in the interactive session interface. Thus you can use whether R interactive shell or R-Studio embedded shell.",
            "title": "Optional: On your local machine"
        },
        {
            "location": "/advanced/R/#on-the-cluster",
            "text": "R is already available in  chaos ,  gaia  and  iris  clusters as a module. Only  iris  has the latest version  3.4.0 . The first step is the reservation of a resource. Connect to your favorite cluster frontend (here:  iris ). We assume you have already configured your  .ssh/config .",
            "title": "On the cluster"
        },
        {
            "location": "/advanced/R/#iris",
            "text": "jdoe@localhost:~$ ssh iris-cluster  Once connected to the user frontend, book 1 core for half an hour (as we will use R in single-threaded mode, we will need only one core).  [jdoe@access2 ~]$ srun -p interactive --qos qos-interactive --time=0:30:0 --pty bash  When the job is running and you are connected load  R  module. For a complete list of availbale modules see:  Software page .  [jdoe@access2 ~]$ module load lang/R  Now you should be able to invoke R and see something like this:  [jdoe@iris-081 ~]$  R\n\nR version 3.4.0 (2017-04-21) -- \"You Stupid Darkness\"\nCopyright (C) 2017 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n>  sessionInfo()  function gives information about R version, loaded libraries etc.  > sessionInfo()\nR version 3.4.0 (2017-04-21)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: CentOS Linux 7 (Core)\n\nMatrix products: default\nBLAS: /mnt/irisgpfs/apps/resif/data/production/v0.1-20170602/default/software/lang/R/3.4.0-intel-2017a-X11-20170314-bare/lib64/R/lib/libR.so\nLAPACK: /mnt/irisgpfs/apps/resif/data/production/v0.1-20170602/default/software/lang/R/3.4.0-intel-2017a-X11-20170314-bare/lib64/R/modules/lapack.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base\n\nloaded via a namespace (and not attached):\n[1] compiler_3.4.0\n>",
            "title": "iris"
        },
        {
            "location": "/advanced/R/#gaia",
            "text": "jdoe@localhost:~$ ssh gaia-cluster  Once connected to the user frontend, book 1 core for half an hour (as we will use R in single-threaded mode, we will need only one core).  jdoe@access(gaia-cluster) ~ $ oarsub -I -l core=1,walltime=0:30  When the job is running and you are connected load  R  module. For a complete list of availbale modules see:  Software page .  jdoe@access2:~$ module load lang/R  Now you should be able to invoke R and see something like this:  jdoe@gaia-59(gaia-cluster)[OAR4159732->29] ~ $ R\n\nR version 3.3.0 (2016-05-03) -- \"Supposedly Educational\"\nCopyright (C) 2016 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n>  sessionInfo()  function gives information about R version, loaded libraries etc.  > sessionInfo()\nR version 3.3.0 (2016-05-03)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Debian GNU/Linux 7 (wheezy)\n\nlocale:\n [1] LC_CTYPE=en_US.ISO-8859-15       LC_NUMERIC=C                    \n [3] LC_TIME=en_US.ISO-8859-15        LC_COLLATE=en_US.ISO-8859-15    \n [5] LC_MONETARY=en_US.ISO-8859-15    LC_MESSAGES=en_US.ISO-8859-15   \n [7] LC_PAPER=en_US.ISO-8859-15       LC_NAME=C                       \n [9] LC_ADDRESS=C                     LC_TELEPHONE=C                  \n[11] LC_MEASUREMENT=en_US.ISO-8859-15 LC_IDENTIFICATION=C\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base\n>",
            "title": "gaia"
        },
        {
            "location": "/advanced/R/#installing-r-packages",
            "text": "To install libraries you can use the  install.packages()  function.  e.g  install.packages(\"ggplot2\")  This will install the  ggplot2  library.  Note: on the first run, R might ask you various questions during the installation. e.g. selecting a CRAN mirror to use for downloading packages. Select a mirror close to your location. For other questions, using default values is ok.  Now, to load this library call:  library(ggplot2)  A call to  sessionInfo()  function will return  ggplot2  version as it is now attached to the current session.",
            "title": "Installing R Packages"
        },
        {
            "location": "/advanced/R/#warm-up-session-simple-plotting",
            "text": "",
            "title": "Warm-up Session -- Simple Plotting"
        },
        {
            "location": "/advanced/R/#from-single-dataset",
            "text": "Movies dataset, derived from data provided by  IMDB  is a sample dataset available as a pacakge in  ggplot2movies  for testing purpose. Its data description can be found  here . Thus, when loading  ggplot2movies  library, this dataset is available under the name:  movies .  (OPTIONAL) An other way to get the dataset would be to download, extract and read it by using the  readr  library (part of the  tidyverse ) that can uncompress by itself:  library(readr)\nmovies <- read_csv(\"https://github.com/hadley/ggplot2movies/raw/master/data-raw/movies.csv\")  ## Parsed with column specification:\n## cols(\n##   .default = col_double(),\n##   title = col_character(),\n##   year = col_integer(),\n##   length = col_integer(),\n##   budget = col_integer(),\n##   votes = col_integer(),\n##   mpaa = col_character(),\n##   Action = col_integer(),\n##   Animation = col_integer(),\n##   Comedy = col_integer(),\n##   Drama = col_integer(),\n##   Documentary = col_integer(),\n##   Romance = col_integer(),\n##   Short = col_integer()\n## )\n\n## See spec(...) for full column specifications.  Now let's take a (reproducible) sample of 1000 movies and plot their distribution regarding their rating.  library(ggplot2)            # load ggplot2 library to use packages functions\nset.seed(5689)              # set the seed for random selection used in `sample()` function\n# movies is the data.frame name, from this data.frame, randomly select 1000 rows\nmovies_sample <- movies[sample(nrow(movies), 1000), ]   \n# construct the graph -- movies_sample will be used as data, we will plot an histogram where x=movies_sample$rating and with a bin size=0.5\ngraph <- ggplot(movies_sample) + geom_histogram(aes(x = rating), binwidth = 0.5)        \nggsave(graph, file = \"movies_hist.pdf\", width = 8, height = 4)                              # save the graph in a pdf file  Now you retrieve the generated pdf on your local workstation for visualization:  jdoe@localhost:~$ scp iris-cluster:movies_hist.pdf .  ggplot2  proposes many functions to plot data according to your needs. Do not hesitate to wander in the  ggplot2 documentation  and to read at provided examples to better understand how to use it. The  ggsave()  function is convenient to export ggplot graphics as  .pdf ,  .jpg  or  .png  files  you should see:",
            "title": "From Single Dataset"
        },
        {
            "location": "/advanced/R/#from-several-datasets",
            "text": "Now, let's say we have two different datasets:  diamonds_fair  and  diamonds_good  that are both extracts from the  diamonds  dataset (provided in the  ggplot2  dataset). In this example we will consider that these two datasets come from different sources, so do not try to understand the next lines, they are just here to setup the example (simply copy-paste these in your R prompt).  set.seed(2109)  \ndiamonds_fair <- data.frame(carat = diamonds$carat[which(diamonds$cut == 'Fair')],\n                            price = diamonds$price[which(diamonds$cut == 'Fair')])\ndiamonds_fair <- diamonds_fair[sample(nrow(diamonds_fair), 20), ]\ndiamonds_good <- data.frame(carat = diamonds$carat[which(diamonds$cut == 'Good')],\n                            price = diamonds$price[which(diamonds$cut == 'Good')])\ndiamonds_good <- diamonds_good[sample(nrow(diamonds_good), 20), ]  To know the class of an R object you can use the  class()  function  class(diamonds_fair)  ## [1] \"data.frame\"  So we have these two datasets, being of class dataframe. In R, a  data.frame  is one kind of data structure whose columns have names and that can contain several rows. Basically it looks like a matrix with columns identified by an index  and  a name, and with rows identified by an index. Let's check how they are organized with the  names()  function that gives a dataset column names.  names(diamonds_fair)\n## [1] \"carat\" \"price\"\nnames(diamonds_good)\n## [1] \"carat\" \"price\"  Thus for each dataset row we have the price and the carat value for a given diamond. We want to add a column to datasets that will describe from which one it comes from, then we will merge these into one single dataset.  diamonds_fair <- cbind(diamonds_fair, cut_class = \"Fair\")                               # add a column named cut_class with all values being \"Fair\" to data.frame diamonds_fair\ndiamonds_good <- cbind(diamonds_good, cut_class = \"Good\")                               # same with \"Good\"\ndiamonds_merge <- rbind(diamonds_fair, diamonds_good)                                   # combine the 2 data.frame with `rbind()` as they both have the same structure  cbind()  function is used to add a column to a dataframe,  rbind()  to combine rows of two dataframes (c is for column, r is for row). Now we have all data merged in a dataframe and a column that describes the origin of data (the column  cut_class ), let's plot data.  Note: To visualize an extract of your data you can do:  diamonds_merge[1:10,]       # returns rows 1 to 10  ##      carat price cut_class\n## 297   1.01  3801      Fair\n## 1104  0.46  1035      Fair\n## 623   1.01  5538      Fair\n## 35    0.91  2854      Fair\n## 290   1.51  3765      Fair\n## 1560  0.96  2540      Fair\n## 590   1.01  5226      Fair\n## 192   0.90  3332      Fair\n## 563   1.00  5027      Fair\n## 1213  0.89  1334      Fair  diamonds_merge[, 3]             # returns column no.3  ##  [1] Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair\n## [15] Fair Fair Fair Fair Fair Fair Good Good Good Good Good Good Good Good\n## [29] Good Good Good Good Good Good Good Good Good Good Good Good\n## Levels: Fair Good  diamonds_merge$cut_class        # returns column named cut_class  ##  [1] Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair Fair\n## [15] Fair Fair Fair Fair Fair Fair Good Good Good Good Good Good Good Good\n## [29] Good Good Good Good Good Good Good Good Good Good Good Good\n## Levels: Fair Good  Then we construct and save the graph.  this time we use ggplot's function  geom_point()  to plot data points. colour=cut_class aestetics option will plot the points according to cut_class values  graph <- ggplot(data = diamonds_merge) + \n  geom_point(aes(x = carat, y = price, colour = cut_class))\nggsave(graph, file = \"diamonds_plot.pdf\", width=8, height=4)  Remember, to get help about a particular function you can type  ?function_name . e.g.  ?cbind  To get package and meta information on a function you can type  ??function_name . e.g.  ??ggsave",
            "title": "From Several Datasets"
        },
        {
            "location": "/advanced/R/#organizing-your-data",
            "text": "Let's say we are working with the full  diamonds  dataset and we want to have the  average  price for a given diamond  cut .  names(diamonds)  ##  [1] \"carat\"   \"cut\"     \"color\"   \"clarity\" \"depth\"   \"table\"   \"price\"  \n##  [8] \"x\"       \"y\"       \"z\"  We could do a for loop to aggregate the data per cuts and manually compute the average price, but in R loops are generally a bad idea. For large datasets it is very long to compute. Thus instead of looping around the dataset, we will use a function from the  dplyr  package part of the  tidyverse  idiom",
            "title": "Organizing your Data"
        },
        {
            "location": "/advanced/R/#dplyr-from-the-tidyverse",
            "text": "You will first need to install and load  dplyr .  install.packages(\"dplyr\")  Of note, all core packages of the  tidyverse  could be installed at once  install.packages(\"tidyverse\")  First of all, we chain the commands using the  pipe   %>% . That avoids many parenthesis and keep the natural reading from left to right.  The first parameter will be the dataset, the second will be the column of the dataset we want to group the results on, third parameter will be the call to the  summarise()  function that will enable to aggregate data on the  cut  column.  library(dplyr)\ndiamonds %>%\n  group_by(cut) %>%\n  summarise(avg_price = mean(price))  ## # A tibble: 5 x 2\n##         cut avg_price\n##       <ord>     <dbl>\n## 1      Fair  4358.758\n## 2      Good  3928.864\n## 3 Very Good  3981.760\n## 4   Premium  4584.258\n## 5     Ideal  3457.542  Note:  summarise()  from the  dplyr  package is similar to  aggregate()  from base package, you can use indifferently one or the other,  dplyr  functions simply provide a more consistent naming convention together with better performance",
            "title": "dplyr from the tidyverse"
        },
        {
            "location": "/advanced/R/#aggregate-from-base",
            "text": "aggregate(price ~ cut,\n          FUN = mean,\n          data = diamonds)  ##         cut    price\n## 1      Fair 4358.758\n## 2      Good 3928.864\n## 3 Very Good 3981.760\n## 4   Premium 4584.258\n## 5     Ideal 3457.542",
            "title": "aggregate from base"
        },
        {
            "location": "/advanced/R/#lapply-from-base",
            "text": "In the previous example we used  aggregate  for the  aggregation , we could also have used  lapply  (but in a slightlier more complicated way):  as.data.frame(cbind(cut = as.character(unique(diamonds$cut)), avg_price = lapply(unique(diamonds$cut), function(x) mean(diamonds$price[which(diamonds$cut == x)]))))  ##         cut avg_price\n## 1     Ideal  3457.542\n## 2   Premium  4584.258\n## 3      Good  3928.864\n## 4 Very Good   3981.76\n## 5      Fair  4358.758",
            "title": "lapply from base"
        },
        {
            "location": "/advanced/R/#datatable",
            "text": "data.table  is a package without dependencies that is super fast. Although the syntax is harder to learn compare to  dplyr . See this long thread  at stackoverflow  for more details.  # install.package(\"data.table\")\nlibrary(data.table)  ## \n## Attaching package: 'data.table'\n\n## The following objects are masked from 'package:dplyr':\n## \n##     between, first, last\n\n## The following object is masked from 'package:purrr':\n## \n##     transpose  DT <- data.table(diamonds)\nDT[, mean(price), by = cut]  ##          cut       V1\n## 1:     Ideal 3457.542\n## 2:   Premium 4584.258\n## 3:      Good 3928.864\n## 4: Very Good 3981.760\n## 5:      Fair 4358.758",
            "title": "data.table"
        },
        {
            "location": "/advanced/R/#plyr",
            "text": "For completeness, we could add  ddply  from  plyr  the first attempt of Hadley Wickham to make data manipulation easier.  library(plyr)  ## -------------------------------------------------------------------------\n\n## You have loaded plyr after dplyr - this is likely to cause problems.\n## If you need functions from both plyr and dplyr, please load plyr first, then dplyr:\n## library(plyr); library(dplyr)\n\n## -------------------------------------------------------------------------\n\n## \n## Attaching package: 'plyr'\n\n## The following objects are masked from 'package:dplyr':\n## \n##     arrange, count, desc, failwith, id, mutate, rename, summarise,\n##     summarize\n\n## The following object is masked from 'package:purrr':\n## \n##     compact  ddply(diamonds, .(cut), summarize, avg_price = mean(price))  ##         cut avg_price\n## 1      Fair  4358.758\n## 2      Good  3928.864\n## 3 Very Good  3981.760\n## 4   Premium  4584.258\n## 5     Ideal  3457.542",
            "title": "plyr"
        },
        {
            "location": "/advanced/R/#perfomance-considerations",
            "text": "So, we want to know which one of the two versions is the most efficient, for that purpose, the library  microbenchmark  is handy.  install.packages(\"microbenchmark\")  We can use the  microbenchmark()  function on several expressions, with a given repetition number to compare them:  library(dplyr) # dplyr needs to be loaded after plyr when both are used\nlibrary(microbenchmark)\nm <- microbenchmark(LAPPLY    = as.data.frame(cbind(cut = as.character(unique(diamonds$cut)), avg_price = lapply(unique(diamonds$cut), function(x) mean(diamonds$price[which(diamonds$cut == x)])))),\n                   AGGREGATE = aggregate(price ~ cut, FUN = mean, data = diamonds),\n                   DDPLY     = ddply(diamonds, .(cut), summarize, avg_price = mean(price)),\n                   DPLYR     = diamonds %>% group_by(cut) %>% summarise(avg_price = mean(price)),\n                   DATATABLE = DT[, .(mean(price)), by = cut],\n                   times     = 1000)\nm  ## Unit: microseconds\n##       expr       min        lq      mean    median        uq       max\n##     LAPPLY  9728.227 11430.066 14327.419 14451.795 15378.028  94718.08\n##  AGGREGATE 34857.006 39880.578 43258.622 41155.558 42766.251 127360.33\n##      DDPLY  9868.561 11775.282 15316.425 14737.079 15663.672  93199.60\n##      DPLYR  2724.228  2964.753  3223.255  3064.992  3199.719  29266.28\n##  DATATABLE   946.433  1204.498  1460.516  1303.794  1398.279   6723.17\n##  neval\n##   1000\n##   1000\n##   1000\n##   1000\n##   1000",
            "title": "Perfomance Considerations"
        },
        {
            "location": "/advanced/R/#plotting-the-benchmark",
            "text": "result gives us a boxplot graph if you use the base function  plot  plot(m)   if you want to save as a  png  file  png(\"benchmark_boxplot.png\")\n## plot the graph\nplot(m)\n## flush the output device to save the graph\ndev.off()                     Of note, you can use  ggplot2  via the  autoplot  function  autoplot(m)",
            "title": "Plotting the benchmark"
        },
        {
            "location": "/advanced/R/#using-the-datatable-package",
            "text": "According to the  data.table documentation   data.table  inherits from  data.frame  to offer fast subset, fast grouping, fast update, fast ordered joins and list columns in a short and flexible syntax, for faster development.  It uses binary search instead of vector scan to perform its operations and thus is scalable. We can convert easily a  data.frame  to a  data.table .  load the  data.table  package then convert the  data.frame  to a  data.table :  MOVIES <- data.table(movies)  As  data.table  uses binary search, we have to define manually the keys that will be used for this search, this is done with  setkey()  function.  Let's now create a new  data.frame . We will make it large enough (10 million rows, 676 groups) to demonstrate the difference between a vector scan and a binary search.  grpsize <- ceiling(1e7 / 26^2) \nsystem.time(DF <- data.frame(x = rep(LETTERS, each = 26 * grpsize),\n                             y = rep(letters, each = grpsize),\n                             v = runif(grpsize * 26^2),\n                             stringsAsFactors = FALSE)\n)     ##    user  system elapsed \n##   0.910   0.067   0.982  This generated a data.frame named DF with 3 columns.   Column  x  is a repetition of uppercase letters from A to Z  column  y  is minorcase letters  Column  v  is a random uniform value.   To illustrate the difference, we take as example the selection in this large dataset of rows where  x == \"R\"  and  y == \"h\" .",
            "title": "Using the data.table package"
        },
        {
            "location": "/advanced/R/#vector-scan",
            "text": "we select rows where x=\"R\" and y=\"h\". For this we have to scan the full data.frame twice.  system.time(ans1 <- DF[DF$x==\"R\" & DF$y==\"h\",])           ##    user  system elapsed \n##   0.316   0.074   0.411",
            "title": "vector scan"
        },
        {
            "location": "/advanced/R/#binary-search",
            "text": "We select rows that match the join between DT and the data.table row: data.table(\"R\",\"h\"). This will return the same result as before but much faster.  DT <- data.table(DF)                                    # convert the data.frame to a data.table\nsetkey(DT, x, y)                                          # set column x and y as data.table keys.\nsystem.time(ans2 <- DT[J(\"R\", \"h\")])                      ##    user  system elapsed \n##   0.002   0.001   0.003  In the first case, we scan the full table twice (once for selecting x's that are equal to \"R\", then y's that are equal to \"h\"), then do the selection. In the second case, we are joining DT to the 1 row, 2 column table returned by data.table(\"R\",\"h\"). We use the alias for joining data.tables called J(), short for join. As we defined x and y as keys, this works like a database join. You can see that vector scan is very long compared to binary search.",
            "title": "binary search"
        },
        {
            "location": "/advanced/R/#grouping",
            "text": "data.table  also provides faster operations for reading files and grouping data.  Now you can compare the same aggregation operation with  data.frame  and  data.table . In both examples we aggregate on x and apply the function  sum()  to corresponding v.  data.frame  and  base  style:  system.time(tapply(DT$v, DT$x, sum))  ##    user  system elapsed \n##   0.500   0.164   0.686  data.table  style, using  by :  system.time(DT[, sum(v), by = x])  ##    user  system elapsed \n##   0.150   0.014   0.177  Question: use  dplyr  instead of  tapply()  in the first example.  Question: return the min and max instead of the sum using data.table",
            "title": "Grouping"
        },
        {
            "location": "/advanced/R/#parallel-r",
            "text": "The first part of the tutorial is now over, you can connect to  gaia  cluster and submit an other job requesting several machines.    jdoe@localhost:~$ ssh gaia-cluster\njdoe@access(gaia-cluster) ~$ oarsub -I -l nodes=1,walltime=1   When the job is running and you are connected load R module (version compiled with Intel Compiler), then run R.  jdoe@access:~$ module load lang/R\n  jdoe@access:~$ R  We will use a large dataset (400K+ rows) to illustrate the effect of parallelization in R (as dataset is large, the following line may take time to complete depending on your network speed).  > air <- read.csv(url(\"http://packages.revolutionanalytics.com/datasets/AirOnTimeCSV2012/airOT201201.csv\"))  NOTE : If downloading the air dataset (above line) takes too much time you can load it from a file on the cluster:  > load(\"~jemeras/data/air.rda\")  (For information you can save your  air  object just as showed above with:  save(air,file= \"~/data/air.rda\") )  If we want to have the number of flights for each destination  DEST  we can do the following:  dests <- as.character(unique(air$DEST))\n  count_flights <- function(x) length(which(air$DEST == x))\n  as.data.frame(cbind(dest = dests, nb = lapply(dests, count_flights)))  As the dataframe is large it takes some time to compute  > microbenchmark(LAPPLY = lapply(dests, count_flights), times = 10)\nUnit: seconds\n   expr      min       lq   median       uq      max neval\n LAPPLY 1.607961 1.609036 1.609638 1.610269 2.023961    10",
            "title": "Parallel R"
        },
        {
            "location": "/advanced/R/#single-machine-parallelization",
            "text": "To parallelize the lapply function we can use  mclapply()  from  parallel  package and give it the number of cores to use.  mclapply()  uses the underlying operating system fork() functionality to achieve parallelization. Using several cores makes the process shorter.  library(parallel)\nas.data.frame(cbind(dest = dests, nb = mclapply(dests, count_flights, mc.cores = 12)))\n\nlibrary(microbenchmark)\nmicrobenchmark(MCLAPPLY = mclapply(dests, count_flights, mc.cores = 12), times = 20)  # or use `detectCores()` from `parallel` package instead of giving cores value.    Unit: milliseconds\n       expr      min       lq   median       uq     max neval\n   MCLAPPLY 233.8035 235.1089 235.9138 236.6393 263.934    10  It is nice to visualize all your cores working on your node with  htop  for example. You can connect to the same node from another terminal by typing:    jdoe@access:~$ oarstat -u\n  Job id     Name           User           Submission Date     S Queue\n  ---------- -------------- -------------- ------------------- - ----------\n  6664321                   jdoe           2015-06-03 14:24:23 R default  Note the  Job id  field. Put this job id in the next command:    jdoe@access:~$ oarsub -C 6664321  Then  htop  will show you your cores working if you call again the  mclapply()  function.   Finally you can save the  air  R object to reuse it in an other R session.  > save(air, file = \"./air.rda\")  Then quit your current R session but  do not  end your current  oar  job.",
            "title": "Single Machine Parallelization"
        },
        {
            "location": "/advanced/R/#dplyr-version",
            "text": "library(dplyr)\ncount(air, DEST)\nmicrobenchmark(DPLYR = count(air, DEST), times = 20)  Unit: milliseconds\n  expr      min       lq     mean   median       uq     max neval\n DPLYR 25.66517 25.86669 26.40936 26.11667 26.83215 28.1973    20  Optimised code may be better than parallelisation. Of note, parallel  dplyr  exists:  multidplyr .",
            "title": "dplyr version"
        },
        {
            "location": "/advanced/R/#cluster-parallelization",
            "text": "The  parLapply()  function will create a cluster of processes, which could even reside on different machines on the network, and they communicate via TCP/IP or MPI in order to pass the tasks and results between each other. Thus you have to load necessary packages and export necessary data and functions to the global environment of the cluster workers.  First, add the module loading at bash login too for enabling it on the nodes. To do so, within a shell type:  echo \"module load lang/R\" >> ~/.bash_login      # /!\\ TO REMOVE AT THE END OF PS /!\\\n  module purge\n  module load lang/R  Warning: do not forget to clean your ~/.bash_login file after the PS (remove the 'module load lang/R/3.2.0-ictce-7.3.5-bare' line).",
            "title": "Cluster Parallelization"
        },
        {
            "location": "/advanced/R/#socket-communications",
            "text": "First, let's load data and initialize variables.  library(parallel)\n\n  # air = read.csv(url(\"http://packages.revolutionanalytics.com/datasets/AirOnTimeCSV2012/airOT201201.csv\"))  # load again the air data.table from http\nload(\"./air.rda\")   # read saved R object from file \"air.rda\"\n  dests = as.character(unique(air$DEST))\n  count_flights = function(x){length(which(air$DEST == x))}\n\n  ## set cluster characteristics -- get OAR nodes to use, type of communication\n  nodes = scan(Sys.getenv(\"OAR_NODE_FILE\"), what=character(0))\n  oar_job_id = as.numeric(Sys.getenv(\"OAR_JOB_ID\"))\n  connector = paste0(\"OAR_JOB_ID=\", oar_job_id)\n  connector = paste0(connector, \" ~jemeras/bin/roarsh\")\n  comm_type = \"PSOCK\"  Then, setup the cluster.    ## set up the cluster\n  cl = makeCluster(nodes, type = comm_type, rshcmd = connector) \n  ## If a particular library <LIB> is needed, load it on the nodes with\n  # clusterEvalQ(cl, { library(<LIB>) })\n  ## or give the full environment with\n  # clusterEvalQ(cl, sessionInfo())\n  ## export air dataset on all the nodes\n  clusterExport(cl, varlist=c(\"air\"))  Do the parallel computation.    ## compute in parallel through sockets\n  as.data.frame(cbind(dest=dests, nb=parLapply(cl, dests, count_flights)))  Finalize and cleanup things.    stopCluster(cl)  Exercise: Plot a speedup graph with different number of cores and/or machines used.",
            "title": "Socket Communications"
        },
        {
            "location": "/advanced/R/#mpi-communications",
            "text": "In this section we will use the same example as previously but with MPI connections. For that purpose we need to install two libraries:  rmpi  and  snow .  As we are using R compiled with Intel compiler we will have to specify manually some paths and which version of MPI we are using when installing  rmpi .  > install.packages(\"Rmpi\",\n                   configure.args =\n                   c(paste(\"--with-Rmpi-include=\",Sys.getenv(\"EBROOTIMPI\"),\"/include64\",sep=\"\"),\n                     paste(\"--with-Rmpi-libpath=\",Sys.getenv(\"EBROOTIMPI\"),\"/lib64\",sep=\"\"),\n                     paste(\"--with-mpi=\",Sys.getenv(\"EBROOTIMPI\"),sep=\"\"),\n                     \"--with-Rmpi-type=OPENMPI\"))  NOTE : if the installation fails you can try using the module with  Rmpi  already installed:    module purge\n  module load lang/R/3.2.0-ictce-7.3.5-Rmpi  Then, outside of R shell write a file named  parallelAirDests.R  with the following code.  Warning : when using parallelization in R, please keep in mind that communication is much slower than computation. Thus if your problem is involving a long computation then you are right to use it, otherwise, if your problem is a large quantity of data you must use  data.table  or  dplyr .    library(Rmpi)\n  library(snow)\n\n  # Initiate the cluster\n  cluster <- makeMPIcluster(length(readLines(Sys.getenv(\"OAR_NODE_FILE\"))))\n\n  # Function that prints the hostname of the caller, just for fun\n  sayhello <- function()\n  {\n      info <- Sys.info()[c(\"nodename\", \"machine\")]\n      paste(\"Hello from\", info[1])\n  }\n\n  # Call the 'sayhello' function on each node of the cluster\n  names <- clusterCall(cluster, sayhello)\n  print(unlist(names))\n\n\n  # Compute the number of flights for a given destination. \n  # Same as previous examples but with MPI communications.\n  load(\"~jemeras/data/air.rda\")\n  dests = as.character(unique(air$DEST))\n  count_flights = function(x){length(which(air$DEST == x))}\n  #as.data.frame(cbind(dest=dests, nb=lapply(dests, count_flights)))\n  clusterExport(cluster, \"air\")\n\n  print(as.data.frame(cbind(dest=dests, nb=parLapply(cluster, dests, count_flights))))\n\n  # Terminate the cluster\n  stopCluster(cluster)  Then, still outside of R and on your job head node run:    mpirun -np 1 -machinefile $OAR_NODE_FILE Rscript parallelAirDests.R  You may find strange the  -np 1 , in fact this is because it is  snow  that manages the processes spawning.",
            "title": "MPI Communications"
        },
        {
            "location": "/advanced/R/#useful-links",
            "text": "CRAN Archive    CRAN HPC Packages    ggplot2 Documentation    Advanced R programming by Hadley Wickham",
            "title": "Useful links"
        },
        {
            "location": "/advanced/Bioinformatics/",
            "text": "-\n- mode: markdown; mode: visual-line; fill-column: 80 -\n-\n\n\nCopyright (c) 2014-2017 UL HPC Team  \nhpc-sysadmins@uni.lu\n\n\n\n\nUL HPC Tutorial: Bioinformatics software on the UL HPC platform\n\n\n \n \n \n \n \n \n\n\n\n\nThe objective of this tutorial is to exemplify the execution of several Bioinformatics packages on top of the \nUL HPC\n platform.\n\n\nThe targeted applications are:\n\n\n\n\nABySS\n\n\nGromacs\n\n\nBowtie2\n / \nTopHat\n\n\nmpiBLAST\n\n\n\n\nThe tutorial will:\n\n\n\n\nshow you how to load and run pre-configured versions of these applications on the clusters\n\n\nshow you how to download and use updated versions of Bowtie2/TopHat\n\n\ndiscuss the parallelization capabilities of these applications\n\n\n\n\nPrerequisites\n\n\nWhen you look at the \nsoftware page\n you will notice that some of the applications are part of the \nlcsb\n (gaia) or \nbioinfo\n (iris) software set. The modules in this set are not visible be default. To use them within a job you have to do:\n\n\n\n\n\n\nGaia\n\n\n(access-gaia)$> module use $RESIF_ROOTINSTALL/lcsb/modules/all\n\n\n\n\n\n\n\nIris\n\n\n(access-iris)$> module use /opt/apps/resif/data/stable/bioinfo/modules/all\n\n\n\n\n\n\n\nIf you want them to always be available, you can add the following line to your \n.bash_private\n (adapt with the above command for iris):\n\n\ncommand -v module >/dev/null 2>&1 && module use $RESIF_ROOTINSTALL/lcsb/modules/all\n\n\n\nThis tutorial relies on several input files for the bioinformatics packages, thus you will need to download them\nbefore following the instructions in the next sections:\n\n\n(access)$> mkdir -p ~/bioinfo-tutorial/gromacs ~/bioinfo-tutorial/tophat ~/bioinfo-tutorial/mpiblast\n(access)$> cd ~/bioinfo-tutorial\n(access)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/gromacs/pr.tpr -O gromacs/pr.tpr\n(access)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/tophat/test_data.tar.gz -O tophat/test_data.tar.gz\n(access)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/tophat/test2_path -O tophat/test2_path\n(access)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/mpiblast/test.fa -O mpiblast/test.fa\n\n\n\nOr simply clone the full tutorials repository and make a link to the Bioinformatics tutorial:\n\n\n(access)$> git clone https://github.com/ULHPC/tutorials.git\n(access)$> ln -s tutorials/advanced/Bioinformatics/ ~/bioinfo-tutorial\n\n\n\nABySS\n\n\nCharacterization: CPU intensive, data intensive, native parallelization\n\n\nDescription\n\n\nABySS\n: Assembly By Short Sequences\n\n\nABySS is a de novo, parallel, paired-end sequence assembler that is designed for short reads.\nThe single-processor version is useful for assembling genomes up to 100 Mbases in size.\nThe parallel version is implemented using MPI and is capable of assembling larger genomes \n[*]\n.\n\n\nExample\n\n\nThis example will be ran in an interactive session, with batch-mode executions\nbeing proposed later on as exercises.\n\n\n\n\n\n\nGaia\n\n\n# Connect to Gaia (Linux/OS X):\n(yourmachine)$> ssh access-gaia.uni.lu\n\n# Request 1 full node in an interactive job:\n(access-gaia)$> oarsub -I -l nodes=1,walltime=00:30:00\n\n\n\n\n\n\n\nIris\n\n\n# Connect to Iris (Linux/OS X):\n(yourmachine)$> ssh access-iris.uni.lu\n\n# Request half a node in an interactive job:\n(access-iris)$> srun -p interactive --qos qos-interactive -t 0-0:30:0 -N 1 -c 1 --ntasks-per-node=14 --pty bash\n\n# Load bioinfo software set\n(node)$> module use /opt/apps/resif/data/stable/bioinfo/modules/all\n\n\n\n\n\n\n\n# Check the ABySS versions installed on the clusters:\n(node)$> module avail 2>&1 | grep -i abyss\n\n# Load the default ABySS version:\n(node)$> module load bio/ABySS\n\n# Check that it has been loaded, along with its dependencies:\n(node)$> module list\n\n# All the ABySS binaries are now in your path (check with TAB autocompletion)\n(node)$> abyss-<TAB>\n\n\n\n\nIn the ABySS package only the \nABYSS-P\n application is parallelized using MPI and can be run on several cores (and across several nodes) using\nthe \nabyss-pe\n launcher.\n\n\n# Create a test directory and go to it\n(node)$> mkdir ~/bioinfo-tutorial/abyss\n(node)$> cd ~/bioinfo-tutorial/abyss\n\n# Set the input files' directory in the environment\n(node)$> export ABYSSINPDIR=/mnt/isilon/projects/ulhpc-tutorials/bioinformatics/abyss\n\n# Give a name to the experiment\n(node)$> export ABYSSNAME='abysstest'\n\n\n\n\n\n\n\nGaia\n\n\n# Set the number of cores to use based on OAR's host file\n(node)$> export ABYSSNPROC=$(cat $OAR_NODEFILE | wc -l)\n\n# Launch the paired end assembler:\n(node)$> abyss-pe mpirun=\"mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE\" name=${ABYSSNAME} np=${ABYSSNPROC} k=31 n=10 lib=pairedend pairedend=\"${ABYSSINPDIR}/SRR001666_1.fastq.bz2 ${ABYSSINPDIR}/SRR001666_2.fastq.bz2\" > ${ABYSSNAME}.out 2> ${ABYSSNAME}.err\n\n\n\n\n\n\n\nIris\n\n\n# Set the number of cores to use based on SLURM environment variables\n(node)$> export ABYSSNPROC=$(expr $SLURM_NNODES \\* $SLURM_NTASKS_PER_NODE \\* $SLURM_CPUS_PER_TASK)\n\n# Create a hostfile\n(node)$> srun hostname | sort -n > hostfile\n\n# Launch the paired end assembler:\n(node)$> abyss-pe mpirun=\"mpirun -x PATH -x LD_LIBRARY_PATH -hostfile hostfile\" name=${ABYSSNAME} np=${ABYSSNPROC} k=31 n=10 lib=pairedend pairedend=\"${ABYSSINPDIR}/SRR001666_1.fastq.bz2 ${ABYSSINPDIR}/SRR001666_2.fastq.bz2\" > ${ABYSSNAME}.out 2> ${ABYSSNAME}.err\n\n\n\n\n\n\n\nQuestion: Why do we use the -x VARIABLE parameters for mpirun?\n\n\nSeveral options seen on the \nabyss-pe\n command line are crucial:\n\n\n\n\nwe explicitly set the mpirun command\n\n\nwe export several environment variables to all the remote nodes, otherwise required paths (for the binaries, libraries) would not be known by the MPI processes running there\n\n\nwe do not specify \n-np $ABYSSNPROC\n in the mpirun command, as it set with \nabyss-pe\n's np parameter and internally passed on to mpirun\n\n\n\n\nThe execution should take around 12 minutes, meanwhile we can check its progress by monitoring the .out/.err output files:\n\n\n(access)$> tail -f ~/bioinfo-tutorial/abyss/abysstest.*\n# We exit the tail program with CTRL-C\n\n\n\nOn \nGaia\n, we can also connect to the job (recall oarsub -C $JOBID) from a different terminal or Screen window and see the different ABySS phases with \nhtop\n.\n\n\nBecause the \nabyss-pe\n workflow (pipeline) includes several processing steps with different applications of which only ABYSS-P is MPI-parallel,\nthe speedup obtained by using more than one node will be limited to ABYSS-P's execution. Several of the other applications that are part of the\nprocessing stages are however parallelized using OpenMP and pthreads and will thus take advantage of the cores available on the node where\n\nabyss-pe\n was started.\n\n\nThe used input dataset is a well known \nIllumina run of E. coli\n.\n\n\nProposed exercises\n\n\nSeveral exercises are proposed for ABySS:\n\n\n\n\ncreate a launcher for ABySS using the commands shown in the previous section\n\n\nlaunch jobs using 1 node: 4, 8 and 12 cores, then 2 and 4 nodes and measure the speedup obtained\n\n\nunpack the two input files and place them on a node's /dev/shm, then rerun the experiment with 4, 8 and 12 cores and measure the speedup\n\n\n\n\nGROMACS\n\n\nCharacterization: CPU intensive, little I/O\n\n\nDescription\n\n\nGROMACS\n: GROningen MAchine for Chemical Simulations\n\n\nGROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.\nIt is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers \n[*]\n.\n\n\nExample\n\n\nThis example will be ran in an interactive session, with batch-mode executions\nbeing proposed later on as exercises.\n\n\n\n\n\n\nGaia\n\n\n# Connect to Gaia (Linux/OS X):\n(yourmachine)$> ssh access-gaia.uni.lu\n\n# Request 1 full node in an interactive job:\n(access-gaia)$> oarsub -I -l nodes=1,walltime=00:30:00\n\n\n\n\n\n\n\nIris\n\n\n# Connect to Iris (Linux/OS X):\n(yourmachine)$> ssh access-iris.uni.lu\n\n# Request half a node in an interactive job:\n(access-iris)$> srun -p interactive --qos qos-interactive -t 0-0:30:0 -N 1 -c 1 --ntasks-per-node=14 --pty bash\n\n\n\n\n\n\n\n# Check the GROMACS versions installed on the clusters:\n(node)$> module avail 2>&1 | grep -i gromacs\n\n\n\n\nOn \nGaia\n, several GROMACS builds are available, we will focus only on the ones corresponding to the version 4.6.5:\n\n\n\n\nbio/GROMACS/4.6.5-goolf-1.4.10-hybrid\n\n\nbio/GROMACS/4.6.5-goolf-1.4.10-mt\n\n\n\n\nWe notice that there is a \nhybrid\n and a \nmt\n version\n\n\n\n\nthe hybrid version is OpenMP and MPI-enabled, all binaries have a '_mpi' suffix\n\n\nthe mt version is only OpenMP-enabled, as such it can take advantage of only one node's cores (however it may be faster on\nsingle-node executions than the hybrid version)\n\n\n\n\nOn \nIris\n currently only the following version of GROMACS is available:\n\n\n\n\nbio/GROMACS/2016.3-intel-2017a-hybrid\n\n\n\n\nWe will perform our tests with the hybrid version:\n\n\n\n\n\n\nGaia\n\n\n# Load the MPI-enabled Gromacs, without CUDA support:\n(node)$> module load bio/GROMACS/4.6.5-goolf-1.4.10-hybrid\n\n# Check that it has been loaded, along with its dependencies:\n(node)$> module list\n\n# Check the capabilities of the mdrun binary, note its suffix:\n(node)$> mdrun_mpi -version 2>/dev/null\n\n# Go to the test directory\n(node)$> cd ~/bioinfo-tutorial/gromacs\n\n# Set the number of OpenMP threads to 1\n(node)$> export OMP_NUM_THREADS=1\n\n# Perform a position restrained Molecular Dynamics run\n(node)$> mpirun -np 12 -hostfile $OAR_NODEFILE -x OMP_NUM_THREADS -x PATH -x LD_LIBRARY_PATH mdrun_mpi -v -s pr -e pr -o pr -c after_pr -g prlog > test.out 2>&1\n\n\n\n\n\n\n\nIris\n\n\n# Load the MPI-enabled Gromacs, without CUDA support:\n(node)$> module load bio/GROMACS\n\n# Check that it has been loaded, along with its dependencies:\n(node)$> module list\n\n# Check the capabilities of the mdrun binary, note its suffix:\n(node)$> gmx_mpi -version 2>/dev/null\n\n# Go to the test directory\n(node)$> cd ~/bioinfo-tutorial/gromacs\n\n# Set the number of OpenMP threads to 1\n(node)$> export OMP_NUM_THREADS=1\n\n# Perform a position restrained Molecular Dynamics run\n(node)$> srun -n 12 gmx_mpi mdrun -v -s pr -e pr -o pr -c after_pr -g prlog > test.out 2>&1\n\n\n\n\n\n\n\nWe notice here that we are running \nmdrun_mpi\n/\ngmx_mpi\n in parallel with mpirun on 12/14 cores, and we explicitly export the OMP_NUM_THREADS\nvariable to any remote node such that only one thread per MPI process will be created.\n\n\nQuestion: What will happen if we do not set the number of OpenMP threads to 1?\n\n\nGROMACS has many parallelization options and several parameters can be tuned to give you better performance depending on your workflow, see the references in the last section of this tutorial.\n\n\nThe used input corresponds to the \nRibonuclease S-peptide\n example,\nwhich has been changed to perform 50k steps in the Molecular Dynamics run with position restraints on the peptide.\n\n\nProposed exercises\n\n\nSeveral exercises are proposed for GROMACS:\n\n\n\n\ncreate a launcher for GROMACS using the commands shown in the previous section\n\n\nlaunch jobs using 1 node: 1, 2, 4, 8, 10 and 12 cores and measure the speedup obtained\n\n\ncheck what happens when executing mdrun with 16 and 24 cores\n\n\nlaunch a job using one full node that has GPU cards and run the GPU-enabled GROMACS to see if a speedup is obtained\n\n\n\n\nBowtie2/TopHat\n\n\nCharacterization: data intensive, RAM intensive\n\n\nDescription\n\n\nBowtie2\n: Fast and sensitive read alignment\n\n\nBowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes \n[*]\n.\n\n\nTopHat\n : A spliced read mapper for RNA-Seq\n\n\nTopHat is a program that aligns RNA-Seq reads to a genome in order to identify exon-exon splice junctions. It is built on the ultrafast short read mapping program Bowtie \n[*]\n.\n\n\nExample\n\n\nThis example will show you how to use the latest version of TopHat in conjunction with the latest Bowtie2, by using the\nversions prebuilt for Linux by the developers.\n\n\n\n\n\n\nGaia\n\n\n# Connect to Gaia (Linux/OS X):\n(yourmachine)$> ssh access-gaia.uni.lu\n\n# Request 1 full node in an interactive job:\n(gaia-frontend)$> oarsub -I -l nodes=1,walltime=00:30:00\n\n\n\n\n\n\n\nIris\n\n\n# Connect to Iris (Linux/OS X):\n(yourmachine)$> ssh access-iris.uni.lu\n\n# Request half a node in an interactive job:\n(access-iris)$> srun -p interactive --qos qos-interactive -t 0-0:30:0 -N 1 -c 14 --ntasks-per-node=1 --pty bash\n\n\n\n\n\n\n\n# Create a folder for the new software and go to it\n(node)$> mkdir ~/bioinfo-tutorial/newsoft\n(node)$> cd ~/bioinfo-tutorial/newsoft\n\n# Download latest Bowtie2 and Tophat, plus the SAM tools dependency:\n(node)$> wget https://downloads.sourceforge.net/project/bowtie-bio/bowtie2/2.3.2/bowtie2-2.3.2-linux-x86_64.zip\n(node)$> wget --no-check-certificate https://ccb.jhu.edu/software/tophat/downloads/tophat-2.1.1.Linux_x86_64.tar.gz\n(node)$> wget https://github.com/samtools/samtools/releases/download/1.4.1/samtools-1.4.1.tar.bz2\n\n# Unpack the three archives\n(node)$> unzip bowtie2-2.3.2-linux-x86_64.zip\n(node)$> tar xzvf tophat-2.1.1.Linux_x86_64.tar.gz\n(node)$> tar xjvf samtools-1.4.1.tar.bz2\n\n\n\n\nSAM tools requires compilation:\n\n\n\n\n\n\nGaia\n\n\n(node)$> cd samtools-1.4.1 && make && cd ..\n\n\n\n\n\n\n\nIris\n\n\n(node)$> module load devel/ncurses/6.0-intel-2017a\n(node)$> module load tools/bzip2/1.0.6-intel-2017a\n(node)$> cd samtools-1.4.1 && make && cd ..\n(node)$> module load lib/tbb\n\n\n\n\n\n\n\n# Create a file containing the paths to the binaries, to be sourced when needed\n(node)$> echo \"export PATH=$HOME/bioinfo-tutorial/newsoft/bowtie2-2.3.2:\\$PATH\" > newsoft\n(node)$> echo \"export PATH=$HOME/bioinfo-tutorial/newsoft/tophat-2.1.1.Linux_x86_64:\\$PATH\" >> newsoft\n(node)$> echo \"export PATH=$HOME/bioinfo-tutorial/newsoft/samtools-1.4.1:\\$PATH\" >> newsoft\n(node)$> source newsoft\n\n# You can now check that both main applications can be run:\n(node)$> bowtie2 --version\n(node)$> tophat2 --version\n\n\n\n\nNow we will make a quick TopHat test, using the provided sample files:\n\n\n# Go to the test directory, unpack the sample dataset and go to it\n(node)$> cd ~/bioinfo-tutorial/tophat\n(node)$> tar xzvf test_data.tar.gz\n(node)$> cd test_data\n\n\n# Launch TopHat, with Bowtie2 in serial mode\n(node)$> tophat -r 20 test_ref reads_1.fq reads_2.fq\n\n# Launch TopHat, with Bowtie2 in parallel mode\n(node)$> tophat -p 12 -r 20 test_ref reads_1.fq reads_2.fq\n\n\n\nWe can see that for this fast execution, increasing the number of threads does not improve the calculation time due to the relatively high overhead of thread creation.\nNote that TopHat / Bowtie are not MPI applications and as such can take advantage of at most one compute node.\n\n\nNext, we will make a longer test, where it will be interesting to monitor the TopHat pipeline (with \nhtop\n for example) to see the transitions between the serial\nand parallel stages (left as an exercise).\n\n\n# Load the file which will export $TOPHATTEST2 in the environment\n(node)$> source ~/bioinfo-tutorial/tophat/test2_path\n\n# Launch TopHat, with Bowtie2 in parallel mode\n(node)$> tophat2 -p 12 -g 1 -r 200 --mate-std-dev 30 -o ./  $TOPHATTEST2/chr10.hs $TOPHATTEST2/SRR027888.SRR027890_chr10_1.fastq $TOPHATTEST2/SRR027888.SRR027890_chr10_2.fastq\n\n\n\n\nThe input data for the first test corresponds to the \nTopHat test set\n,\nwhile the second test is an example of aligning reads to the chromosome 10 of the human genome \nas given here\n.\n\n\nProposed exercises\n\n\nThe following exercises are proposed for TopHat/Bowtie2:\n\n\n\n\ncreate a launcher for TopHat using the commands shown in the previous section\n\n\nlaunch jobs with 1, 2, 4, 8 and 10 cores on one node, using the second test files, and measure the speedup obtained\n\n\n\n\nmpiBLAST\n\n\nCharacterization: data intensive, little RAM overhead, native parallelization\n\n\nDescription\n\n\nmpiBLAST\n: Open-Source Parallel BLAST\n\n\nmpiBLAST is a freely available, open-source, parallel implementation of NCBI BLAST. By efficiently utilizing distributed computational resources through database fragmentation, query segmentation, intelligent scheduling, and parallel I/O, mpiBLAST improves NCBI BLAST performance by several orders of magnitude while scaling to hundreds of processors  \n[*]\n.\n\n\nExample\n\n\nThis example will be ran in an interactive session, with batch-mode executions\nbeing proposed later on as exercises.\n\n\n\n\n\n\nGaia\n\n\n# Connect to Gaia (Linux/OS X):\n(yourmachine)$> ssh access-gaia.uni.lu\n\n# Request 1 full node in an interactive job:\n(access-gaia)$> oarsub -I -l nodes=1,walltime=00:30:00\n\n# Load the lcsb software set\n(node)$> module use $RESIF_ROOTINSTALL/lcsb/modules/all\n\n\n\n\n\n\n\nIris\n\n\n# Connect to Iris (Linux/OS X):\n(yourmachine)$> ssh access-iris.uni.lu\n\n# Request half a node in an interactive job:\n(access-iris)$> srun -p interactive --qos qos-interactive -t 0-0:30:0 -N 1 -c 1 --ntasks-per-node=14 --pty bash\n\n# Load the bioinfo software set\n(node)$> module use /opt/apps/resif/data/stable/bioinfo/modules/all\n\n\n\n\n\n\n\n# Check the mpiBLAST versions installed on the clusters:\n(node)$> module avail 2>&1 | grep -i mpiblast\n\n# Load the default mpiBLAST version:\n(node)$> module load bio/mpiBLAST\n\n# Check that it has been loaded, along with its dependencies:\n(node)$> module list\n\n# The mpiBLAST binaries should now be in your path\n(node)$> mpiformatdb --version\n(node)$> mpiblast --version\n\n\n\n\nmpiBLAST requires access to NCBI substitution matrices and pre-formatted BLAST databases. For the purposes of this tutorial, a FASTA (NR)\ndatabase has been formatted and split into 12 fragments, enabling the parallel alignment of a query against the database.\n\n\nA \n.ncbirc\n file containing the paths to the necessary data files can be downloaded from \nhere\n\nand placed in your $HOME directory (make sure to backup an existing $HOME/.ncbirc before overwriting it with the one in this tutorial).\n\n\nQuestion: Knowing that the databases can take tens of gigabytes, what is an appropriate storage location for them on the clusters?\n\n\nWe will run a test using mpiBLAST. Note that mpiBLAST requires running with at least 3 processes, 2 dedicated for scheduling tasks and\ncoordinating file output, with the additional processes performing the search.\n\n\n\n\n\n\nGaia\n\n\n# Go to the test directory and execute mpiBLAST with one core for search\n(node)$> cd ~/bioinfo-tutorial/mpiblast\n(node)$> mpirun -np 3 mpiblast -p blastp -d nr -i test.fa -o test.out\n\n# Note the speedup when using 12 cores\n(node)$> mpirun -np 12 mpiblast -p blastp -d nr -i test.fa -o test.out\n\n\n\n\n\n\n\nIris\n\n\n# Go to the test directory and execute mpiBLAST with one core for search\n(node)$> cd ~/bioinfo-tutorial/mpiblast\n(node)$> srun -np 3 mpiblast -p blastp -d nr -i test.fa -o test.out\n\n# Note the speedup when using 14 cores\n(node)$> srun -np 14 mpiblast -p blastp -d nr -i test.fa -o test.out\n\n\n\n\n\n\n\nProposed exercises\n\n\nThe following exercises are proposed for mpiBLAST:\n\n\n\n\ncreate a launcher for mpiBLAST, making sure to export the required environment to the remote nodes\n\n\nlaunch jobs with 8, 14 and 24 cores across two nodes and measure the speedup obtained\n\n\n\n\nUseful references\n\n\n\n\nABySS at SEQanswers wiki\n\n\nGromacs parallelization\n\n\nGromacs GPU acceleration\n\n\nGromacs USA workshop\n\n\nTutorial on GROMACS parallelization schemes",
            "title": "Running bioinformatics software"
        },
        {
            "location": "/advanced/Bioinformatics/#ul-hpc-tutorial-bioinformatics-software-on-the-ul-hpc-platform",
            "text": "The objective of this tutorial is to exemplify the execution of several Bioinformatics packages on top of the  UL HPC  platform.  The targeted applications are:   ABySS  Gromacs  Bowtie2  /  TopHat  mpiBLAST   The tutorial will:   show you how to load and run pre-configured versions of these applications on the clusters  show you how to download and use updated versions of Bowtie2/TopHat  discuss the parallelization capabilities of these applications",
            "title": "UL HPC Tutorial: Bioinformatics software on the UL HPC platform"
        },
        {
            "location": "/advanced/Bioinformatics/#prerequisites",
            "text": "When you look at the  software page  you will notice that some of the applications are part of the  lcsb  (gaia) or  bioinfo  (iris) software set. The modules in this set are not visible be default. To use them within a job you have to do:    Gaia  (access-gaia)$> module use $RESIF_ROOTINSTALL/lcsb/modules/all    Iris  (access-iris)$> module use /opt/apps/resif/data/stable/bioinfo/modules/all    If you want them to always be available, you can add the following line to your  .bash_private  (adapt with the above command for iris):  command -v module >/dev/null 2>&1 && module use $RESIF_ROOTINSTALL/lcsb/modules/all  This tutorial relies on several input files for the bioinformatics packages, thus you will need to download them\nbefore following the instructions in the next sections:  (access)$> mkdir -p ~/bioinfo-tutorial/gromacs ~/bioinfo-tutorial/tophat ~/bioinfo-tutorial/mpiblast\n(access)$> cd ~/bioinfo-tutorial\n(access)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/gromacs/pr.tpr -O gromacs/pr.tpr\n(access)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/tophat/test_data.tar.gz -O tophat/test_data.tar.gz\n(access)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/tophat/test2_path -O tophat/test2_path\n(access)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/mpiblast/test.fa -O mpiblast/test.fa  Or simply clone the full tutorials repository and make a link to the Bioinformatics tutorial:  (access)$> git clone https://github.com/ULHPC/tutorials.git\n(access)$> ln -s tutorials/advanced/Bioinformatics/ ~/bioinfo-tutorial",
            "title": "Prerequisites"
        },
        {
            "location": "/advanced/Bioinformatics/#abyss",
            "text": "Characterization: CPU intensive, data intensive, native parallelization",
            "title": "ABySS"
        },
        {
            "location": "/advanced/Bioinformatics/#description",
            "text": "ABySS : Assembly By Short Sequences  ABySS is a de novo, parallel, paired-end sequence assembler that is designed for short reads.\nThe single-processor version is useful for assembling genomes up to 100 Mbases in size.\nThe parallel version is implemented using MPI and is capable of assembling larger genomes  [*] .",
            "title": "Description"
        },
        {
            "location": "/advanced/Bioinformatics/#example",
            "text": "This example will be ran in an interactive session, with batch-mode executions\nbeing proposed later on as exercises.    Gaia  # Connect to Gaia (Linux/OS X):\n(yourmachine)$> ssh access-gaia.uni.lu\n\n# Request 1 full node in an interactive job:\n(access-gaia)$> oarsub -I -l nodes=1,walltime=00:30:00    Iris  # Connect to Iris (Linux/OS X):\n(yourmachine)$> ssh access-iris.uni.lu\n\n# Request half a node in an interactive job:\n(access-iris)$> srun -p interactive --qos qos-interactive -t 0-0:30:0 -N 1 -c 1 --ntasks-per-node=14 --pty bash\n\n# Load bioinfo software set\n(node)$> module use /opt/apps/resif/data/stable/bioinfo/modules/all    # Check the ABySS versions installed on the clusters:\n(node)$> module avail 2>&1 | grep -i abyss\n\n# Load the default ABySS version:\n(node)$> module load bio/ABySS\n\n# Check that it has been loaded, along with its dependencies:\n(node)$> module list\n\n# All the ABySS binaries are now in your path (check with TAB autocompletion)\n(node)$> abyss-<TAB>  In the ABySS package only the  ABYSS-P  application is parallelized using MPI and can be run on several cores (and across several nodes) using\nthe  abyss-pe  launcher.  # Create a test directory and go to it\n(node)$> mkdir ~/bioinfo-tutorial/abyss\n(node)$> cd ~/bioinfo-tutorial/abyss\n\n# Set the input files' directory in the environment\n(node)$> export ABYSSINPDIR=/mnt/isilon/projects/ulhpc-tutorials/bioinformatics/abyss\n\n# Give a name to the experiment\n(node)$> export ABYSSNAME='abysstest'    Gaia  # Set the number of cores to use based on OAR's host file\n(node)$> export ABYSSNPROC=$(cat $OAR_NODEFILE | wc -l)\n\n# Launch the paired end assembler:\n(node)$> abyss-pe mpirun=\"mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE\" name=${ABYSSNAME} np=${ABYSSNPROC} k=31 n=10 lib=pairedend pairedend=\"${ABYSSINPDIR}/SRR001666_1.fastq.bz2 ${ABYSSINPDIR}/SRR001666_2.fastq.bz2\" > ${ABYSSNAME}.out 2> ${ABYSSNAME}.err    Iris  # Set the number of cores to use based on SLURM environment variables\n(node)$> export ABYSSNPROC=$(expr $SLURM_NNODES \\* $SLURM_NTASKS_PER_NODE \\* $SLURM_CPUS_PER_TASK)\n\n# Create a hostfile\n(node)$> srun hostname | sort -n > hostfile\n\n# Launch the paired end assembler:\n(node)$> abyss-pe mpirun=\"mpirun -x PATH -x LD_LIBRARY_PATH -hostfile hostfile\" name=${ABYSSNAME} np=${ABYSSNPROC} k=31 n=10 lib=pairedend pairedend=\"${ABYSSINPDIR}/SRR001666_1.fastq.bz2 ${ABYSSINPDIR}/SRR001666_2.fastq.bz2\" > ${ABYSSNAME}.out 2> ${ABYSSNAME}.err    Question: Why do we use the -x VARIABLE parameters for mpirun?  Several options seen on the  abyss-pe  command line are crucial:   we explicitly set the mpirun command  we export several environment variables to all the remote nodes, otherwise required paths (for the binaries, libraries) would not be known by the MPI processes running there  we do not specify  -np $ABYSSNPROC  in the mpirun command, as it set with  abyss-pe 's np parameter and internally passed on to mpirun   The execution should take around 12 minutes, meanwhile we can check its progress by monitoring the .out/.err output files:  (access)$> tail -f ~/bioinfo-tutorial/abyss/abysstest.*\n# We exit the tail program with CTRL-C  On  Gaia , we can also connect to the job (recall oarsub -C $JOBID) from a different terminal or Screen window and see the different ABySS phases with  htop .  Because the  abyss-pe  workflow (pipeline) includes several processing steps with different applications of which only ABYSS-P is MPI-parallel,\nthe speedup obtained by using more than one node will be limited to ABYSS-P's execution. Several of the other applications that are part of the\nprocessing stages are however parallelized using OpenMP and pthreads and will thus take advantage of the cores available on the node where abyss-pe  was started.  The used input dataset is a well known  Illumina run of E. coli .",
            "title": "Example"
        },
        {
            "location": "/advanced/Bioinformatics/#proposed-exercises",
            "text": "Several exercises are proposed for ABySS:   create a launcher for ABySS using the commands shown in the previous section  launch jobs using 1 node: 4, 8 and 12 cores, then 2 and 4 nodes and measure the speedup obtained  unpack the two input files and place them on a node's /dev/shm, then rerun the experiment with 4, 8 and 12 cores and measure the speedup",
            "title": "Proposed exercises"
        },
        {
            "location": "/advanced/Bioinformatics/#gromacs",
            "text": "Characterization: CPU intensive, little I/O",
            "title": "GROMACS"
        },
        {
            "location": "/advanced/Bioinformatics/#description_1",
            "text": "GROMACS : GROningen MAchine for Chemical Simulations  GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.\nIt is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers  [*] .",
            "title": "Description"
        },
        {
            "location": "/advanced/Bioinformatics/#example_1",
            "text": "This example will be ran in an interactive session, with batch-mode executions\nbeing proposed later on as exercises.    Gaia  # Connect to Gaia (Linux/OS X):\n(yourmachine)$> ssh access-gaia.uni.lu\n\n# Request 1 full node in an interactive job:\n(access-gaia)$> oarsub -I -l nodes=1,walltime=00:30:00    Iris  # Connect to Iris (Linux/OS X):\n(yourmachine)$> ssh access-iris.uni.lu\n\n# Request half a node in an interactive job:\n(access-iris)$> srun -p interactive --qos qos-interactive -t 0-0:30:0 -N 1 -c 1 --ntasks-per-node=14 --pty bash    # Check the GROMACS versions installed on the clusters:\n(node)$> module avail 2>&1 | grep -i gromacs  On  Gaia , several GROMACS builds are available, we will focus only on the ones corresponding to the version 4.6.5:   bio/GROMACS/4.6.5-goolf-1.4.10-hybrid  bio/GROMACS/4.6.5-goolf-1.4.10-mt   We notice that there is a  hybrid  and a  mt  version   the hybrid version is OpenMP and MPI-enabled, all binaries have a '_mpi' suffix  the mt version is only OpenMP-enabled, as such it can take advantage of only one node's cores (however it may be faster on\nsingle-node executions than the hybrid version)   On  Iris  currently only the following version of GROMACS is available:   bio/GROMACS/2016.3-intel-2017a-hybrid   We will perform our tests with the hybrid version:    Gaia  # Load the MPI-enabled Gromacs, without CUDA support:\n(node)$> module load bio/GROMACS/4.6.5-goolf-1.4.10-hybrid\n\n# Check that it has been loaded, along with its dependencies:\n(node)$> module list\n\n# Check the capabilities of the mdrun binary, note its suffix:\n(node)$> mdrun_mpi -version 2>/dev/null\n\n# Go to the test directory\n(node)$> cd ~/bioinfo-tutorial/gromacs\n\n# Set the number of OpenMP threads to 1\n(node)$> export OMP_NUM_THREADS=1\n\n# Perform a position restrained Molecular Dynamics run\n(node)$> mpirun -np 12 -hostfile $OAR_NODEFILE -x OMP_NUM_THREADS -x PATH -x LD_LIBRARY_PATH mdrun_mpi -v -s pr -e pr -o pr -c after_pr -g prlog > test.out 2>&1    Iris  # Load the MPI-enabled Gromacs, without CUDA support:\n(node)$> module load bio/GROMACS\n\n# Check that it has been loaded, along with its dependencies:\n(node)$> module list\n\n# Check the capabilities of the mdrun binary, note its suffix:\n(node)$> gmx_mpi -version 2>/dev/null\n\n# Go to the test directory\n(node)$> cd ~/bioinfo-tutorial/gromacs\n\n# Set the number of OpenMP threads to 1\n(node)$> export OMP_NUM_THREADS=1\n\n# Perform a position restrained Molecular Dynamics run\n(node)$> srun -n 12 gmx_mpi mdrun -v -s pr -e pr -o pr -c after_pr -g prlog > test.out 2>&1    We notice here that we are running  mdrun_mpi / gmx_mpi  in parallel with mpirun on 12/14 cores, and we explicitly export the OMP_NUM_THREADS\nvariable to any remote node such that only one thread per MPI process will be created.  Question: What will happen if we do not set the number of OpenMP threads to 1?  GROMACS has many parallelization options and several parameters can be tuned to give you better performance depending on your workflow, see the references in the last section of this tutorial.  The used input corresponds to the  Ribonuclease S-peptide  example,\nwhich has been changed to perform 50k steps in the Molecular Dynamics run with position restraints on the peptide.",
            "title": "Example"
        },
        {
            "location": "/advanced/Bioinformatics/#proposed-exercises_1",
            "text": "Several exercises are proposed for GROMACS:   create a launcher for GROMACS using the commands shown in the previous section  launch jobs using 1 node: 1, 2, 4, 8, 10 and 12 cores and measure the speedup obtained  check what happens when executing mdrun with 16 and 24 cores  launch a job using one full node that has GPU cards and run the GPU-enabled GROMACS to see if a speedup is obtained",
            "title": "Proposed exercises"
        },
        {
            "location": "/advanced/Bioinformatics/#bowtie2tophat",
            "text": "Characterization: data intensive, RAM intensive",
            "title": "Bowtie2/TopHat"
        },
        {
            "location": "/advanced/Bioinformatics/#description_2",
            "text": "Bowtie2 : Fast and sensitive read alignment  Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes  [*] .  TopHat  : A spliced read mapper for RNA-Seq  TopHat is a program that aligns RNA-Seq reads to a genome in order to identify exon-exon splice junctions. It is built on the ultrafast short read mapping program Bowtie  [*] .",
            "title": "Description"
        },
        {
            "location": "/advanced/Bioinformatics/#example_2",
            "text": "This example will show you how to use the latest version of TopHat in conjunction with the latest Bowtie2, by using the\nversions prebuilt for Linux by the developers.    Gaia  # Connect to Gaia (Linux/OS X):\n(yourmachine)$> ssh access-gaia.uni.lu\n\n# Request 1 full node in an interactive job:\n(gaia-frontend)$> oarsub -I -l nodes=1,walltime=00:30:00    Iris  # Connect to Iris (Linux/OS X):\n(yourmachine)$> ssh access-iris.uni.lu\n\n# Request half a node in an interactive job:\n(access-iris)$> srun -p interactive --qos qos-interactive -t 0-0:30:0 -N 1 -c 14 --ntasks-per-node=1 --pty bash    # Create a folder for the new software and go to it\n(node)$> mkdir ~/bioinfo-tutorial/newsoft\n(node)$> cd ~/bioinfo-tutorial/newsoft\n\n# Download latest Bowtie2 and Tophat, plus the SAM tools dependency:\n(node)$> wget https://downloads.sourceforge.net/project/bowtie-bio/bowtie2/2.3.2/bowtie2-2.3.2-linux-x86_64.zip\n(node)$> wget --no-check-certificate https://ccb.jhu.edu/software/tophat/downloads/tophat-2.1.1.Linux_x86_64.tar.gz\n(node)$> wget https://github.com/samtools/samtools/releases/download/1.4.1/samtools-1.4.1.tar.bz2\n\n# Unpack the three archives\n(node)$> unzip bowtie2-2.3.2-linux-x86_64.zip\n(node)$> tar xzvf tophat-2.1.1.Linux_x86_64.tar.gz\n(node)$> tar xjvf samtools-1.4.1.tar.bz2  SAM tools requires compilation:    Gaia  (node)$> cd samtools-1.4.1 && make && cd ..    Iris  (node)$> module load devel/ncurses/6.0-intel-2017a\n(node)$> module load tools/bzip2/1.0.6-intel-2017a\n(node)$> cd samtools-1.4.1 && make && cd ..\n(node)$> module load lib/tbb    # Create a file containing the paths to the binaries, to be sourced when needed\n(node)$> echo \"export PATH=$HOME/bioinfo-tutorial/newsoft/bowtie2-2.3.2:\\$PATH\" > newsoft\n(node)$> echo \"export PATH=$HOME/bioinfo-tutorial/newsoft/tophat-2.1.1.Linux_x86_64:\\$PATH\" >> newsoft\n(node)$> echo \"export PATH=$HOME/bioinfo-tutorial/newsoft/samtools-1.4.1:\\$PATH\" >> newsoft\n(node)$> source newsoft\n\n# You can now check that both main applications can be run:\n(node)$> bowtie2 --version\n(node)$> tophat2 --version  Now we will make a quick TopHat test, using the provided sample files:  # Go to the test directory, unpack the sample dataset and go to it\n(node)$> cd ~/bioinfo-tutorial/tophat\n(node)$> tar xzvf test_data.tar.gz\n(node)$> cd test_data\n\n\n# Launch TopHat, with Bowtie2 in serial mode\n(node)$> tophat -r 20 test_ref reads_1.fq reads_2.fq\n\n# Launch TopHat, with Bowtie2 in parallel mode\n(node)$> tophat -p 12 -r 20 test_ref reads_1.fq reads_2.fq  We can see that for this fast execution, increasing the number of threads does not improve the calculation time due to the relatively high overhead of thread creation.\nNote that TopHat / Bowtie are not MPI applications and as such can take advantage of at most one compute node.  Next, we will make a longer test, where it will be interesting to monitor the TopHat pipeline (with  htop  for example) to see the transitions between the serial\nand parallel stages (left as an exercise).  # Load the file which will export $TOPHATTEST2 in the environment\n(node)$> source ~/bioinfo-tutorial/tophat/test2_path\n\n# Launch TopHat, with Bowtie2 in parallel mode\n(node)$> tophat2 -p 12 -g 1 -r 200 --mate-std-dev 30 -o ./  $TOPHATTEST2/chr10.hs $TOPHATTEST2/SRR027888.SRR027890_chr10_1.fastq $TOPHATTEST2/SRR027888.SRR027890_chr10_2.fastq  The input data for the first test corresponds to the  TopHat test set ,\nwhile the second test is an example of aligning reads to the chromosome 10 of the human genome  as given here .",
            "title": "Example"
        },
        {
            "location": "/advanced/Bioinformatics/#proposed-exercises_2",
            "text": "The following exercises are proposed for TopHat/Bowtie2:   create a launcher for TopHat using the commands shown in the previous section  launch jobs with 1, 2, 4, 8 and 10 cores on one node, using the second test files, and measure the speedup obtained",
            "title": "Proposed exercises"
        },
        {
            "location": "/advanced/Bioinformatics/#mpiblast",
            "text": "Characterization: data intensive, little RAM overhead, native parallelization",
            "title": "mpiBLAST"
        },
        {
            "location": "/advanced/Bioinformatics/#description_3",
            "text": "mpiBLAST : Open-Source Parallel BLAST  mpiBLAST is a freely available, open-source, parallel implementation of NCBI BLAST. By efficiently utilizing distributed computational resources through database fragmentation, query segmentation, intelligent scheduling, and parallel I/O, mpiBLAST improves NCBI BLAST performance by several orders of magnitude while scaling to hundreds of processors   [*] .",
            "title": "Description"
        },
        {
            "location": "/advanced/Bioinformatics/#example_3",
            "text": "This example will be ran in an interactive session, with batch-mode executions\nbeing proposed later on as exercises.    Gaia  # Connect to Gaia (Linux/OS X):\n(yourmachine)$> ssh access-gaia.uni.lu\n\n# Request 1 full node in an interactive job:\n(access-gaia)$> oarsub -I -l nodes=1,walltime=00:30:00\n\n# Load the lcsb software set\n(node)$> module use $RESIF_ROOTINSTALL/lcsb/modules/all    Iris  # Connect to Iris (Linux/OS X):\n(yourmachine)$> ssh access-iris.uni.lu\n\n# Request half a node in an interactive job:\n(access-iris)$> srun -p interactive --qos qos-interactive -t 0-0:30:0 -N 1 -c 1 --ntasks-per-node=14 --pty bash\n\n# Load the bioinfo software set\n(node)$> module use /opt/apps/resif/data/stable/bioinfo/modules/all    # Check the mpiBLAST versions installed on the clusters:\n(node)$> module avail 2>&1 | grep -i mpiblast\n\n# Load the default mpiBLAST version:\n(node)$> module load bio/mpiBLAST\n\n# Check that it has been loaded, along with its dependencies:\n(node)$> module list\n\n# The mpiBLAST binaries should now be in your path\n(node)$> mpiformatdb --version\n(node)$> mpiblast --version  mpiBLAST requires access to NCBI substitution matrices and pre-formatted BLAST databases. For the purposes of this tutorial, a FASTA (NR)\ndatabase has been formatted and split into 12 fragments, enabling the parallel alignment of a query against the database.  A  .ncbirc  file containing the paths to the necessary data files can be downloaded from  here \nand placed in your $HOME directory (make sure to backup an existing $HOME/.ncbirc before overwriting it with the one in this tutorial).  Question: Knowing that the databases can take tens of gigabytes, what is an appropriate storage location for them on the clusters?  We will run a test using mpiBLAST. Note that mpiBLAST requires running with at least 3 processes, 2 dedicated for scheduling tasks and\ncoordinating file output, with the additional processes performing the search.    Gaia  # Go to the test directory and execute mpiBLAST with one core for search\n(node)$> cd ~/bioinfo-tutorial/mpiblast\n(node)$> mpirun -np 3 mpiblast -p blastp -d nr -i test.fa -o test.out\n\n# Note the speedup when using 12 cores\n(node)$> mpirun -np 12 mpiblast -p blastp -d nr -i test.fa -o test.out    Iris  # Go to the test directory and execute mpiBLAST with one core for search\n(node)$> cd ~/bioinfo-tutorial/mpiblast\n(node)$> srun -np 3 mpiblast -p blastp -d nr -i test.fa -o test.out\n\n# Note the speedup when using 14 cores\n(node)$> srun -np 14 mpiblast -p blastp -d nr -i test.fa -o test.out",
            "title": "Example"
        },
        {
            "location": "/advanced/Bioinformatics/#proposed-exercises_3",
            "text": "The following exercises are proposed for mpiBLAST:   create a launcher for mpiBLAST, making sure to export the required environment to the remote nodes  launch jobs with 8, 14 and 24 cores across two nodes and measure the speedup obtained",
            "title": "Proposed exercises"
        },
        {
            "location": "/advanced/Bioinformatics/#useful-references",
            "text": "ABySS at SEQanswers wiki  Gromacs parallelization  Gromacs GPU acceleration  Gromacs USA workshop  Tutorial on GROMACS parallelization schemes",
            "title": "Useful references"
        },
        {
            "location": "/advanced/Galaxy/",
            "text": "-\n- mode: markdown; mode: visual-line; fill-column: 80 -\n-\n\n\nCopyright (c) 2015 Sarah Diehl \nsarah.diehl@uni.lu\n\n\n\n\nGalaxy Introduction Exercise: From Peaks to Genes\n\n\n \n \n \n \n \n \n\n\n\n\nFor a version of this tutorial with the results of important steps embedded and direct links to workflows, go to the Galaxy server, select \"Shared Data\" and then \"Pages\" from the top menu and have a look at \nGalaxy Introduction\n.\n\n\nScenario\n\n\nWe stumbled upon a paper (\nLi et al., Cell Stem Cell 2012\n) that contains the analysis of possible target genes of an interesting protein. The targets were obtained by ChIP-seq and the raw data is available through \nGEO\n. The list of genes however is neither in the supplement of the paper nor part of the GEO submission. The closest thing we can find is a list of the regions where the signal is significantly enriched (peaks). The goal of this exercise is to turn this list of genomic regions into a list of possible target genes.\n\n\n(Disclaimer: We are not affiliated with the authors of the paper and we don't make a statement about the relevance or quality of the paper. It is just a fitting example and nothing else.)\n\n\nStep 1: Upload peaks\n\n\nDownload the list of peaks (the file \"GSE37268_mof3.out.hpeak.txt.gz\") from GEO (\nclick here to get to the GEO entry\n) to your computer. Use the upload button to upload the file to Galaxy and select \"mm9\" as the genome. Galaxy will automatically unpack the file.\n\n\nThis file is not in any standard format and just by looking at it, we cannot find out what the numbers in the different columns mean. In the paper they mention that they used the peak caller \nHPeak\n. By looking at the HPeak manual we can find out that the columns contain the following information:\n\n\n\n\nchromosome name*\n\n\nstart coordinate\n\n\nend coordinate\n\n\nlength\n\n\nlocation within the peak that has the highest hypothetical DNA fragment coverage (summit)\n\n\nnot relevant\n\n\nnot relevant\n\n\n\n\n(*Note that the first column only contains the chromosome number, and X and Y are replaced by 20 and 21 for easier numerical manipulations.)\n\n\nStep 2: Get genes from UCSC\n\n\nWe also need a list of genes in mouse, which we can obtain from UCSC. Galaxy has the UCSC table browser integrated as a tool, so we don't need to download the data to our computers.\n\n\n\n\nTool: Get Data -> UCSC main table browser\n\n\nSelect clade \"Mammal\", genome \"Mouse\", assembly \"mm9\"\n\n\nSelect group \"Genes and Gene Prediction Tracks\", track \"RefSeq Genes\"\n\n\nSelect table \"refGene\"\n\n\nSelect region \"genome\"\n\n\nSelect output format \"BED\"\n\n\nClick button \"get output\"\n\n\nClick button \"Send query to Galaxy\"\n\n\n\n\nStep 3: Adjust chromosome naming\n\n\nHave a look at both input files (either in the little preview window in the history or click on the eye icon to see one in the main frame) and find out what are the differences in the chromosome naming.\n\n\nApply the following workflow to GSE37268_mof3.out.hpeak.txt: Workflow 'Add \"chr\" at beginning of each line'.\n\n\nAfter importing you can in the future use it by scrolling to the bottom of the tool panel, click on \"All workflows\" and then on the workflow name.\n\n\nFrom carefully reading the HPeak manual, we should remember that it puts \"20\" and \"21\" instead of \"X\" and \"Y\". So now the chromosome names all start properly with \"chr\", but we still have \"chr20\" and \"chr21\" instead of \"chrX\" and \"chrY\".\n\n\n\n\nTool: Text Manipulation -> Replace text in a specific column\n\n\nInput: result of workflow (Text reformatting on data X)\n\n\nIn colum: Column 1\n\n\nFind pattern: chr20\n\n\nReplace with: chrX\n\n\nDo the same for \"chr21\" and \"chrY\", make sure you use the result of the first replacement as input (use rerun button and change input and search/replace)\n\n\n\n\nMake sure the format of the output file is \"interval\", otherwise change it by clicking the pencil icon (do not convert to new format, but change data type).\n\n\nStep 4: Visualize peaks\n\n\nTo visualize the peaks it's best to convert them to BED format first, because most viewers cannot deal with interval (because interval format just exists in Galaxy).\n\n\n\n\nClick on the pencil icon of the latest dataset\n\n\nUnder the header \"Convert to new format\" select \"Convert Genomic Intervals to BED\"\n\n\nClick \"Convert\"\n\n\nLook at the new dataset. Some columns with generic names have been added and others were removed to comply to BED format rules.\n\n\nThis generated a new dataset in BED format which we'll use for visualization. We will however continue to work with the interval dataset.\n\n\n\n\nDisplay in IGB:\n\n\n\n\nGo to the \nIGB website\n\n\nDownload and install the Integrated Genome Browser on your computer\n\n\nStart IGB and in the right panel select species \"Mus musculus\" and  genome version \"M_musculus_Jul_2007\"\n\n\nGo back to Galaxy\n\n\nClick on the link \"View\" after \"display with IGB\" (expanded history view of BED dataset)\n\n\nType in your HPC credentials again, to allow IGB to access the data (you might also need to allow some connections and/or accept certificates)\n\n\nBack in IGB, click \"Load Data\" next to the scroll bar on top to get to see the new track\n\n\n\n\nStep 5: Add promoter region to gene records\n\n\n\n\nTool: Operate on Genomic Intervals -> Get flanks\n\n\nInput dataset: RefSeq genes from UCSC (UCSC Main on Mouse: refGene (genome))\n\n\nOptions: Region: \"Around Start\", Location: \"Upstream\",  Offset: 10000, Length: 12000\n\n\n\n\nInspect the resulting BED file and through comparing with the input find out what this operation actually did. Just look at the contents and compare the rows in the input to the rows in the output to find out how the start and end positions changed. Rename the dataset (by clicking on the pencil icon) to reflect your findings.\n\n\nStep 6: Find overlaps\n\n\n\n\nTool: Operate on Genomic Intervals -> Intersect\n\n\nReturn: Overlapping Intervals\n\n\nof: result of step 5 (Get flanks on data X)\n\n\nthat intersect: result of step 3 (second Replace text)\n\n\n\n\nThe order of the inputs is important! We want to end up with a list of genes, so the corresponding dataset needs to be the first input.\n\n\nStep 7: Count genes on different chromosomes\n\n\nTo get a better overview of the genes we obtained, we want to look at their distribution across the different chromosomes.\n\n\n\n\nTool: Statistics -> Count occurrences of each record\n\n\nInput: result from step 6 (Intersect on data X and data X)\n\n\nSelect column 1 (c1) with the chromosome names\n\n\n\n\nStep 8: Draw barchart\n\n\n\n\nTool: Bar chart (use tool search to find it)\n\n\nInput: result of step 7\n\n\nUse X Tick labels: Yes, column 2\n\n\nNumerical column: c1\n\n\nPlot title is up to you\n\n\nLabel for Y axis: number of genes\n\n\n\n\nGalaxy has a second option to visualise tabular data, with built-in dynamic visualisations:\n\n\n\n\nExpand the dataset view and click on the visualization icon\n\n\nChoose \"Charts\"\n\n\nEnter a chart title, e.g. \"Genes on different chromsomes\"\n\n\nSelect \"Bar diagrams\" -> \"Regular\"\n\n\nOn the top, click on \"Add Data\"\n\n\nEnter a label, e.g. \"count\"\n\n\nValues for x-axis: Column: 2 [str]\n\n\nValues for y-axis: Column: 1 [int]\n\n\nOn the very top, click \"Draw\"\n\n\n\n\nStep 9: Name your history\n\n\nIn the history column click on \"Unnamed history\" at the top to rename it.\n\n\nStep 10: Make a workflow out of steps 6 to 8\n\n\n\n\nClick on the history options and select \"Extract workflow\"\n\n\nOn the top click on \"Uncheck all\", then specifically check \"Treat as input dataset\" on GSE37268_mof3.out.hpeak.txt and UCSC Main on Mouse: refGene (genome), as well as \"Include\" on the Intersect, Count and Bar chart\n\n\nClick \"Create Workflow\"\n\n\n\n\nTo make sure our workflow is correct we look at it in the editor and make some small adjustments.\n\n\n\n\nTop menu: Workflow\n\n\nClick on the name of your new workflow and select \"Edit\"\n\n\n\n\nThe individual steps are displayed as boxes and their outputs and inputs are connected through lines. When you click on a box you see the tool options on the right. Besides the tools you should see two additional boxes titled \"Input dataset\". These represent the data we want to feed into our workflow. Although we have our two inputs in the workflow they are missing their connection to the first tool (Intersect), because we didn't carry over the intermediate steps. Connect each input dataset to the Intersect tool by dragging the arrow pointing outwards on the right of its box (which denotes an output) to an arrow on the left of the Intersect box pointing inwards (which denotes an input). Connect each input dataset with a different input of Intersect.\n\n\nYou should also change the names of the input datasets to remember that the first one contains genes and the second one peaks. Don't forget to save it in the end by clicking on \"Options\" (top right) and selecting \"Save\".\n\n\nStep 11: Share workflow\n\n\nShare your new workflow with the person to your left.\n\n\n\n\nTop menu: Workflow\n\n\nClick on your workflow's name and select \"Share or publish\"\n\n\nClick \"Share with a user\"\n\n\nEnter the username of the person to your left\n\n\nHit \"Share\"\n\n\nWait for the person on your right to do the same\n\n\nReload the workflows by clicking again on \"Workflow\" in the top menu\n\n\nUnder the header \"Workflows shared with you by others\" you should now see your right neighbour's workflow\n\n\nClick on its name and select \"View\"\n\n\nCompare with your workflow\n\n\n\n\nStep 12: Cleaning up\n\n\nDownload your workflow:\n\n\n\n\nTop menu: Workflow\n\n\nClick on your workflow's name and select \"Download or Export\"\n\n\nClick on \"Download workflow to file so that it can be saved or imported into another Galaxy server\"\n\n\nSave the workflow file on your computer\n\n\n\n\nClean up history:\n\nDelete all datasets that are neither initial input nor final results. Everything that can be easily recreated or is just part of an intermediate step can go. What I would keep are the extended genes, the intersect result and the bar chart (for a real analysis I would recommend to also download all final results). Deleted datasets can be undeleted for some time (see history options) and will only be ultimately removed from the server if they aren't used somewhere else or by somebody else and stay deleted for several weeks.\n\n\nYou can create new histories in the history options with \"Create New\".\n\n\nTo delete old histories:\n\n\n\n\nHistory options: Saved histories\n\n\nCheck the history you want to delete\n\n\nClick \"Delete Permanently\" on the bottom if you need to free up space or just \"Delete\"\n\n\n\n\nEND",
            "title": "Galaxy"
        },
        {
            "location": "/advanced/Galaxy/#galaxy-introduction-exercise-from-peaks-to-genes",
            "text": "For a version of this tutorial with the results of important steps embedded and direct links to workflows, go to the Galaxy server, select \"Shared Data\" and then \"Pages\" from the top menu and have a look at  Galaxy Introduction .",
            "title": "Galaxy Introduction Exercise: From Peaks to Genes"
        },
        {
            "location": "/advanced/Galaxy/#scenario",
            "text": "We stumbled upon a paper ( Li et al., Cell Stem Cell 2012 ) that contains the analysis of possible target genes of an interesting protein. The targets were obtained by ChIP-seq and the raw data is available through  GEO . The list of genes however is neither in the supplement of the paper nor part of the GEO submission. The closest thing we can find is a list of the regions where the signal is significantly enriched (peaks). The goal of this exercise is to turn this list of genomic regions into a list of possible target genes.  (Disclaimer: We are not affiliated with the authors of the paper and we don't make a statement about the relevance or quality of the paper. It is just a fitting example and nothing else.)",
            "title": "Scenario"
        },
        {
            "location": "/advanced/Galaxy/#step-1-upload-peaks",
            "text": "Download the list of peaks (the file \"GSE37268_mof3.out.hpeak.txt.gz\") from GEO ( click here to get to the GEO entry ) to your computer. Use the upload button to upload the file to Galaxy and select \"mm9\" as the genome. Galaxy will automatically unpack the file.  This file is not in any standard format and just by looking at it, we cannot find out what the numbers in the different columns mean. In the paper they mention that they used the peak caller  HPeak . By looking at the HPeak manual we can find out that the columns contain the following information:   chromosome name*  start coordinate  end coordinate  length  location within the peak that has the highest hypothetical DNA fragment coverage (summit)  not relevant  not relevant   (*Note that the first column only contains the chromosome number, and X and Y are replaced by 20 and 21 for easier numerical manipulations.)",
            "title": "Step 1: Upload peaks"
        },
        {
            "location": "/advanced/Galaxy/#step-2-get-genes-from-ucsc",
            "text": "We also need a list of genes in mouse, which we can obtain from UCSC. Galaxy has the UCSC table browser integrated as a tool, so we don't need to download the data to our computers.   Tool: Get Data -> UCSC main table browser  Select clade \"Mammal\", genome \"Mouse\", assembly \"mm9\"  Select group \"Genes and Gene Prediction Tracks\", track \"RefSeq Genes\"  Select table \"refGene\"  Select region \"genome\"  Select output format \"BED\"  Click button \"get output\"  Click button \"Send query to Galaxy\"",
            "title": "Step 2: Get genes from UCSC"
        },
        {
            "location": "/advanced/Galaxy/#step-3-adjust-chromosome-naming",
            "text": "Have a look at both input files (either in the little preview window in the history or click on the eye icon to see one in the main frame) and find out what are the differences in the chromosome naming.  Apply the following workflow to GSE37268_mof3.out.hpeak.txt: Workflow 'Add \"chr\" at beginning of each line'.  After importing you can in the future use it by scrolling to the bottom of the tool panel, click on \"All workflows\" and then on the workflow name.  From carefully reading the HPeak manual, we should remember that it puts \"20\" and \"21\" instead of \"X\" and \"Y\". So now the chromosome names all start properly with \"chr\", but we still have \"chr20\" and \"chr21\" instead of \"chrX\" and \"chrY\".   Tool: Text Manipulation -> Replace text in a specific column  Input: result of workflow (Text reformatting on data X)  In colum: Column 1  Find pattern: chr20  Replace with: chrX  Do the same for \"chr21\" and \"chrY\", make sure you use the result of the first replacement as input (use rerun button and change input and search/replace)   Make sure the format of the output file is \"interval\", otherwise change it by clicking the pencil icon (do not convert to new format, but change data type).",
            "title": "Step 3: Adjust chromosome naming"
        },
        {
            "location": "/advanced/Galaxy/#step-4-visualize-peaks",
            "text": "To visualize the peaks it's best to convert them to BED format first, because most viewers cannot deal with interval (because interval format just exists in Galaxy).   Click on the pencil icon of the latest dataset  Under the header \"Convert to new format\" select \"Convert Genomic Intervals to BED\"  Click \"Convert\"  Look at the new dataset. Some columns with generic names have been added and others were removed to comply to BED format rules.  This generated a new dataset in BED format which we'll use for visualization. We will however continue to work with the interval dataset.   Display in IGB:   Go to the  IGB website  Download and install the Integrated Genome Browser on your computer  Start IGB and in the right panel select species \"Mus musculus\" and  genome version \"M_musculus_Jul_2007\"  Go back to Galaxy  Click on the link \"View\" after \"display with IGB\" (expanded history view of BED dataset)  Type in your HPC credentials again, to allow IGB to access the data (you might also need to allow some connections and/or accept certificates)  Back in IGB, click \"Load Data\" next to the scroll bar on top to get to see the new track",
            "title": "Step 4: Visualize peaks"
        },
        {
            "location": "/advanced/Galaxy/#step-5-add-promoter-region-to-gene-records",
            "text": "Tool: Operate on Genomic Intervals -> Get flanks  Input dataset: RefSeq genes from UCSC (UCSC Main on Mouse: refGene (genome))  Options: Region: \"Around Start\", Location: \"Upstream\",  Offset: 10000, Length: 12000   Inspect the resulting BED file and through comparing with the input find out what this operation actually did. Just look at the contents and compare the rows in the input to the rows in the output to find out how the start and end positions changed. Rename the dataset (by clicking on the pencil icon) to reflect your findings.",
            "title": "Step 5: Add promoter region to gene records"
        },
        {
            "location": "/advanced/Galaxy/#step-6-find-overlaps",
            "text": "Tool: Operate on Genomic Intervals -> Intersect  Return: Overlapping Intervals  of: result of step 5 (Get flanks on data X)  that intersect: result of step 3 (second Replace text)   The order of the inputs is important! We want to end up with a list of genes, so the corresponding dataset needs to be the first input.",
            "title": "Step 6: Find overlaps"
        },
        {
            "location": "/advanced/Galaxy/#step-7-count-genes-on-different-chromosomes",
            "text": "To get a better overview of the genes we obtained, we want to look at their distribution across the different chromosomes.   Tool: Statistics -> Count occurrences of each record  Input: result from step 6 (Intersect on data X and data X)  Select column 1 (c1) with the chromosome names",
            "title": "Step 7: Count genes on different chromosomes"
        },
        {
            "location": "/advanced/Galaxy/#step-8-draw-barchart",
            "text": "Tool: Bar chart (use tool search to find it)  Input: result of step 7  Use X Tick labels: Yes, column 2  Numerical column: c1  Plot title is up to you  Label for Y axis: number of genes   Galaxy has a second option to visualise tabular data, with built-in dynamic visualisations:   Expand the dataset view and click on the visualization icon  Choose \"Charts\"  Enter a chart title, e.g. \"Genes on different chromsomes\"  Select \"Bar diagrams\" -> \"Regular\"  On the top, click on \"Add Data\"  Enter a label, e.g. \"count\"  Values for x-axis: Column: 2 [str]  Values for y-axis: Column: 1 [int]  On the very top, click \"Draw\"",
            "title": "Step 8: Draw barchart"
        },
        {
            "location": "/advanced/Galaxy/#step-9-name-your-history",
            "text": "In the history column click on \"Unnamed history\" at the top to rename it.",
            "title": "Step 9: Name your history"
        },
        {
            "location": "/advanced/Galaxy/#step-10-make-a-workflow-out-of-steps-6-to-8",
            "text": "Click on the history options and select \"Extract workflow\"  On the top click on \"Uncheck all\", then specifically check \"Treat as input dataset\" on GSE37268_mof3.out.hpeak.txt and UCSC Main on Mouse: refGene (genome), as well as \"Include\" on the Intersect, Count and Bar chart  Click \"Create Workflow\"   To make sure our workflow is correct we look at it in the editor and make some small adjustments.   Top menu: Workflow  Click on the name of your new workflow and select \"Edit\"   The individual steps are displayed as boxes and their outputs and inputs are connected through lines. When you click on a box you see the tool options on the right. Besides the tools you should see two additional boxes titled \"Input dataset\". These represent the data we want to feed into our workflow. Although we have our two inputs in the workflow they are missing their connection to the first tool (Intersect), because we didn't carry over the intermediate steps. Connect each input dataset to the Intersect tool by dragging the arrow pointing outwards on the right of its box (which denotes an output) to an arrow on the left of the Intersect box pointing inwards (which denotes an input). Connect each input dataset with a different input of Intersect.  You should also change the names of the input datasets to remember that the first one contains genes and the second one peaks. Don't forget to save it in the end by clicking on \"Options\" (top right) and selecting \"Save\".",
            "title": "Step 10: Make a workflow out of steps 6 to 8"
        },
        {
            "location": "/advanced/Galaxy/#step-11-share-workflow",
            "text": "Share your new workflow with the person to your left.   Top menu: Workflow  Click on your workflow's name and select \"Share or publish\"  Click \"Share with a user\"  Enter the username of the person to your left  Hit \"Share\"  Wait for the person on your right to do the same  Reload the workflows by clicking again on \"Workflow\" in the top menu  Under the header \"Workflows shared with you by others\" you should now see your right neighbour's workflow  Click on its name and select \"View\"  Compare with your workflow",
            "title": "Step 11: Share workflow"
        },
        {
            "location": "/advanced/Galaxy/#step-12-cleaning-up",
            "text": "Download your workflow:   Top menu: Workflow  Click on your workflow's name and select \"Download or Export\"  Click on \"Download workflow to file so that it can be saved or imported into another Galaxy server\"  Save the workflow file on your computer   Clean up history: \nDelete all datasets that are neither initial input nor final results. Everything that can be easily recreated or is just part of an intermediate step can go. What I would keep are the extended genes, the intersect result and the bar chart (for a real analysis I would recommend to also download all final results). Deleted datasets can be undeleted for some time (see history options) and will only be ultimately removed from the server if they aren't used somewhere else or by somebody else and stay deleted for several weeks.  You can create new histories in the history options with \"Create New\".  To delete old histories:   History options: Saved histories  Check the history you want to delete  Click \"Delete Permanently\" on the bottom if you need to free up space or just \"Delete\"",
            "title": "Step 12: Cleaning up"
        },
        {
            "location": "/advanced/Galaxy/#end",
            "text": "",
            "title": "END"
        },
        {
            "location": "/advanced/Allinea/",
            "text": "-\n- mode: markdown; mode: visual-line; fill-column: 80 -\n-\n\n\n\n\nHPC tutorial: Unified profiling and debugging with Allinea\n\n\n \n \n \n \n\n\n\n\nby\n \nP. Wohlschlegel (Allinea)\n\n\nDeveloping an HPC application designed for MPI or hybrid environments can be a very challenging task - especially when it comes to fixing bugs, optimizing workload or even resolving both type of issues simultaneously. Those challenges are made easier with Allinea Tools. Using Allinea environment, it is now possible for developers to adopt instantly efficient and scalable development tools and to focus immediately on their core activity : science.\n\n\nAllinea tools are available for you in University of Luxembourg, and this workshop has been arranged to help you get started with Allinea MAP and Allinea DDT. During this workshop, we will review the capabilities of Allinea tools and see, through hands-on exercises, how this environment can help reduce your time to results to the minimum.\n\n\nHands-on 1: Getting Started with Allinea\n\n\nFiles:\n\n\n\n\nExercice 1 slides\n\n\ncstartmpi.c\n\n\nMakefile\n\n\n\n\nHands-on 2: Allinea MAP\n\n\nFiles:\n\n\n\n\nExercise 2 slides\n\n\nslow.f90\n\n\nMakefile",
            "title": "Allinea"
        },
        {
            "location": "/advanced/Allinea/#hpc-tutorial-unified-profiling-and-debugging-with-allinea",
            "text": "by   P. Wohlschlegel (Allinea)  Developing an HPC application designed for MPI or hybrid environments can be a very challenging task - especially when it comes to fixing bugs, optimizing workload or even resolving both type of issues simultaneously. Those challenges are made easier with Allinea Tools. Using Allinea environment, it is now possible for developers to adopt instantly efficient and scalable development tools and to focus immediately on their core activity : science.  Allinea tools are available for you in University of Luxembourg, and this workshop has been arranged to help you get started with Allinea MAP and Allinea DDT. During this workshop, we will review the capabilities of Allinea tools and see, through hands-on exercises, how this environment can help reduce your time to results to the minimum.",
            "title": "HPC tutorial: Unified profiling and debugging with Allinea"
        },
        {
            "location": "/advanced/Allinea/#hands-on-1-getting-started-with-allinea",
            "text": "Files:   Exercice 1 slides  cstartmpi.c  Makefile",
            "title": "Hands-on 1: Getting Started with Allinea"
        },
        {
            "location": "/advanced/Allinea/#hands-on-2-allinea-map",
            "text": "Files:   Exercise 2 slides  slow.f90  Makefile",
            "title": "Hands-on 2: Allinea MAP"
        },
        {
            "location": "/advanced/TotalView/",
            "text": "-\n- mode: markdown; mode: auto-fill; fill-column: 80 -\n-\n\n\nCopyright (c) 2014 Sebastien Varrette \nSebastien.Varrette@uni.lu\n\n\n\n\nUL HPC Tutorial: Direct, Reverse and parallel Memory debugging with TotalView\n\n\n \n \n \n \n \n \n\n\n\n\nThe objective of this tutorial is to get a brief overview of the \nTotalView\n, a GUI-based source code defect analysis tool that gives you unprecedented control over processes and thread execution and visibility into program state and variables.\n\n\nThis practical session will be organized as follows:\n\n\n\n\nStartup and Overview\n\n\nIHM Navigation and process control\n\n\nAction points (TP)\n\n\nExamination and Data Analysis (TP)\n\n\nDebugging Parallel Applications (TP)\n\n\nMemory reports using MemoryScape (TP)\n\n\nRemote Debugging\n\n\nCUDA Debugging\n\n\nXeon Phi Debugging\n\n\nMemory Debugging with MemoryScape\n\n\nEvents and memory errors (TP)\n\n\nDelayed Scripted debugging (non-interactive) (TP)\n\n\nReverse debugging using ReplayEngine (TP)\n\n\nAsynchronous control of Parallel Applications (TP)\n\n\nType Transformation (TP)\n\n\n\n\nWhile TotalView is available on the \nUL HPC Platform\n, the specific exercises proposed have been embedded into a Virtual Machine (VM) you will need to setup to run this practical session.\n\n\nPre-Requisites\n\n\n\n\nInstall \nOracle's VirtualBox\n\n\nDownload the bootable ISO \ntv_trainingcdU10_20140815.iso\n (md5sum: \n30a844ddda80ddf505c28eb4f4b6f1bf\n) which contains:\n\n\na bootable Linux Ubuntu distribition\n\n\na version of TotalView\n\n\nthe PDF documents describing the practical session.\n\n\n\n\nIf the participants did not have the time to download the ISO, they shall come with a USB stick having a capacity of at least 2GB.\n\n\nTo create a new VM:\n\n\n\n\nOpen VirtualBox\n\n\nSelect the \nNew\n icon\n\n\nName: \nTotalView\n\n\nType: \nLinux\n\n\nVersion: \nUbuntu 64 bits\n\n\nMemory Size: \n512MB\n\n\nCreate a virtual hard drive now\n\n\n\n\nClick on \"Create\": select a VDI format, dynamically allocated.\n* Now select the \nStart\n icon over the newly created VM\n  * open the folder icon to browse your disk and select the  \ntv_trainingcdU10_20140815.iso\n ISO on which you'll boot\n\n\nYou're now ready for the tutorial.\n\n\nTutorial\n\n\n\n\nContent of the tutorial\n\n\nAnswers of the tutorial",
            "title": "TotalView"
        },
        {
            "location": "/advanced/TotalView/#ul-hpc-tutorial-direct-reverse-and-parallel-memory-debugging-with-totalview",
            "text": "The objective of this tutorial is to get a brief overview of the  TotalView , a GUI-based source code defect analysis tool that gives you unprecedented control over processes and thread execution and visibility into program state and variables.  This practical session will be organized as follows:   Startup and Overview  IHM Navigation and process control  Action points (TP)  Examination and Data Analysis (TP)  Debugging Parallel Applications (TP)  Memory reports using MemoryScape (TP)  Remote Debugging  CUDA Debugging  Xeon Phi Debugging  Memory Debugging with MemoryScape  Events and memory errors (TP)  Delayed Scripted debugging (non-interactive) (TP)  Reverse debugging using ReplayEngine (TP)  Asynchronous control of Parallel Applications (TP)  Type Transformation (TP)   While TotalView is available on the  UL HPC Platform , the specific exercises proposed have been embedded into a Virtual Machine (VM) you will need to setup to run this practical session.",
            "title": "UL HPC Tutorial: Direct, Reverse and parallel Memory debugging with TotalView"
        },
        {
            "location": "/advanced/TotalView/#pre-requisites",
            "text": "Install  Oracle's VirtualBox  Download the bootable ISO  tv_trainingcdU10_20140815.iso  (md5sum:  30a844ddda80ddf505c28eb4f4b6f1bf ) which contains:  a bootable Linux Ubuntu distribition  a version of TotalView  the PDF documents describing the practical session.   If the participants did not have the time to download the ISO, they shall come with a USB stick having a capacity of at least 2GB.  To create a new VM:   Open VirtualBox  Select the  New  icon  Name:  TotalView  Type:  Linux  Version:  Ubuntu 64 bits  Memory Size:  512MB  Create a virtual hard drive now   Click on \"Create\": select a VDI format, dynamically allocated.\n* Now select the  Start  icon over the newly created VM\n  * open the folder icon to browse your disk and select the   tv_trainingcdU10_20140815.iso  ISO on which you'll boot  You're now ready for the tutorial.",
            "title": "Pre-Requisites"
        },
        {
            "location": "/advanced/TotalView/#tutorial",
            "text": "Content of the tutorial  Answers of the tutorial",
            "title": "Tutorial"
        },
        {
            "location": "/advanced/Vagrant/",
            "text": "-\n- mode: markdown; mode: visual-line; fill-column: 80 -\n-\n\n\nCopyright (c) 2014-2017 UL HPC Team  \nhpc-sysadmins@uni.lu\n\n\n\n\nUL HPC Tutorial: Create and reproduce work environments using Vagrant\n\n\n \n \n \n \n \n\n\n/!\\ IMPORTANT\n Up-to-date instructions for Vagrant can be found in the \n\"Reproducible Research at the Cloud Era\"\n Tutorial. Below instructions are probably outdated but kept for archive purposes.\n\n\n\n\nVagrant is a tool that allows to easily and rapidly create and configure reproducible and portable work environments using Virtual Machines. This is especially useful if you want to test your work in a stable and controlled environment and minimize the various unintended or untrackable changes that may occur on a physical machine.\n\n\nIn this tutorial, we are going to explain the steps to install Vagrant and create your first basic Linux Virtual Machine with it.\n\n\nVagrant installation\n\n\nPrerequisite:\n\n\nVagrant can use many Virtual Machine providers such as \nVirtualBox\n, \nVMware\n and \nDocker\n with VirtualBox being the easiest to use, and the default option in Vagrant.\n\n\nOur first step is to install VirtualBox, you can download and install the correct version for your operating system from the \nofficial website\n. In many Linux distributions it is provided as a package from the standard repositories, thus you can use your usual package manager to install it.\n\n\nOnce this prerequisite is met, we can install Vagrant. Download the correct version for your operating system on the \nofficial website\n and install it.\n\n\nUsing Vagrant to create a Virtual Machine\n\n\nThe main advantage of Vagrant is that it lets you import and use pre-configured Virtual Machines (called \nboxes\n in this context) which can become bases for your own customizations (installed applications, libraries, etc). With Vagrant it becomes really fast and effortless to create and run a new Virtual Machine.\n\n\nThe Vagrant boxes contain the disk image of a VM without the virtual hardware details of the VM, which are initialized by Vagrant and can be edited by the user.\n\n\nThe first step is to choose a pre-configured box to use. It is possible to create your own from scratch yet this is not in the scope of the current tutorial.\nFreely available boxes can be found at the following two main sources:\n\n\n\n\nAtlas corps box catalog\n\n\nvagrantbox.es catalaog\n\n\n\n\nThe first catalog is the default box download location for Vagrant. This means that you can directly use the name of the boxes you find here with Vagrant (e.g. \nubuntu/trusty64\n).\nTo use the second catalog you would additionaly need to provide the source box URL, yet this catalog provides a much richer variety of boxes.\n\n\nAdding a new box\n\n\nTo add a box and make it usable in Vagrant, we are going to use the \nvagrant box add\n command. In the example below we will add one box from each of the catalogs in order to present the different possibilities.\nWe are going to add the \nubuntu/trusty64\n box from the Atlas catalog and the \nUbuntu 14.04\n box (by its \nurl\n) from the vagrantbox.es catalog.\n\n\nTo add the first box, we use the following command (which may take some time due to the time needed to download the box):\n\n\n    $> vagrant box add ubuntu/trusty64\n    ==> box: Loading metadata for box 'ubuntu/trusty64'\n        box: URL: https://vagrantcloud.com/ubuntu/trusty64\n    ==> box: Adding box 'ubuntu/trusty64' (v14.04) for provider: virtualbox\n        box: Downloading: https://vagrantcloud.com/ubuntu/boxes/trusty64/versions/14.04/providers/virtualbox.box\n    ==> box: Successfully added box 'ubuntu/trusty64' (v14.04) for 'virtualbox'!\n\n\n\nIn this case, you just had to give the name of the box and Vagrant found the box by itself and added the box under the \nubuntu/trusty64\n name.\n\n\nTo list the local boxes available to Vagrant for initialization of new VMs, we use the \nvagrant box list\n command:\n\n\n    $> vagrant box list\n    ubuntu/trusty64    (virtualbox, 14.04)\n\n\n\nTo add the second box, you need to use a slightly different syntax since you need to precise the name you want to give to the box as well as its source URL:\n\n\n    $> vagrant box add ubuntu14.04 https://github.com/kraksoft/vagrant-box-ubuntu/releases/download/14.04/ubuntu-14.04-amd64.box\n    ==> box: Adding box 'ubuntu14.04' (v0) for provider:\n        box: Downloading: https://github.com/kraksoft/vagrant-box-ubuntu/releases/download/14.04/ubuntu-14.04-amd64.box\n    ==> box: Successfully added box 'ubuntu14.04' (v0) for 'virtualbox'!\n\n\n\nNow a second box will be available to Vagrant under the name \nubuntu14.04\n:\n\n\n    $> vagrant box list\n    ubuntu/trusty64    (virtualbox, 14.04)\n    ubuntu14.04        (virtualbox, 0)\n\n\n\nIn the rest of the tutorial we are only going to use the first box. To remove a box we use the \nvagrant box remove\n command as follows:\n\n\n    $> vagrant box remove ubuntu14.04\n    Removing box 'ubuntu14.04' (v0) with provider 'virtualbox'...\n\n\n\nChecking that it has been removed:\n\n\n    $> vagran box list\n    ubuntu/trusty64    (virtualbox, 14.04)\n\n\n\nCreating a new Virtual Machine\n\n\nNow we are going to create a new Virtual Machine using the \nubuntu/trusty64\n box.\nWe will initialize it in an empty directory (which is not absolutely mandatory):\n\n\n    $> mkdir vagrant && cd vagrant\n\n\n\nNext, we make Vagrant prepare the configuration file describing the VM:\n\n\n    $> vagrant init ubuntu/trusty64\n    A `Vagrantfile` has been placed in this directory. You are now\n    ready to `vagrant up` your first virtual environment! Please read\n    the comments in the Vagrantfile as well as documentation on\n    `vagrantup.com` for more information on using Vagrant.\n\n\n\nYou should now see a file named \nVagrantfile\n in your directory. This file contains the minimal information for Vagrant to launch the VM. We could modify it to set up specific parameters of the VM (number of virtual cores, memory size, etc), but this constitutes advanced usage for which full documentation that can be found on the \nofficial site\n. However, it may be interesting to understand what is actually needed in this file, since it contains a lot of commented information.\nThe minimal content of a \nVagrantfile\n is as follows:\n\n\n    VAGRANTFILE_API_VERSION = \"2\"\n    Vagrant.configure(\"VAGRANTFILE_API_VERSION\") do |config|\n        config.vm.box = \"hashicorp/trusty64\"\n    end\n\n\n\nThis basically defines which version of the Vagrant API will be used to build the VM using the box given as a base.\n\n\nNow, to launch the VM you only need to use the single \nvagrant up\n command in the same directory where the \nVagrantfile\n exists (this may take some time since Vagrant is going to boot the VM and set its basic configuration):\n\n\n    $> vagrant up\n    Bringing machine 'default' up with 'virtualbox' provider...\n    ==> default: Importing base box 'ubuntu/trusty64'...\n    ==> default: Matching MAC address for NAT networking...\n    ==> default: Checking if box 'ubuntu/trusty64' is up to date...\n    ==> default: Setting the name of the VM: vagrant_default_1425476252413_67101\n    ==> default: Clearing any previously set forwarded ports...\n    ==> default: Clearing any previously set network interfaces...\n    ==> default: Preparing network interfaces based on configuration...\n        default: Adapter 1: nat\n    ==> default: Forwarding ports...\n        default: 22 => 2222 (adapter 1)\n    ==> default: Booting VM...\n    ==> default: Waiting for machine to boot. This may take a few minutes...\n        default: SSH address: 127.0.0.1:2222\n        default: SSH username: vagrant\n        default: SSH auth method: private key\n        default: Warning: Connection timeout. Retrying...\n        default: Warning: Remote connection disconnect. Retrying...\n    ==> default: Machine booted and ready!\n    ==> default: Checking for guest additions in VM...\n    ==> default: Mounting shared folders...\n        default: /vagrant => /tmp/vagrant\n\n\n\nYour VM is now up and running at this point. To access it, use the \nvagrant ssh\n command within the same directory :\n\n\n    $> vagrant ssh\n\n\n\nYou should now be connected to your VM and ready to work.\n\n\nAn interesting feature of Vagrant is that your computer (the \"host\") shares the directory that contains the \nVagrantfile\n with your VM (the \"guest\"), where it is seen as \n/vagrant\n.\n\n\nAssuming you have a script or data files you want to access from within the VM, you simply put them in the same directory as the \nVagrantfile\n and then use them in the VM under \n/vagrant\n. The reverse is also true.\n\n\nTo learn more than the basics covered in this tutorial, we encourage you to refer to the \nofficial documentation\n.",
            "title": "Vagrant"
        },
        {
            "location": "/advanced/Vagrant/#ul-hpc-tutorial-create-and-reproduce-work-environments-using-vagrant",
            "text": "/!\\ IMPORTANT  Up-to-date instructions for Vagrant can be found in the  \"Reproducible Research at the Cloud Era\"  Tutorial. Below instructions are probably outdated but kept for archive purposes.   Vagrant is a tool that allows to easily and rapidly create and configure reproducible and portable work environments using Virtual Machines. This is especially useful if you want to test your work in a stable and controlled environment and minimize the various unintended or untrackable changes that may occur on a physical machine.  In this tutorial, we are going to explain the steps to install Vagrant and create your first basic Linux Virtual Machine with it.",
            "title": "UL HPC Tutorial: Create and reproduce work environments using Vagrant"
        },
        {
            "location": "/advanced/Vagrant/#vagrant-installation",
            "text": "Prerequisite:  Vagrant can use many Virtual Machine providers such as  VirtualBox ,  VMware  and  Docker  with VirtualBox being the easiest to use, and the default option in Vagrant.  Our first step is to install VirtualBox, you can download and install the correct version for your operating system from the  official website . In many Linux distributions it is provided as a package from the standard repositories, thus you can use your usual package manager to install it.  Once this prerequisite is met, we can install Vagrant. Download the correct version for your operating system on the  official website  and install it.",
            "title": "Vagrant installation"
        },
        {
            "location": "/advanced/Vagrant/#using-vagrant-to-create-a-virtual-machine",
            "text": "The main advantage of Vagrant is that it lets you import and use pre-configured Virtual Machines (called  boxes  in this context) which can become bases for your own customizations (installed applications, libraries, etc). With Vagrant it becomes really fast and effortless to create and run a new Virtual Machine.  The Vagrant boxes contain the disk image of a VM without the virtual hardware details of the VM, which are initialized by Vagrant and can be edited by the user.  The first step is to choose a pre-configured box to use. It is possible to create your own from scratch yet this is not in the scope of the current tutorial.\nFreely available boxes can be found at the following two main sources:   Atlas corps box catalog  vagrantbox.es catalaog   The first catalog is the default box download location for Vagrant. This means that you can directly use the name of the boxes you find here with Vagrant (e.g.  ubuntu/trusty64 ).\nTo use the second catalog you would additionaly need to provide the source box URL, yet this catalog provides a much richer variety of boxes.",
            "title": "Using Vagrant to create a Virtual Machine"
        },
        {
            "location": "/advanced/Vagrant/#adding-a-new-box",
            "text": "To add a box and make it usable in Vagrant, we are going to use the  vagrant box add  command. In the example below we will add one box from each of the catalogs in order to present the different possibilities.\nWe are going to add the  ubuntu/trusty64  box from the Atlas catalog and the  Ubuntu 14.04  box (by its  url ) from the vagrantbox.es catalog.  To add the first box, we use the following command (which may take some time due to the time needed to download the box):      $> vagrant box add ubuntu/trusty64\n    ==> box: Loading metadata for box 'ubuntu/trusty64'\n        box: URL: https://vagrantcloud.com/ubuntu/trusty64\n    ==> box: Adding box 'ubuntu/trusty64' (v14.04) for provider: virtualbox\n        box: Downloading: https://vagrantcloud.com/ubuntu/boxes/trusty64/versions/14.04/providers/virtualbox.box\n    ==> box: Successfully added box 'ubuntu/trusty64' (v14.04) for 'virtualbox'!  In this case, you just had to give the name of the box and Vagrant found the box by itself and added the box under the  ubuntu/trusty64  name.  To list the local boxes available to Vagrant for initialization of new VMs, we use the  vagrant box list  command:      $> vagrant box list\n    ubuntu/trusty64    (virtualbox, 14.04)  To add the second box, you need to use a slightly different syntax since you need to precise the name you want to give to the box as well as its source URL:      $> vagrant box add ubuntu14.04 https://github.com/kraksoft/vagrant-box-ubuntu/releases/download/14.04/ubuntu-14.04-amd64.box\n    ==> box: Adding box 'ubuntu14.04' (v0) for provider:\n        box: Downloading: https://github.com/kraksoft/vagrant-box-ubuntu/releases/download/14.04/ubuntu-14.04-amd64.box\n    ==> box: Successfully added box 'ubuntu14.04' (v0) for 'virtualbox'!  Now a second box will be available to Vagrant under the name  ubuntu14.04 :      $> vagrant box list\n    ubuntu/trusty64    (virtualbox, 14.04)\n    ubuntu14.04        (virtualbox, 0)  In the rest of the tutorial we are only going to use the first box. To remove a box we use the  vagrant box remove  command as follows:      $> vagrant box remove ubuntu14.04\n    Removing box 'ubuntu14.04' (v0) with provider 'virtualbox'...  Checking that it has been removed:      $> vagran box list\n    ubuntu/trusty64    (virtualbox, 14.04)",
            "title": "Adding a new box"
        },
        {
            "location": "/advanced/Vagrant/#creating-a-new-virtual-machine",
            "text": "Now we are going to create a new Virtual Machine using the  ubuntu/trusty64  box.\nWe will initialize it in an empty directory (which is not absolutely mandatory):      $> mkdir vagrant && cd vagrant  Next, we make Vagrant prepare the configuration file describing the VM:      $> vagrant init ubuntu/trusty64\n    A `Vagrantfile` has been placed in this directory. You are now\n    ready to `vagrant up` your first virtual environment! Please read\n    the comments in the Vagrantfile as well as documentation on\n    `vagrantup.com` for more information on using Vagrant.  You should now see a file named  Vagrantfile  in your directory. This file contains the minimal information for Vagrant to launch the VM. We could modify it to set up specific parameters of the VM (number of virtual cores, memory size, etc), but this constitutes advanced usage for which full documentation that can be found on the  official site . However, it may be interesting to understand what is actually needed in this file, since it contains a lot of commented information.\nThe minimal content of a  Vagrantfile  is as follows:      VAGRANTFILE_API_VERSION = \"2\"\n    Vagrant.configure(\"VAGRANTFILE_API_VERSION\") do |config|\n        config.vm.box = \"hashicorp/trusty64\"\n    end  This basically defines which version of the Vagrant API will be used to build the VM using the box given as a base.  Now, to launch the VM you only need to use the single  vagrant up  command in the same directory where the  Vagrantfile  exists (this may take some time since Vagrant is going to boot the VM and set its basic configuration):      $> vagrant up\n    Bringing machine 'default' up with 'virtualbox' provider...\n    ==> default: Importing base box 'ubuntu/trusty64'...\n    ==> default: Matching MAC address for NAT networking...\n    ==> default: Checking if box 'ubuntu/trusty64' is up to date...\n    ==> default: Setting the name of the VM: vagrant_default_1425476252413_67101\n    ==> default: Clearing any previously set forwarded ports...\n    ==> default: Clearing any previously set network interfaces...\n    ==> default: Preparing network interfaces based on configuration...\n        default: Adapter 1: nat\n    ==> default: Forwarding ports...\n        default: 22 => 2222 (adapter 1)\n    ==> default: Booting VM...\n    ==> default: Waiting for machine to boot. This may take a few minutes...\n        default: SSH address: 127.0.0.1:2222\n        default: SSH username: vagrant\n        default: SSH auth method: private key\n        default: Warning: Connection timeout. Retrying...\n        default: Warning: Remote connection disconnect. Retrying...\n    ==> default: Machine booted and ready!\n    ==> default: Checking for guest additions in VM...\n    ==> default: Mounting shared folders...\n        default: /vagrant => /tmp/vagrant  Your VM is now up and running at this point. To access it, use the  vagrant ssh  command within the same directory :      $> vagrant ssh  You should now be connected to your VM and ready to work.  An interesting feature of Vagrant is that your computer (the \"host\") shares the directory that contains the  Vagrantfile  with your VM (the \"guest\"), where it is seen as  /vagrant .  Assuming you have a script or data files you want to access from within the VM, you simply put them in the same directory as the  Vagrantfile  and then use them in the VM under  /vagrant . The reverse is also true.  To learn more than the basics covered in this tutorial, we encourage you to refer to the  official documentation .",
            "title": "Creating a new Virtual Machine"
        },
        {
            "location": "/advanced/vm5k/",
            "text": "-\n- mode: markdown; mode: visual-line; fill-column: 80 -\n-\n\n\nCopyright (c) 2015 Hyacinthe Cartiaux \nHyacinthe.Cartiaux@uni.lu\nhpc-sysadmins@uni.lu\n\n\n\n\nDeploying virtual machines with Vm5k on Grid'5000\n\n\n \n \n \n \n\n\nGrid\u20195000 is a scientific instrument distributed in 10 sites (mainly in France)\nfor research in large-scale parallel and distributed systems. It aims at providing\na highly reconfigurable, controllable and monitorable experimental platform to its users.\n\n\nThe infrastructure has reached 1035 nodes and 7782 cores and all sites are connected\nto RENATER with a 10 Gb/s link, except Reims and Nantes (1 Gb/s).\n\n\n\n\nThe objectives of this tutorial are:\n\n\n\n\nconnect to Grid'5000\n\n\ndiscover the basic features of Grid'5000\n\n\nuse vm5k in order to deploy virtual machines on the grid\n\n\n\n\nGetting started\n\n\nUser charter\n\n\nYou should first read the \nUser Charter\n.\nThe mains points are:\n\n\n\n\nmaintain your \nuser reports\n up-to-date (publications, experiments, etc)\n\n\nduring working days and office hours (09:00 to 19:00), you should not use more than the equivalent of 2 hours of all the resources available in a cluster\n\n\nyour jobs should not cross the 09:00 and 19:00 boundaries during week days\n\n\nyou should not have more than 2 reservations in advance\n\n\n\n\nBasically: develop your experiments during day time, launch them over the nights and week-ends\n\n\nAccount\n\n\nFill the \naccount request form\n\nAt the Manager entry where you\u2019re asked the Grid\u20195000 login of the person\nwho\u2019s responsible of the account you\u2019re requesting, answer \nsvarrett\n\n\nConnection\n\n\nGrid'5000 provided 2 national access servers, located in Lille and Sophia.\nThese servers allow the user to connect the site's frontend.\n\n\n(user)   ssh <login>@access.grid5000.fr\n(access) ssh luxembourg\n\n\n\nAs an alternative, we can connect directly to the Luxembourg frontend from within the UL network:\n\n\n(user)   ssh <login>@grid5000.uni.lu\n\n\n\nReservation and deployment\n\n\nGrid'5000 uses OAR, all the oar commands you use on Gaia and Chaos clusters are valid.\n\n\n\n\nOAR documentation on hpc.uni.lu\n\n\n\n\nAdditionally to the computing nodes, G5K provides more resources:\n\n\n\n\nsubnets (ranges of IP for virtualization / cloud experiments)\n\n\nvlan (reconfigure the network equipments)\n\n\nstorage (iscsi / nfs)\n\n\n...\n\n\n\n\nThe job type \"deploy\" is also supported, which means that you can use kadeploy to reinstall\na cluster node and gain root access during the time of your jobs\n\n\nTutorials\n\n\nIt is highly recommended to read and follow the \nGetting Started tutorial\n,\nand all the others tutorials available in the \nUser Portal\n\n\nVM5K\n\n\nVm5k\n is a tool used to deploy a large number of virtual machines on the Grid\u20185000 platform.\n\n\nIn short, Vm5k\n\n\n\n\nmanages the reservation, locally if you work on one site, or globally with \noargridsub\n\n\ninstall the hosts with kadeploy, and configure the virtualization stack for you\n\n\nconfigure the network (bridges)\n\n\ndeploy the virtual machines\n\n\n\n\nInstallation (from git)\n\n\nYou will install VM5K in your home directory.\n\n\n\n\n\n\nSpecify the proxy configuration\n\n\n(frontend) export http_proxy=\"http://proxy:3128\"\n(frontend) export https_proxy=\"https://proxy:3128\"\n\n\n\n\n\n\n\nInstall execo, which is a dependency of vm5k\n\n\n(frontend) easy_install --user execo\n\n\n\n\n\n\n\nClone the git repository of vm5k and install it\n\n\n(frontend) git clone https://github.com/lpouillo/vm5k.git\n(frontend) cd vm5k\n(frontend) python setup.py  install --user\n\n\n\n\n\n\n\nIn your bashrc file, add the ~/.local/bin directory to the PATH environment variable\n\n\n(frontend) echo 'export PATH=$PATH:~/.local/bin' >> ~/.bashrc\n\n\n\n\n\n\n\nUsage\n\n\nBasic features\n\n\nEach deployment takes around 20 minutes, so you don't have to execute all the following examples:\n\n\n\n\nspawn 20 VMs on the granduc cluster, with a walltime of 30 minutes, and write the output files in the directory \nvm5k_test\n:\n(frontend) vm5k --n_vm 10 -r granduc -w 0:30:00 -o vm5k_test\n\n\n\n\n\n\n\nYou'll find the list of VMs with their ips in the file vm5k_test/vms.list\n\n\n10.172.1.45     vm-1\n10.172.1.46     vm-2\n10.172.1.47     vm-3\n10.172.1.48     vm-4\n10.172.1.49     vm-5\n10.172.1.50     vm-6\n10.172.1.51     vm-7\n10.172.1.52     vm-8\n...\n\n\n\n\n\n\n\nLet's spawn 100 VMs on 2 hosts in Nancy and Luxembourg\n\n\n(frontend) vm5k --n_vm 10 -r nancy:1,luxembourg:1 -w 0:30:00 -o vm5k_test\n\n\n\n\n\n\n\nWe can also specify the VM template (resources) with the parameter \n--vm_template\n\n\n(frontend) vm5k --n_vm 10 -r nancy:2,luxembourg:2 -w 0:30:00 -o vm5k_test --vm_template '<vm mem=\"4096\" hdd=\"10\" n_cpu=\"4\" cpuset=\"auto\"/>'\n\n\n\n\n\n\n\nDistribution\n\n\n\n\n\n\nBalance the nodes on all the reserved hosts\n\n\n(frontend) vm5k --n_vm 100 -r granduc:4 -o vm5k_test -d n_by_hosts\n\n\n\n\n\n\n\nConcentrate the VMs on a minimal number of hosts\n\n\n(frontend) vm5k -r grid5000:20 -n 100 -o vm5k_test -d concentrated\n\n\n\n\n\n\n\nAdvanced feature: define the deployment topology\n\n\nYou can control the deployment topology, and specify finely the clusters, nodes and virtual machines per node.\n\n\nCreate a file named \ntopology.xml\n, and change the sites, cluster and host id as needed:\n\n\n<vm5k>\n  <site id=\"luxembourg\">\n    <cluster id=\"granduc\">\n      <host id=\"granduc-2\">\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-33\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-34\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-35\" cpu=\"1\"/>\n      </host>\n      <host id=\"granduc-3\">\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-43\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-44\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-45\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-43\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-44\" cpu=\"1\"/>\n      </host>\n    </cluster>\n  </site>\n  <site id=\"nancy\">\n    <cluster id=\"graphene\">\n      <host id=\"graphene-30\">\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-30\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-31\" cpu=\"1\"/>\n      </host>\n    </cluster>\n  </site>\n</vm5k>\n\n\n\nGive this file to vm5k\n\n\n(frontend) vm5k -i topology.xml -w 0:30:0 -o vm5k_test\n\n\n\nExperiment\n\n\nWe will deploy 100 VMs and deploy the munin monitoring software.\nThis is an example, munin will allow you to monitor the activity on all the VMs.\n\n\nInstall munin clients on all the other servers\n\n\n\n\n\n\nSpawn 100 VMs\n\n\n(frontend) vm5k --n_vm 50 -w 2:00:00 -r luxembourg -o hpcschool2015 -o vm5k_xp\n\n\n\n\n\n\n\nLaunch a script on all the VMs after their deployment, we will use \ntaktuk\n (you could also clush, pdsh, etc)\n\n\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ apt-get update ]\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ apt-get install -y munin-node stress ]\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ 'echo cidr_allow 10.0.0.0/8 >> /etc/munin/munin-node.conf' ]\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ '/etc/init.d/munin-node restart' ]\n\n\n\n\n\n\n\nInstall munin server on the first physical host\n\n\n\n\n\n\nChoose the first virtual machines\n\n\n(frontend) head -n 1 vm5k_xp/vms.list\n10.172.1.45     vm-1\n\n\n\n\n\n\n\nTransfer the list of virtual machines to the VM\n\n\n(frontend) scp vm5k_xp/vms.list root@10.172.1.45:/tmp/\n\n\n\n\n\n\n\nConnect to the VM in order to install and configure munin\n\n\n(frontend) ssh root@10.172.1.45\n\n(vm-1) apt-get install munin apache2\n\n\n\n\n\n\n\nConfigure the Apache http server\n\n\n(vm-1) sed -i '/[aA]llow/d' /etc/apache2/conf.d/munin\n(vm-1) apache2ctl restart\n\n\n\n\n\n\n\nGenerate the munin configuration\n\n\n(vm-1) cat /tmp/vms.list  | awk '{print \"[\"$2\".g5k]\\n    address \"$1\"\\n    use_node_name yes\\n\"}' >> /etc/munin/munin.conf\n(vm-1) /etc/init.d/munin restart\n\n\n\n\n\n\n\nConnect to munin\n\n\n\n\n\n\nLet's generate a fake activity, stress the VM during 60 seconds\n\n\n(frontend) taktuk -l root -f hpcschool2015/vms.list broadcast exec [ 'stress -c 1 -t 60' ]\n\n\n\n\n\n\n\nOpen a ssh tunnel on port 80\n\n\n(user) ssh -L1080:10.172.1.45:80 <login>@grid5000.uni.lu\n\n\n\n\n\n\n\nOpen a browser and navigates to \nhttp://localhost:1080",
            "title": "Grid5000 - Automatic VM deployment with VM5K"
        },
        {
            "location": "/advanced/vm5k/#deploying-virtual-machines-with-vm5k-on-grid5000",
            "text": "Grid\u20195000 is a scientific instrument distributed in 10 sites (mainly in France)\nfor research in large-scale parallel and distributed systems. It aims at providing\na highly reconfigurable, controllable and monitorable experimental platform to its users.  The infrastructure has reached 1035 nodes and 7782 cores and all sites are connected\nto RENATER with a 10 Gb/s link, except Reims and Nantes (1 Gb/s).   The objectives of this tutorial are:   connect to Grid'5000  discover the basic features of Grid'5000  use vm5k in order to deploy virtual machines on the grid",
            "title": "Deploying virtual machines with Vm5k on Grid'5000"
        },
        {
            "location": "/advanced/vm5k/#getting-started",
            "text": "",
            "title": "Getting started"
        },
        {
            "location": "/advanced/vm5k/#user-charter",
            "text": "You should first read the  User Charter .\nThe mains points are:   maintain your  user reports  up-to-date (publications, experiments, etc)  during working days and office hours (09:00 to 19:00), you should not use more than the equivalent of 2 hours of all the resources available in a cluster  your jobs should not cross the 09:00 and 19:00 boundaries during week days  you should not have more than 2 reservations in advance   Basically: develop your experiments during day time, launch them over the nights and week-ends",
            "title": "User charter"
        },
        {
            "location": "/advanced/vm5k/#account",
            "text": "Fill the  account request form \nAt the Manager entry where you\u2019re asked the Grid\u20195000 login of the person\nwho\u2019s responsible of the account you\u2019re requesting, answer  svarrett",
            "title": "Account"
        },
        {
            "location": "/advanced/vm5k/#connection",
            "text": "Grid'5000 provided 2 national access servers, located in Lille and Sophia.\nThese servers allow the user to connect the site's frontend.  (user)   ssh <login>@access.grid5000.fr\n(access) ssh luxembourg  As an alternative, we can connect directly to the Luxembourg frontend from within the UL network:  (user)   ssh <login>@grid5000.uni.lu",
            "title": "Connection"
        },
        {
            "location": "/advanced/vm5k/#reservation-and-deployment",
            "text": "Grid'5000 uses OAR, all the oar commands you use on Gaia and Chaos clusters are valid.   OAR documentation on hpc.uni.lu   Additionally to the computing nodes, G5K provides more resources:   subnets (ranges of IP for virtualization / cloud experiments)  vlan (reconfigure the network equipments)  storage (iscsi / nfs)  ...   The job type \"deploy\" is also supported, which means that you can use kadeploy to reinstall\na cluster node and gain root access during the time of your jobs",
            "title": "Reservation and deployment"
        },
        {
            "location": "/advanced/vm5k/#tutorials",
            "text": "It is highly recommended to read and follow the  Getting Started tutorial ,\nand all the others tutorials available in the  User Portal",
            "title": "Tutorials"
        },
        {
            "location": "/advanced/vm5k/#vm5k",
            "text": "Vm5k  is a tool used to deploy a large number of virtual machines on the Grid\u20185000 platform.  In short, Vm5k   manages the reservation, locally if you work on one site, or globally with  oargridsub  install the hosts with kadeploy, and configure the virtualization stack for you  configure the network (bridges)  deploy the virtual machines",
            "title": "VM5K"
        },
        {
            "location": "/advanced/vm5k/#installation-from-git",
            "text": "You will install VM5K in your home directory.    Specify the proxy configuration  (frontend) export http_proxy=\"http://proxy:3128\"\n(frontend) export https_proxy=\"https://proxy:3128\"    Install execo, which is a dependency of vm5k  (frontend) easy_install --user execo    Clone the git repository of vm5k and install it  (frontend) git clone https://github.com/lpouillo/vm5k.git\n(frontend) cd vm5k\n(frontend) python setup.py  install --user    In your bashrc file, add the ~/.local/bin directory to the PATH environment variable  (frontend) echo 'export PATH=$PATH:~/.local/bin' >> ~/.bashrc",
            "title": "Installation (from git)"
        },
        {
            "location": "/advanced/vm5k/#usage",
            "text": "",
            "title": "Usage"
        },
        {
            "location": "/advanced/vm5k/#basic-features",
            "text": "Each deployment takes around 20 minutes, so you don't have to execute all the following examples:   spawn 20 VMs on the granduc cluster, with a walltime of 30 minutes, and write the output files in the directory  vm5k_test : (frontend) vm5k --n_vm 10 -r granduc -w 0:30:00 -o vm5k_test    You'll find the list of VMs with their ips in the file vm5k_test/vms.list  10.172.1.45     vm-1\n10.172.1.46     vm-2\n10.172.1.47     vm-3\n10.172.1.48     vm-4\n10.172.1.49     vm-5\n10.172.1.50     vm-6\n10.172.1.51     vm-7\n10.172.1.52     vm-8\n...    Let's spawn 100 VMs on 2 hosts in Nancy and Luxembourg  (frontend) vm5k --n_vm 10 -r nancy:1,luxembourg:1 -w 0:30:00 -o vm5k_test    We can also specify the VM template (resources) with the parameter  --vm_template  (frontend) vm5k --n_vm 10 -r nancy:2,luxembourg:2 -w 0:30:00 -o vm5k_test --vm_template '<vm mem=\"4096\" hdd=\"10\" n_cpu=\"4\" cpuset=\"auto\"/>'",
            "title": "Basic features"
        },
        {
            "location": "/advanced/vm5k/#distribution",
            "text": "Balance the nodes on all the reserved hosts  (frontend) vm5k --n_vm 100 -r granduc:4 -o vm5k_test -d n_by_hosts    Concentrate the VMs on a minimal number of hosts  (frontend) vm5k -r grid5000:20 -n 100 -o vm5k_test -d concentrated",
            "title": "Distribution"
        },
        {
            "location": "/advanced/vm5k/#advanced-feature-define-the-deployment-topology",
            "text": "You can control the deployment topology, and specify finely the clusters, nodes and virtual machines per node.  Create a file named  topology.xml , and change the sites, cluster and host id as needed:  <vm5k>\n  <site id=\"luxembourg\">\n    <cluster id=\"granduc\">\n      <host id=\"granduc-2\">\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-33\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-34\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-35\" cpu=\"1\"/>\n      </host>\n      <host id=\"granduc-3\">\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-43\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-44\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-45\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-43\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-44\" cpu=\"1\"/>\n      </host>\n    </cluster>\n  </site>\n  <site id=\"nancy\">\n    <cluster id=\"graphene\">\n      <host id=\"graphene-30\">\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-30\" cpu=\"1\"/>\n        <vm mem=\"2048\" hdd=\"4\" id=\"vm-31\" cpu=\"1\"/>\n      </host>\n    </cluster>\n  </site>\n</vm5k>  Give this file to vm5k  (frontend) vm5k -i topology.xml -w 0:30:0 -o vm5k_test",
            "title": "Advanced feature: define the deployment topology"
        },
        {
            "location": "/advanced/vm5k/#experiment",
            "text": "We will deploy 100 VMs and deploy the munin monitoring software.\nThis is an example, munin will allow you to monitor the activity on all the VMs.",
            "title": "Experiment"
        },
        {
            "location": "/advanced/vm5k/#install-munin-clients-on-all-the-other-servers",
            "text": "Spawn 100 VMs  (frontend) vm5k --n_vm 50 -w 2:00:00 -r luxembourg -o hpcschool2015 -o vm5k_xp    Launch a script on all the VMs after their deployment, we will use  taktuk  (you could also clush, pdsh, etc)  (frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ apt-get update ]\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ apt-get install -y munin-node stress ]\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ 'echo cidr_allow 10.0.0.0/8 >> /etc/munin/munin-node.conf' ]\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ '/etc/init.d/munin-node restart' ]",
            "title": "Install munin clients on all the other servers"
        },
        {
            "location": "/advanced/vm5k/#install-munin-server-on-the-first-physical-host",
            "text": "Choose the first virtual machines  (frontend) head -n 1 vm5k_xp/vms.list\n10.172.1.45     vm-1    Transfer the list of virtual machines to the VM  (frontend) scp vm5k_xp/vms.list root@10.172.1.45:/tmp/    Connect to the VM in order to install and configure munin  (frontend) ssh root@10.172.1.45\n\n(vm-1) apt-get install munin apache2    Configure the Apache http server  (vm-1) sed -i '/[aA]llow/d' /etc/apache2/conf.d/munin\n(vm-1) apache2ctl restart    Generate the munin configuration  (vm-1) cat /tmp/vms.list  | awk '{print \"[\"$2\".g5k]\\n    address \"$1\"\\n    use_node_name yes\\n\"}' >> /etc/munin/munin.conf\n(vm-1) /etc/init.d/munin restart",
            "title": "Install munin server on the first physical host"
        },
        {
            "location": "/advanced/vm5k/#connect-to-munin",
            "text": "Let's generate a fake activity, stress the VM during 60 seconds  (frontend) taktuk -l root -f hpcschool2015/vms.list broadcast exec [ 'stress -c 1 -t 60' ]    Open a ssh tunnel on port 80  (user) ssh -L1080:10.172.1.45:80 <login>@grid5000.uni.lu    Open a browser and navigates to  http://localhost:1080",
            "title": "Connect to munin"
        },
        {
            "location": "/advanced/MultiPhysics/",
            "text": "-\n- mode: markdown; mode: visual-line; fill-column: 80 -\n-\n\n\nCopyright (c) 2015-2017 UL HPC Team  \nhpc-sysadmins@uni.lu\n\n\n\n\nUL HPC MPI Tutorial: Running parallel software: test cases on CFD / MD / Chemistry applications\n\n\n \n \n \n \n \n\n\nThe objective of this session is to exemplify the execution of several common, parallel, Computational Fluid Dynamics, Molecular Dynamics and Chemistry software on the \nUL HPC\n platform.\n\n\nTargeted applications include:\n\n\n\n\nOpenFOAM\n: CFD package for solving complex fluid flows involving chemical reactions, turbulence and heat transfer\n\n\nNAMD\n: parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems\n\n\nASE\n: Atomistic Simulation Environment (Python-based) with the aim of setting up, steering, and analyzing atomistic simulations\n\n\nABINIT\n: materials science package implementing DFT, DFPT, MBPT and TDDFT\n\n\nQuantum Espresso\n: integrated suite of tools for electronic-structure calculations and materials modeling at the nanoscale\n\n\n\n\nThe tutorial will cover:\n\n\n\n\nBasics for parallel execution with OAR and SLURM\n\n\ndifferent MPI suites available on UL HPC\n\n\nrunning simple test cases in parallel\n\n\nrunning QuantumEspresso in parallel over a single node and over multiple nodes\n\n\nrunning OpenFOAM in parallel over a single node and over multiple nodes\n\n\nrunning ABINIT in parallel over a single node and over multiple nodes\n\n\nthe interesting case of the ASE toolkit\n\n\n\n\nPrerequisites\n\n\nAs part of this tutorial several input files will be required and you will need to download them\nbefore following the instructions in the next sections:\n\n\nOr simply clone the full tutorials repository and make a link to this tutorial\n\n\n    (gaia-frontend)$> git clone https://github.com/ULHPC/tutorials.git\n    (gaia-frontend)$> ln -s tutorials/advanced/MultiPhysics/ ~/multiphysics-tutorial\n\n\n\nBasics\n\n\nNote: you can check out either the instructions for the OAR scheduler (gaia and chaos clusters) or SLURM (for iris).\n\n\nOAR basics for parallel execution\n\n\nFirst of all, we will submit a job with 2 cores on each of 2 compute nodes for 1 hour.\nPlease note that the Gaia cluster can be replaced at any point in this tutorial with the Chaos cluster if not enough resources are immediately available.\n\n\n   (gaia-frontend)$> oarsub -I -l nodes=2/core=2,walltime=1\n   (node)$>\n\n\n\nThe OAR scheduler provides several environment variables once we are inside a job, check them out with\n\n\n   (node)$> env | grep OAR_\n\n\n\nWe are interested especially in the environment variable which points to the file containing the list of hostnames reserved for the job.\nThis variable is OAR_NODEFILE, yet there are several others pointing to the same (OAR_NODE_FILE, OAR_FILE_NODES and OAR_RESOURCE_FILE).\nLet's check its content:\n\n\n   (node)$> cat $OAR_NODEFILE\n\n\n\nTo get the number of cores available in the job, we can use the wordcount \nwc\n utility, in line counting mode:\n\n\n   (node)$> cat $OAR_NODEFILE | wc -l\n\n\n\n\n\nSLURM basics for parallel execution\n\n\nFirst of all, we will submit on the iris cluster an interactive job with 2 tasks on each of 2 compute nodes for 1 hour.\n\n\n   (iris-frontend)$> srun -p interactive --qos qos-interactive -N 2 --ntasks-per-node 2 --pty bash -i\n   (node)$>\n\n\n\nThe SLURM scheduler provides several environment variables once we are inside a job, check them out with\n\n\n   (node)$> env | grep SLURM_\n\n\n\nWe are interested especially in the environment variable which lists the compute nodes reserved for the job -- SLURM_NODELIST.\nLet's check its content:\n\n\n   (node)$> cat $SLURM_NODELIST\n\n\n\nTo get the total number of cores available in the job, we can use the wordcount \nwc\n utility, in line counting mode:\n\n\n   (node)$> srun hostname | wc -l\n\n\n\nTo get the number of cores available on the current compute node:\n\n\n   (node)$> echo $SLURM_CPUS_ON_NODE\n\n\n\n\n\nMPI suites available on the platform\n\n\nOn Gaia:\n\n\nNow, let's check for the environment modules (available through Lmod) which match MPI (Message Passing Interface) the libraries that provide inter-process communication over a network:\n\n\n   (node)$> module avail mpi/\n\n   ---------------------------------------------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/mpi ----------------------------------------------------\n      mpi/OpenMPI/1.6.4-GCC-4.7.2    mpi/OpenMPI/1.8.4-GCC-4.9.2 (D)    mpi/impi/4.1.0.030-iccifort-2013.3.163    mpi/impi/5.0.3.048-iccifort-2015.3.187 (D)\n\n   ------------------------------------------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/toolchain -------------------------------------------------\n      toolchain/gompi/1.4.10    toolchain/iimpi/5.3.0    toolchain/iimpi/7.3.5 (D)\n\n     Where:\n      (D):  Default Module\n\n   Use \"module spider\" to find all possible modules.\n   Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n\n\n\nPerform the same search for the toolchains:\n\n\n   (node)$> module avail toolchain/\n\n\n\nToolchains represent sets of compilers together with libraries commonly required to build software, such as MPI, BLAS/LAPACK (linear algebra) and FFT (Fast Fourier Transforms).\nFor more details, see \nthe EasyBuild page on toolchains\n.\n\n\nFor our initial tests we will use the \ngoolf\n toolchain which includes GCC, OpenMPI, OpenBLAS/LAPACK, ScaLAPACK(/BLACS) and FFTW:\n\n\n   (node)$> module load toolchain/goolf/1.4.10\n   (node)$> module list\n\n\n\nThe main alternative to this toolchain (as of June 2015) is \nictce\n (toolchain/ictce/7.3.5) that includes the Intel tools icc, ifort, impi and imkl.\n\n\nOn Iris:\n\n\n   (node)$> module avail mpi/\n\n   ----------------------------- /opt/apps/resif/data/stable/default/modules/all ----------------------------------------------------\n      mpi/MVAPICH2/2.2b-GCC-4.9.3-2.25    mpi/OpenMPI/2.1.1-GCC-6.3.0-2.28                       (D)    toolchain/gompi/2017a\n      mpi/OpenMPI/2.1.1-GCC-6.3.0-2.27    mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27        toolchain/iimpi/2017a\n\n     Where:\n      D:  Default Module\n\n   Use \"module spider\" to find all possible modules.\n   Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n\n\n  (node)$> module avail toolchain/\n\n\n\nOn iris the main two toolchains are \nfoss\n (GCC, OpenMPI, OpenBLAS, ScaLAPACK and FFTW) and \nintel\n (Intel C/C++/Fortran, MKL).\n\n\nSimple test cases on Gaia\n\n\nWe will now try to run the \nhostname\n application, which simply shows a system's host name.\nCheck out the differences (if any) between the following executions:\n\n\n   (node)$> hostname\n   (node)$> mpirun hostname\n   (node)$> mpirun -n 2 hostname\n   (node)$> mpirun -np 2 hostname\n   (node)$> mpirun -n 4 hostname\n   (node)$> mpirun -hostfile $OAR_NODEFILE hostname\n   (node)$> mpirun -hostfile $OAR_NODEFILE -n 2 hostname\n   (node)$> mpirun -hostfile $OAR_NODEFILE -n 3 hostname\n   (node)$> mpirun -hostfile $OAR_NODEFILE -npernode 1 hostname\n\n\n\nNote that the \nhostname\n application is \nnot\n a parallel application, with MPI we are simply launching it on the different nodes available in the job.\n\n\nNow, we will compile and run a simple \nhellompi\n MPI application. Save the following source code in /tmp/hellompi.c :\n\n\n   #include <mpi.h>\n   #include <stdio.h>\n   #include <stdlib.h>\n   int main(int argc, char** argv) {\n     MPI_Init(NULL, NULL);\n     int world_size;\n     MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n     int world_rank;\n     MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n     char processor_name[MPI_MAX_PROCESSOR_NAME];\n     int name_len;\n     MPI_Get_processor_name(processor_name, &name_len);\n     printf(\"Hello world from %s, rank %d out of %d CPUs\\n\", processor_name, world_rank, world_size);\n     MPI_Finalize();\n   }\n\n\n\nCompile it:\n\n\n   (node)$> cd /tmp\n   (node)$> mpicc -o hellompi hellompi.c\n\n\n\nRun it:\n\n\n   (node)$> mpirun -hostfile $OAR_NODEFILE ./hellompi\n\n\n\nWhy didn't it work? Remember we stored this application on a compute node's /tmp directory, which is local to each node, not shared.\nThus the application couldn't be found (more on this later) on the remote nodes.\n\n\nLet's move it to the \n$HOME\n directory which is common across the cluster, and try again:\n\n\n   (node)$> mv /tmp/hellompi ~/multiphysics-tutorial\n   (node)$> cd ~/multiphysics-tutorial\n   (node)$> mpirun -hostfile $OAR_NODEFILE ./hellompi\n\n\n\nNow some different error messages are shown, about loading shared libraries, and the execution hangs (stop it with Ctrl-C). Why?\nRemember that on the current node we have loaded the goolf toolchain module, which has populated the environment with important details such as the paths  to applications and libraries. This environment is not magically synchronized across the multiple nodes in our OAR job, thus when the hellompi process is started remotely, some libraries are not found.\n\n\nWe will explicitly tell mpirun to export two important environment variables to the remote nodes:\n\n\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE ./hellompi\n\n\n\nNow it (should have) worked and we can try different executions:\n\n\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -n 2 ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -np 2 ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -n 4 ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -n 2 ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -n 3 ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -npernode 1 ./hellompi\n\n\n\nAt the end of these tests, we clean the environment by running \nmodule purge\n:\n\n\n   (node)$> module purge\n   (node)$> module list\n\n\n\nSimple test cases on iris\n\n\nWe will use the same \nhellompi.c\n code from above, so transfer it to the Iris cluster,\nget an interactive job and let's compile it:\n\n\n   (iris-frontend)$> srun -p interactive --qos qos-interactive -N 2 --ntasks-per-node 2 --pty bash -i\n   (node)$> module load toolchain/intel\n   (node)$> mpiicc -o hellompi hellompi.c\n\n\n\nNow we will run it in different ways and see what happens:\n\n\n   (node)$> srun -n 1 hellompi\n   (node)$> srun -n 2 hellompi\n   (node)$> srun -n 3 hellompi\n   (node)$> srun -n 4 hellompi\n   (node)$> srun hellompi\n\n\n\nNote that SLURM's \nsrun\n knows the environment of your job and this will drive parallel execution, if you do not override it explicitly!\n\n\nQuantumESPRESSO\n\n\nCheck for the available versions of QuantumESPRESSO (QE in short), as of June 2015 this shows on the Gaia cluster:\n\n\n   (node)$> module spider quantum\n\n   ----------------------------------------------------------------------------------------------------------------------------------------------------------\n     chem/QuantumESPRESSO:\n   ----------------------------------------------------------------------------------------------------------------------------------------------------------\n       Description:\n         Quantum ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is\n         based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). - Homepage: http://www.pwscf.org/\n\n        Versions:\n           chem/QuantumESPRESSO/5.0.2-goolf-1.4.10-hybrid\n           chem/QuantumESPRESSO/5.0.2-goolf-1.4.10\n           chem/QuantumESPRESSO/5.0.2-ictce-5.3.0-hybrid\n           chem/QuantumESPRESSO/5.0.2-ictce-5.3.0\n           chem/QuantumESPRESSO/5.1.2-ictce-7.3.5\n\n\n\nOne thing we note is that some versions have a \n-hybrid\n suffix. These versions are hybrid MPI+OpenMP builds of QE.\nMPI+OpenMP QE can give better performance than the pure MPI versions, by running only one MPI process per node (instead of one MPI process for each core in the job) that creates (OpenMP) threads which run in parallel locally.\n\n\nLoad the latest QE version:\n\n\n   (node)$> module load chem/QuantumESPRESSO/5.1.2-ictce-7.3.5\n\n\n\nWe will use the PWscf (Plane-Wave Self-Consistent Field) package of QE for our tests.\nRun it in sequential mode, it will wait for your input. You should see a \"Parallel version (MPI), running on     1 processors\" message, and can stop it with CTRL-C:\n\n\n   (node)$> pw.x\n   (node)$> mpirun -n 1 pw.x\n\n\n\nNow try the parallel run over all the nodes/cores in the job:\n\n\n   (node)$> mpirun -hostfile $OAR_NODEFILE pw.x\n\n\n\nBefore stopping it, check that pw.x processes are created on the remote node. You will need to:\n\n\n\n\nopen a second connection to the cluster, or a second window if you're using \nscreen\n or \ntmux\n\n\nconnect to the job with \noarsub -C $JOBID\n\n\nconnect from the head node of the job to the remote job with \noarsh $hostname\n\n\nuse \nhtop\n to show the processes, filter the shown list to see only your user with \nu\n and then selecting your username\n\n\n\n\nNote that this check procedure can be invaluable when you are running an application for the first time, or with new options.\nGenerally, some things to look for are:\n\n\n\n\nthat processes \nare\n created on the remote node, instead of all of them on the head node (which leads to huge slowdowns)\n\n\nthe percentage of CPU usage those processes have, for CPU-intensive work, the values in the CPU% column should be close to 100%\n\n\nif the values are constantly close to 50%, or 25% (or even less) it may mean that more parallel processes were started than should have on that node (e.g. if all processes are running on the head node) and that they are constantly competing for the same cores, which makes execution very slow\n\n\nthe number of threads created by each process\n\n\nhere the number of OpenMP threads, controlled through the OMP_NUM_THREADS environment variable or Intel MKL threads (MKL_NUM_THREADS) may need to be tuned\n\n\n\n\nNow we will run \npw.x\n to perform electronic structure calculations in the presence of a finite homogeneous electric field, and we will use sample input (PW example10) to calculate high-frequency dielectric constant of bulk Silicon.\nFor reference, many examples are given in the installation directory of QE, see \n$EBROOTQUANTUMESPRESSO/espresso-$EBVERSIONQUANTUMESPRESSO/PW/examples\n.\n\n\n   (node)$> cd ~/multiphysics-tutorial\n   (node)$> cd inputs/qe\n   (node)$> pw.x < si.scf.efield2.in\n\n\n\nWe will see the calculation progress, this serial execution should take around 2 minutes.\n\n\nNext, we will clean up the directory holding output files, and re-run the example in parallel:\n\n\n   (node)$> rm -rf out\n   (node)$> mpirun -hostfile $OAR_NODEFILE pw.x < si.scf.efield2.in > si.scf.efield2.out\n\n\n\nWhen the execution ends, we can take a look at the last 10 lines of output and check the execution time:\n\n\n   (node)$> tail si.scf.efield2.out\n\n\n\nYou can now try to run the same examples but with the \nchem/QuantumESPRESSO/5.0.2-goolf-1.4.10-hybrid\n module.\nThings to test:\n\n\n\n\nbasic execution vs usage of the \nnpernode\n parameter of OpenMPI's mpirun\n\n\nexplicitly setting the number of OpenMP threads\n\n\nincreasing the number of OpenMP threads\n\n\n\n\nFinally, we clean the environment by running \nmodule purge\n:\n\n\n   (node)$> module purge\n   (node)$> module list\n\n\n\nOn Iris\n\n\nAs of June 2017, the Iris cluster has a newer QuantumESPRESSO available, let us find it:\n\n\n   (node)$> module avail Quantum\n\n   --------------------- /opt/apps/resif/data/stable/default/modules/all --------------------------\n      chem/QuantumESPRESSO/6.1-intel-2017a\n\n\n\nUsing the same input data as above (transfer the input file to Iris) let's run QE in parallel:\n\n\n   (node)$> module load chem/QuantumESPRESSO\n   (node)$> srun pw.x < si.scf.efield2.in > si.scf.efield2.out\n\n\n\nReferences\n\n\n\n\nQE: user's guide\n\n\nQE: understanding parallelism\n\n\nQE: running on parallel machines\n\n\nQE: parallelization levels\n\n\n\n\nOpenFOAM\n\n\nCheck for the available versions of OpenFOAM on Gaia:\n\n\n   (node)$> module spider openfoam\n\n\n\nWe will use the \ncae/OpenFOAM/2.3.0-goolf-1.4.10\n version:\n\n\n    (node)$> module load cae/OpenFOAM/2.3.0-goolf-1.4.10\n\n\n\nWe load OpenFOAM's startup file:\n\n\n   (node)$> source $FOAM_BASH\n\n\n\nNow we will run the \nreactingParcelFilmFoam\n solver of OpenFOAM on an example showing the spray-film cooling of hot boxes (lagrangian/reactingParcelFilmFoam/hotBoxes).\nFor reference, many examples are given in the installation directory of OpenFOAM, see \n$FOAM_TUTORIALS\n.\n\n\nBefore the main execution, some pre-processing steps:\n\n\n   (node)$> cd ~/multiphysics-tutorial/inputs/openfoam\n   (node)$> cp -rf 0.org 0\n   (node)$> blockMesh\n   (node)$> topoSet\n   (node)$> subsetMesh c0 -patch wallFilm -overwrite\n   (node)$> ./patchifyObstacles > log.patchifyObstacles 2>&1\n   (node)$> extrudeToRegionMesh -overwrite\n   (node)$> changeDictionary\n   (node)$> rm -rf system/wallFilmRegion\n   (node)$> cp -r system/wallFilmRegion.org system/wallFilmRegion\n   (node)$> find ./0 -maxdepth 1 -type f -exec sed -i \"s/wallFilm/\\\"(region0_to.*)\\\"/g\" {} \\;\n   (node)$> paraFoam -touch\n   (node)$> paraFoam -touch -region wallFilmRegion\n   (node)$> decomposePar -region wallFilmRegion\n   (node)$> decomposePar\n\n\n\nSolver execution, note the environment variables we need to export:\n\n\n   (node)$> mpirun -x MPI_BUFFER_SIZE -x WM_PROJECT_DIR -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE reactingParcelFilmFoam -parallel\n\n\n\nNote that the solver execution will take a long time - you can interrupt and/or test in a larger job, which would require:\n\n\n\n\nediting the \ndecomposeParDict\n file to change the \nnumberOfSubdomains\n directive in order to match the new number of processes\n\n\nthen rerun the \ndecomposePar\n commands as above\n\n\n\n\nParenthesis: how can you achieve the best (fastest) execution time? Some questions to think about:\n\n\n\n\nis just increasing the number of cores (oarsub -l core=XYZ) optimal? (no, but why not?)\n\n\nis it better if you check the network hierarchy on the Gaia cluster and target nodes 'closer together'? (yes)\n\n\nwould it be fastest if you run on the fastest (most XYZ GHz) nodes? (not exactly, why?)\n\n\nwill using OAR properties to target the most recent nodes be the best? (yes but not yet optimal, why not?)\n\n\ncan you get an additional speedup if you recompile OpenFOAM on the most recent nodes to take advantage of the newer instruction set available in their CPUs? (yes!)\n\n\n\n\nAfter the main execution, post-processing steps:\n\n\n   (node)$> reconstructPar -region wallFilmRegion\n   (node)$> reconstructPar\n\n\n\nYou can now try to copy and run additional examples from OpenFOAM, note:\n\n\n\n\nthe ones which include an \nAllrun-parallel\n file can be run in parallel\n\n\nyou can run the \nAllrun.pre\n script to prepare the execution\n\n\nyou have to run yourself further pre-execution instructions from the \nAllrun-parallel\n script\n\n\ninstead of \nrunParallel $application 4\n you will have to run mpirun with the correct parameters and the particular application name yourself\n\n\nlast post-processing steps from \nAllrun-parallel\n have to be run manually\n\n\n\n\nFinally, we clean the environment:\n\n\n   (node)$> module purge\n   (node)$> module list\n\n\n\nReferences\n\n\n\n\nOpenFOAM: user's guide\n\n\nOpenFOAM: running applications in parallel\n\n\n\n\nABINIT\n\n\nCheck for the available versions of ABINIT on Gaia:\n\n\n   (node)$> module spider abinit\n\n\n\nAs of June 2015 there is only one version, we load it with:\n\n\n   (node)$> module load chem/ABINIT\n\n\n\nThere is no dependency on a MPI suite in the build of ABINIT, we can use the latest Intel toolchain:\n\n\n   (node)$> module load toolchain/ictce/7.3.5\n\n\n\nWe will use one of ABINIT's parallel test cases to exemplify parallel execution.\nFor reference, many examples are given in the installation directory of ABINIT, see \n$EBROOTABINIT/share/abinit-test\n.\n\n\n   (node)$> cd ~/multiphysics-tutorial/inputs/abinit\n   (node)$> mpirun -hostfile $OAR_NODEFILE abinit < si_kpt_band_fft.files\n\n\n\nAfter some initial processing and messages, we will see:\n\n\n    finddistrproc.F90:394:WARNING\n    Your input dataset does not let Abinit find an appropriate process distribution with nproc=    4\n    Try to comment all the np* vars and set paral_kgb=    -4 to have advices on process distribution.\n\n    abinit : WARNING -\n     The product of npkpt, npfft, npband and npspinor is bigger than the number of processors.\n     The user-defined values of npkpt, npfft, npband or npspinor will be modified,\n     in order to bring this product below nproc .\n     At present, only a very simple algorithm is used ...\n\n    abinit : WARNING -\n     Set npfft to 1\n\n    initmpi_grid.F90:108:WARNING\n      The number of band*FFT*kpt*spinor processors, npband*npfft*npkpt*npspinor should be\n     equal to the total number of processors, nproc.\n     However, npband   =    2           npfft    =    1           npkpt    =    1           npspinor =    1       and nproc    =    4\n\n\n\nAs shown above, ABINIT itself can give details into how to tune input parameters for the dataset used.\n\n\nEdit the input file \nsi_kpt_band_fft\n as per ABINIT's instructions, then re-run ABINIT.\nThe following message will be shown, with a list of parameters that you will need to edit in \nsi_kpt_band_fft\n.\n\n\n   \"Computing all possible proc distrib for this input with nproc less than      4\"\n\n\n\nNext, ensure you can now run ABINIT on this example to completion.\n\n\nParenthesis: will a parallel application always allow execution on any number of cores? Some questions to think about:\n\n\n\n\nare there cases where an input problem cannot be split in some particular ways? (yes!)\n\n\nare all ways to split a problem optimal for solving it as fast as possible? (no!)\n\n\nis it possible to split a problem such that the solver has unbalanced cases and works much slower? (yes)\n\n\nis there a generic way to tune the problem in order to be solved as fast as possible? (no, it's domain & application specific!)\n\n\n\n\nFinally, we clean the environment:\n\n\n   (node)$> module purge\n   (node)$> module list\n\n\n\nReferences\n\n\n\n\nABINIT: user's guide\n\n\nABINIT: tutorials\n\n\n\n\nNAMD\n\n\nNAMD\n, recipient of a 2002 Gordon Bell Award and a 2012 Sidney Fernbach Award, is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. Based on Charm++ parallel objects, NAMD scales to hundreds of cores for typical simulations and beyond 500,000 cores for the largest simulations.\n\n\nThe latest NAMD 2.12 is available on the \niris\n cluster as of June 2017, and on Debian 8 nodes of \ngaia\n as of August 2017, let's check for it:\n\n\n    (node)$> module avail NAMD\n\n    --------------------- /opt/apps/resif/data/production/v0.3/default/modules/all -----------------\n       chem/NAMD/2.12-intel-2017a-mpi\n\n\n\nWe will use one of the benchmark inputs of NAMD to test it, specifically the \nreference\n\n\nSTMV (virus) benchmark (1,066,628 atoms, periodic, PME)\n.\n\n\n    (node)$> mkdir namd-test\n    (node)$> cd namd-test\n    (node)$> wget http://www.ks.uiuc.edu/Research/namd/utilities/stmv.tar.gz\n    (node)$> tar xf stmv.tar.gz\n    (node)$> cd stmv\n    (node)$> module load chem/NAMD\n\n\n\nNow, we will need to set the \noutputName\n parameter within the input file to the path that we want:\n\n\n    (node)$> sed -i 's/^outputName.*$/outputName    generated-data/g' stmv.namd\n\n\n\nNext we will perform the parallel execution of NAMD, showing its runtime output both on console and storing it to file using \ntee\n:\n\n\nOn Gaia\n\n\n    (node)$> mpirun -hostfile $OAR_NODEFILE namd2 stmv.namd | tee out\n\n\n\nOn Iris\n\n\n    (node)$> srun namd2 stmv.namd | tee out\n\n\n\nASE\n\n\nASE is a Python library for working with atoms \n*\n.\n\n\nASE can interface with many external codes as \ncalculators\n: Asap, GPAW, Hotbit, ABINIT, CP2K, CASTEP, DFTB+, ELK, EXCITING, FHI-aims, FLEUR, GAUSSIAN, Gromacs, Jacapo, LAMMPS, MOPAC, NWChem, SIESTA, TURBOMOLE and VASP.\n\n\nReferences\n\n\n\n\nASE: tutorials\n\n\nASE: calculators",
            "title": "Advanced Parallel execution"
        },
        {
            "location": "/advanced/MultiPhysics/#ul-hpc-mpi-tutorial-running-parallel-software-test-cases-on-cfd-md-chemistry-applications",
            "text": "The objective of this session is to exemplify the execution of several common, parallel, Computational Fluid Dynamics, Molecular Dynamics and Chemistry software on the  UL HPC  platform.  Targeted applications include:   OpenFOAM : CFD package for solving complex fluid flows involving chemical reactions, turbulence and heat transfer  NAMD : parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems  ASE : Atomistic Simulation Environment (Python-based) with the aim of setting up, steering, and analyzing atomistic simulations  ABINIT : materials science package implementing DFT, DFPT, MBPT and TDDFT  Quantum Espresso : integrated suite of tools for electronic-structure calculations and materials modeling at the nanoscale   The tutorial will cover:   Basics for parallel execution with OAR and SLURM  different MPI suites available on UL HPC  running simple test cases in parallel  running QuantumEspresso in parallel over a single node and over multiple nodes  running OpenFOAM in parallel over a single node and over multiple nodes  running ABINIT in parallel over a single node and over multiple nodes  the interesting case of the ASE toolkit",
            "title": "UL HPC MPI Tutorial: Running parallel software: test cases on CFD / MD / Chemistry applications"
        },
        {
            "location": "/advanced/MultiPhysics/#prerequisites",
            "text": "As part of this tutorial several input files will be required and you will need to download them\nbefore following the instructions in the next sections:  Or simply clone the full tutorials repository and make a link to this tutorial      (gaia-frontend)$> git clone https://github.com/ULHPC/tutorials.git\n    (gaia-frontend)$> ln -s tutorials/advanced/MultiPhysics/ ~/multiphysics-tutorial",
            "title": "Prerequisites"
        },
        {
            "location": "/advanced/MultiPhysics/#basics",
            "text": "Note: you can check out either the instructions for the OAR scheduler (gaia and chaos clusters) or SLURM (for iris).",
            "title": "Basics"
        },
        {
            "location": "/advanced/MultiPhysics/#oar-basics-for-parallel-execution",
            "text": "First of all, we will submit a job with 2 cores on each of 2 compute nodes for 1 hour.\nPlease note that the Gaia cluster can be replaced at any point in this tutorial with the Chaos cluster if not enough resources are immediately available.     (gaia-frontend)$> oarsub -I -l nodes=2/core=2,walltime=1\n   (node)$>  The OAR scheduler provides several environment variables once we are inside a job, check them out with     (node)$> env | grep OAR_  We are interested especially in the environment variable which points to the file containing the list of hostnames reserved for the job.\nThis variable is OAR_NODEFILE, yet there are several others pointing to the same (OAR_NODE_FILE, OAR_FILE_NODES and OAR_RESOURCE_FILE).\nLet's check its content:     (node)$> cat $OAR_NODEFILE  To get the number of cores available in the job, we can use the wordcount  wc  utility, in line counting mode:     (node)$> cat $OAR_NODEFILE | wc -l",
            "title": "OAR basics for parallel execution"
        },
        {
            "location": "/advanced/MultiPhysics/#slurm-basics-for-parallel-execution",
            "text": "First of all, we will submit on the iris cluster an interactive job with 2 tasks on each of 2 compute nodes for 1 hour.     (iris-frontend)$> srun -p interactive --qos qos-interactive -N 2 --ntasks-per-node 2 --pty bash -i\n   (node)$>  The SLURM scheduler provides several environment variables once we are inside a job, check them out with     (node)$> env | grep SLURM_  We are interested especially in the environment variable which lists the compute nodes reserved for the job -- SLURM_NODELIST.\nLet's check its content:     (node)$> cat $SLURM_NODELIST  To get the total number of cores available in the job, we can use the wordcount  wc  utility, in line counting mode:     (node)$> srun hostname | wc -l  To get the number of cores available on the current compute node:     (node)$> echo $SLURM_CPUS_ON_NODE",
            "title": "SLURM basics for parallel execution"
        },
        {
            "location": "/advanced/MultiPhysics/#mpi-suites-available-on-the-platform",
            "text": "",
            "title": "MPI suites available on the platform"
        },
        {
            "location": "/advanced/MultiPhysics/#on-gaia",
            "text": "Now, let's check for the environment modules (available through Lmod) which match MPI (Message Passing Interface) the libraries that provide inter-process communication over a network:     (node)$> module avail mpi/\n\n   ---------------------------------------------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/mpi ----------------------------------------------------\n      mpi/OpenMPI/1.6.4-GCC-4.7.2    mpi/OpenMPI/1.8.4-GCC-4.9.2 (D)    mpi/impi/4.1.0.030-iccifort-2013.3.163    mpi/impi/5.0.3.048-iccifort-2015.3.187 (D)\n\n   ------------------------------------------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/toolchain -------------------------------------------------\n      toolchain/gompi/1.4.10    toolchain/iimpi/5.3.0    toolchain/iimpi/7.3.5 (D)\n\n     Where:\n      (D):  Default Module\n\n   Use \"module spider\" to find all possible modules.\n   Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".  Perform the same search for the toolchains:     (node)$> module avail toolchain/  Toolchains represent sets of compilers together with libraries commonly required to build software, such as MPI, BLAS/LAPACK (linear algebra) and FFT (Fast Fourier Transforms).\nFor more details, see  the EasyBuild page on toolchains .  For our initial tests we will use the  goolf  toolchain which includes GCC, OpenMPI, OpenBLAS/LAPACK, ScaLAPACK(/BLACS) and FFTW:     (node)$> module load toolchain/goolf/1.4.10\n   (node)$> module list  The main alternative to this toolchain (as of June 2015) is  ictce  (toolchain/ictce/7.3.5) that includes the Intel tools icc, ifort, impi and imkl.",
            "title": "On Gaia:"
        },
        {
            "location": "/advanced/MultiPhysics/#on-iris",
            "text": "(node)$> module avail mpi/\n\n   ----------------------------- /opt/apps/resif/data/stable/default/modules/all ----------------------------------------------------\n      mpi/MVAPICH2/2.2b-GCC-4.9.3-2.25    mpi/OpenMPI/2.1.1-GCC-6.3.0-2.28                       (D)    toolchain/gompi/2017a\n      mpi/OpenMPI/2.1.1-GCC-6.3.0-2.27    mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27        toolchain/iimpi/2017a\n\n     Where:\n      D:  Default Module\n\n   Use \"module spider\" to find all possible modules.\n   Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n\n\n  (node)$> module avail toolchain/  On iris the main two toolchains are  foss  (GCC, OpenMPI, OpenBLAS, ScaLAPACK and FFTW) and  intel  (Intel C/C++/Fortran, MKL).",
            "title": "On Iris:"
        },
        {
            "location": "/advanced/MultiPhysics/#simple-test-cases-on-gaia",
            "text": "We will now try to run the  hostname  application, which simply shows a system's host name.\nCheck out the differences (if any) between the following executions:     (node)$> hostname\n   (node)$> mpirun hostname\n   (node)$> mpirun -n 2 hostname\n   (node)$> mpirun -np 2 hostname\n   (node)$> mpirun -n 4 hostname\n   (node)$> mpirun -hostfile $OAR_NODEFILE hostname\n   (node)$> mpirun -hostfile $OAR_NODEFILE -n 2 hostname\n   (node)$> mpirun -hostfile $OAR_NODEFILE -n 3 hostname\n   (node)$> mpirun -hostfile $OAR_NODEFILE -npernode 1 hostname  Note that the  hostname  application is  not  a parallel application, with MPI we are simply launching it on the different nodes available in the job.  Now, we will compile and run a simple  hellompi  MPI application. Save the following source code in /tmp/hellompi.c :     #include <mpi.h>\n   #include <stdio.h>\n   #include <stdlib.h>\n   int main(int argc, char** argv) {\n     MPI_Init(NULL, NULL);\n     int world_size;\n     MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n     int world_rank;\n     MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n     char processor_name[MPI_MAX_PROCESSOR_NAME];\n     int name_len;\n     MPI_Get_processor_name(processor_name, &name_len);\n     printf(\"Hello world from %s, rank %d out of %d CPUs\\n\", processor_name, world_rank, world_size);\n     MPI_Finalize();\n   }  Compile it:     (node)$> cd /tmp\n   (node)$> mpicc -o hellompi hellompi.c  Run it:     (node)$> mpirun -hostfile $OAR_NODEFILE ./hellompi  Why didn't it work? Remember we stored this application on a compute node's /tmp directory, which is local to each node, not shared.\nThus the application couldn't be found (more on this later) on the remote nodes.  Let's move it to the  $HOME  directory which is common across the cluster, and try again:     (node)$> mv /tmp/hellompi ~/multiphysics-tutorial\n   (node)$> cd ~/multiphysics-tutorial\n   (node)$> mpirun -hostfile $OAR_NODEFILE ./hellompi  Now some different error messages are shown, about loading shared libraries, and the execution hangs (stop it with Ctrl-C). Why?\nRemember that on the current node we have loaded the goolf toolchain module, which has populated the environment with important details such as the paths  to applications and libraries. This environment is not magically synchronized across the multiple nodes in our OAR job, thus when the hellompi process is started remotely, some libraries are not found.  We will explicitly tell mpirun to export two important environment variables to the remote nodes:     (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE ./hellompi  Now it (should have) worked and we can try different executions:     (node)$> mpirun -x PATH -x LD_LIBRARY_PATH ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -n 2 ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -np 2 ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -n 4 ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -n 2 ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -n 3 ./hellompi\n   (node)$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -npernode 1 ./hellompi  At the end of these tests, we clean the environment by running  module purge :     (node)$> module purge\n   (node)$> module list",
            "title": "Simple test cases on Gaia"
        },
        {
            "location": "/advanced/MultiPhysics/#simple-test-cases-on-iris",
            "text": "We will use the same  hellompi.c  code from above, so transfer it to the Iris cluster,\nget an interactive job and let's compile it:     (iris-frontend)$> srun -p interactive --qos qos-interactive -N 2 --ntasks-per-node 2 --pty bash -i\n   (node)$> module load toolchain/intel\n   (node)$> mpiicc -o hellompi hellompi.c  Now we will run it in different ways and see what happens:     (node)$> srun -n 1 hellompi\n   (node)$> srun -n 2 hellompi\n   (node)$> srun -n 3 hellompi\n   (node)$> srun -n 4 hellompi\n   (node)$> srun hellompi  Note that SLURM's  srun  knows the environment of your job and this will drive parallel execution, if you do not override it explicitly!",
            "title": "Simple test cases on iris"
        },
        {
            "location": "/advanced/MultiPhysics/#quantumespresso",
            "text": "Check for the available versions of QuantumESPRESSO (QE in short), as of June 2015 this shows on the Gaia cluster:     (node)$> module spider quantum\n\n   ----------------------------------------------------------------------------------------------------------------------------------------------------------\n     chem/QuantumESPRESSO:\n   ----------------------------------------------------------------------------------------------------------------------------------------------------------\n       Description:\n         Quantum ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is\n         based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). - Homepage: http://www.pwscf.org/\n\n        Versions:\n           chem/QuantumESPRESSO/5.0.2-goolf-1.4.10-hybrid\n           chem/QuantumESPRESSO/5.0.2-goolf-1.4.10\n           chem/QuantumESPRESSO/5.0.2-ictce-5.3.0-hybrid\n           chem/QuantumESPRESSO/5.0.2-ictce-5.3.0\n           chem/QuantumESPRESSO/5.1.2-ictce-7.3.5  One thing we note is that some versions have a  -hybrid  suffix. These versions are hybrid MPI+OpenMP builds of QE.\nMPI+OpenMP QE can give better performance than the pure MPI versions, by running only one MPI process per node (instead of one MPI process for each core in the job) that creates (OpenMP) threads which run in parallel locally.  Load the latest QE version:     (node)$> module load chem/QuantumESPRESSO/5.1.2-ictce-7.3.5  We will use the PWscf (Plane-Wave Self-Consistent Field) package of QE for our tests.\nRun it in sequential mode, it will wait for your input. You should see a \"Parallel version (MPI), running on     1 processors\" message, and can stop it with CTRL-C:     (node)$> pw.x\n   (node)$> mpirun -n 1 pw.x  Now try the parallel run over all the nodes/cores in the job:     (node)$> mpirun -hostfile $OAR_NODEFILE pw.x  Before stopping it, check that pw.x processes are created on the remote node. You will need to:   open a second connection to the cluster, or a second window if you're using  screen  or  tmux  connect to the job with  oarsub -C $JOBID  connect from the head node of the job to the remote job with  oarsh $hostname  use  htop  to show the processes, filter the shown list to see only your user with  u  and then selecting your username   Note that this check procedure can be invaluable when you are running an application for the first time, or with new options.\nGenerally, some things to look for are:   that processes  are  created on the remote node, instead of all of them on the head node (which leads to huge slowdowns)  the percentage of CPU usage those processes have, for CPU-intensive work, the values in the CPU% column should be close to 100%  if the values are constantly close to 50%, or 25% (or even less) it may mean that more parallel processes were started than should have on that node (e.g. if all processes are running on the head node) and that they are constantly competing for the same cores, which makes execution very slow  the number of threads created by each process  here the number of OpenMP threads, controlled through the OMP_NUM_THREADS environment variable or Intel MKL threads (MKL_NUM_THREADS) may need to be tuned   Now we will run  pw.x  to perform electronic structure calculations in the presence of a finite homogeneous electric field, and we will use sample input (PW example10) to calculate high-frequency dielectric constant of bulk Silicon.\nFor reference, many examples are given in the installation directory of QE, see  $EBROOTQUANTUMESPRESSO/espresso-$EBVERSIONQUANTUMESPRESSO/PW/examples .     (node)$> cd ~/multiphysics-tutorial\n   (node)$> cd inputs/qe\n   (node)$> pw.x < si.scf.efield2.in  We will see the calculation progress, this serial execution should take around 2 minutes.  Next, we will clean up the directory holding output files, and re-run the example in parallel:     (node)$> rm -rf out\n   (node)$> mpirun -hostfile $OAR_NODEFILE pw.x < si.scf.efield2.in > si.scf.efield2.out  When the execution ends, we can take a look at the last 10 lines of output and check the execution time:     (node)$> tail si.scf.efield2.out  You can now try to run the same examples but with the  chem/QuantumESPRESSO/5.0.2-goolf-1.4.10-hybrid  module.\nThings to test:   basic execution vs usage of the  npernode  parameter of OpenMPI's mpirun  explicitly setting the number of OpenMP threads  increasing the number of OpenMP threads   Finally, we clean the environment by running  module purge :     (node)$> module purge\n   (node)$> module list",
            "title": "QuantumESPRESSO"
        },
        {
            "location": "/advanced/MultiPhysics/#on-iris_1",
            "text": "As of June 2017, the Iris cluster has a newer QuantumESPRESSO available, let us find it:     (node)$> module avail Quantum\n\n   --------------------- /opt/apps/resif/data/stable/default/modules/all --------------------------\n      chem/QuantumESPRESSO/6.1-intel-2017a  Using the same input data as above (transfer the input file to Iris) let's run QE in parallel:     (node)$> module load chem/QuantumESPRESSO\n   (node)$> srun pw.x < si.scf.efield2.in > si.scf.efield2.out",
            "title": "On Iris"
        },
        {
            "location": "/advanced/MultiPhysics/#references",
            "text": "QE: user's guide  QE: understanding parallelism  QE: running on parallel machines  QE: parallelization levels",
            "title": "References"
        },
        {
            "location": "/advanced/MultiPhysics/#openfoam",
            "text": "Check for the available versions of OpenFOAM on Gaia:     (node)$> module spider openfoam  We will use the  cae/OpenFOAM/2.3.0-goolf-1.4.10  version:      (node)$> module load cae/OpenFOAM/2.3.0-goolf-1.4.10  We load OpenFOAM's startup file:     (node)$> source $FOAM_BASH  Now we will run the  reactingParcelFilmFoam  solver of OpenFOAM on an example showing the spray-film cooling of hot boxes (lagrangian/reactingParcelFilmFoam/hotBoxes).\nFor reference, many examples are given in the installation directory of OpenFOAM, see  $FOAM_TUTORIALS .  Before the main execution, some pre-processing steps:     (node)$> cd ~/multiphysics-tutorial/inputs/openfoam\n   (node)$> cp -rf 0.org 0\n   (node)$> blockMesh\n   (node)$> topoSet\n   (node)$> subsetMesh c0 -patch wallFilm -overwrite\n   (node)$> ./patchifyObstacles > log.patchifyObstacles 2>&1\n   (node)$> extrudeToRegionMesh -overwrite\n   (node)$> changeDictionary\n   (node)$> rm -rf system/wallFilmRegion\n   (node)$> cp -r system/wallFilmRegion.org system/wallFilmRegion\n   (node)$> find ./0 -maxdepth 1 -type f -exec sed -i \"s/wallFilm/\\\"(region0_to.*)\\\"/g\" {} \\;\n   (node)$> paraFoam -touch\n   (node)$> paraFoam -touch -region wallFilmRegion\n   (node)$> decomposePar -region wallFilmRegion\n   (node)$> decomposePar  Solver execution, note the environment variables we need to export:     (node)$> mpirun -x MPI_BUFFER_SIZE -x WM_PROJECT_DIR -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE reactingParcelFilmFoam -parallel  Note that the solver execution will take a long time - you can interrupt and/or test in a larger job, which would require:   editing the  decomposeParDict  file to change the  numberOfSubdomains  directive in order to match the new number of processes  then rerun the  decomposePar  commands as above   Parenthesis: how can you achieve the best (fastest) execution time? Some questions to think about:   is just increasing the number of cores (oarsub -l core=XYZ) optimal? (no, but why not?)  is it better if you check the network hierarchy on the Gaia cluster and target nodes 'closer together'? (yes)  would it be fastest if you run on the fastest (most XYZ GHz) nodes? (not exactly, why?)  will using OAR properties to target the most recent nodes be the best? (yes but not yet optimal, why not?)  can you get an additional speedup if you recompile OpenFOAM on the most recent nodes to take advantage of the newer instruction set available in their CPUs? (yes!)   After the main execution, post-processing steps:     (node)$> reconstructPar -region wallFilmRegion\n   (node)$> reconstructPar  You can now try to copy and run additional examples from OpenFOAM, note:   the ones which include an  Allrun-parallel  file can be run in parallel  you can run the  Allrun.pre  script to prepare the execution  you have to run yourself further pre-execution instructions from the  Allrun-parallel  script  instead of  runParallel $application 4  you will have to run mpirun with the correct parameters and the particular application name yourself  last post-processing steps from  Allrun-parallel  have to be run manually   Finally, we clean the environment:     (node)$> module purge\n   (node)$> module list",
            "title": "OpenFOAM"
        },
        {
            "location": "/advanced/MultiPhysics/#references_1",
            "text": "OpenFOAM: user's guide  OpenFOAM: running applications in parallel",
            "title": "References"
        },
        {
            "location": "/advanced/MultiPhysics/#abinit",
            "text": "Check for the available versions of ABINIT on Gaia:     (node)$> module spider abinit  As of June 2015 there is only one version, we load it with:     (node)$> module load chem/ABINIT  There is no dependency on a MPI suite in the build of ABINIT, we can use the latest Intel toolchain:     (node)$> module load toolchain/ictce/7.3.5  We will use one of ABINIT's parallel test cases to exemplify parallel execution.\nFor reference, many examples are given in the installation directory of ABINIT, see  $EBROOTABINIT/share/abinit-test .     (node)$> cd ~/multiphysics-tutorial/inputs/abinit\n   (node)$> mpirun -hostfile $OAR_NODEFILE abinit < si_kpt_band_fft.files  After some initial processing and messages, we will see:      finddistrproc.F90:394:WARNING\n    Your input dataset does not let Abinit find an appropriate process distribution with nproc=    4\n    Try to comment all the np* vars and set paral_kgb=    -4 to have advices on process distribution.\n\n    abinit : WARNING -\n     The product of npkpt, npfft, npband and npspinor is bigger than the number of processors.\n     The user-defined values of npkpt, npfft, npband or npspinor will be modified,\n     in order to bring this product below nproc .\n     At present, only a very simple algorithm is used ...\n\n    abinit : WARNING -\n     Set npfft to 1\n\n    initmpi_grid.F90:108:WARNING\n      The number of band*FFT*kpt*spinor processors, npband*npfft*npkpt*npspinor should be\n     equal to the total number of processors, nproc.\n     However, npband   =    2           npfft    =    1           npkpt    =    1           npspinor =    1       and nproc    =    4  As shown above, ABINIT itself can give details into how to tune input parameters for the dataset used.  Edit the input file  si_kpt_band_fft  as per ABINIT's instructions, then re-run ABINIT.\nThe following message will be shown, with a list of parameters that you will need to edit in  si_kpt_band_fft .     \"Computing all possible proc distrib for this input with nproc less than      4\"  Next, ensure you can now run ABINIT on this example to completion.  Parenthesis: will a parallel application always allow execution on any number of cores? Some questions to think about:   are there cases where an input problem cannot be split in some particular ways? (yes!)  are all ways to split a problem optimal for solving it as fast as possible? (no!)  is it possible to split a problem such that the solver has unbalanced cases and works much slower? (yes)  is there a generic way to tune the problem in order to be solved as fast as possible? (no, it's domain & application specific!)   Finally, we clean the environment:     (node)$> module purge\n   (node)$> module list",
            "title": "ABINIT"
        },
        {
            "location": "/advanced/MultiPhysics/#references_2",
            "text": "ABINIT: user's guide  ABINIT: tutorials",
            "title": "References"
        },
        {
            "location": "/advanced/MultiPhysics/#namd",
            "text": "NAMD , recipient of a 2002 Gordon Bell Award and a 2012 Sidney Fernbach Award, is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. Based on Charm++ parallel objects, NAMD scales to hundreds of cores for typical simulations and beyond 500,000 cores for the largest simulations.  The latest NAMD 2.12 is available on the  iris  cluster as of June 2017, and on Debian 8 nodes of  gaia  as of August 2017, let's check for it:      (node)$> module avail NAMD\n\n    --------------------- /opt/apps/resif/data/production/v0.3/default/modules/all -----------------\n       chem/NAMD/2.12-intel-2017a-mpi  We will use one of the benchmark inputs of NAMD to test it, specifically the  reference  STMV (virus) benchmark (1,066,628 atoms, periodic, PME) .      (node)$> mkdir namd-test\n    (node)$> cd namd-test\n    (node)$> wget http://www.ks.uiuc.edu/Research/namd/utilities/stmv.tar.gz\n    (node)$> tar xf stmv.tar.gz\n    (node)$> cd stmv\n    (node)$> module load chem/NAMD  Now, we will need to set the  outputName  parameter within the input file to the path that we want:      (node)$> sed -i 's/^outputName.*$/outputName    generated-data/g' stmv.namd  Next we will perform the parallel execution of NAMD, showing its runtime output both on console and storing it to file using  tee :",
            "title": "NAMD"
        },
        {
            "location": "/advanced/MultiPhysics/#on-gaia_1",
            "text": "(node)$> mpirun -hostfile $OAR_NODEFILE namd2 stmv.namd | tee out",
            "title": "On Gaia"
        },
        {
            "location": "/advanced/MultiPhysics/#on-iris_2",
            "text": "(node)$> srun namd2 stmv.namd | tee out",
            "title": "On Iris"
        },
        {
            "location": "/advanced/MultiPhysics/#ase",
            "text": "ASE is a Python library for working with atoms  * .  ASE can interface with many external codes as  calculators : Asap, GPAW, Hotbit, ABINIT, CP2K, CASTEP, DFTB+, ELK, EXCITING, FHI-aims, FLEUR, GAUSSIAN, Gromacs, Jacapo, LAMMPS, MOPAC, NWChem, SIESTA, TURBOMOLE and VASP.",
            "title": "ASE"
        },
        {
            "location": "/advanced/MultiPhysics/#references_3",
            "text": "ASE: tutorials  ASE: calculators",
            "title": "References"
        },
        {
            "location": "/advanced/Spark/",
            "text": "-\n- mode: markdown; mode: visual-line; fill-column: 80 -\n-\n\n\nCopyright (c) 2013-2017 UL HPC Team  \nhpc-sysadmins@uni.lu\n\n\n    Time-stamp: <Tue 2017-06-20 11:56 svarrette>\n\n\n\n\n\nRunning Big Data Application using Apache Spark on UL HPC platform\n\n\n \n \n \n \n \n\n\nThe objective of this tutorial is to compile and run on \nApache Spark\n  on top of the \nUL HPC\n platform.\n\n\nAdvanced users only\n: rely on \nscreen\n (see  \ntutorial\n or the \nUL HPC tutorial\n on the  frontend prior to running any \noarsub\n or \nsrun/sbatch\n command to be more resilient to disconnection.\n\n\nThe latest version of this tutorial is available on\n\nGithub\n\n\nObjectives\n\n\nApache Spark\n is a large-scale data processing engine that performs in-memory computing. Spark offers bindings in Java, Scala, Python and R for building parallel applications.\nhigh-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n\n\nIn this tutorial, we are going to build Spark using Easybuild and perform some basic examples.\n\n\nBuilding Spark\n\n\nPre-requisite: Installing Easybuild\n\n\nSee also \nPS3\n.\n\n\nFirst we are going to install Easybuild following \nthe official instructions\n.\n\n\nAdd the following entries to your \n~/.bashrc\n:\n\n\nexport EASYBUILD_PREFIX=$HOME/.local/easybuild\nexport EASYBUILD_MODULES_TOOL=Lmod\nexport EASYBUILD_MODULE_NAMING_SCHEME=CategorizedModuleNamingScheme\n# Use the below variable to run:\n#    module use $LOCAL_MODULES\n#    module load tools/EasyBuild\nexport LOCAL_MODULES=${EASYBUILD_PREFIX}/modules/all\n\n\n\n\nThen source this file to expose the environment variables:\n\n\n$> source ~/.bashrc\n$> echo $EASYBUILD_PREFIX\n/home/users/svarrette/.local/easybuild\n\n\n\n\nNow let's install Easybuild following the \nboostrapping procedure\n\n\n$> cd /tmp/\n# download script\ncurl -o /tmp/bootstrap_eb.py  https://raw.githubusercontent.com/hpcugent/easybuild-framework/develop/easybuild/scripts/bootstrap_eb.py\n\n# install Easybuild\n$> python /tmp/bootstrap_eb.py $EASYBUILD_PREFIX\n\n# Load it\n$> echo $MODULEPATH\n$> module use $LOCAL_MODULES\n$> echo $MODULEPATH\n$> module spider Easybuild\n$> module load tools/EasyBuild\n\n\n\n\nSearch for an Easybuild Recipe for Spark\n\n\n$> eb -S Spark\n# Try to install the most recent version\n$> eb Spark-2.0.2.eb -Dr    # Dry-run\n$> eb Spark-2.0.2.eb -r\n\n\n\n\nThis is going to fail because of the Java dependency which is unable to build.\nSo we are going to create a custom easyconfig file\n\n\n$> mkdir -p ~/tutorials/Spark\n$> cd ~/tutorials/Spark\n# Check the source easyconfig file\n$> eb -S Spark\n$> cp <path/to>/easyconfigs/s/Spark/Spark-2.0.2.eb  Spark-2.0.2.custom.eb\n\n\n\n\nModify it to ensure a successful build.\n\n\n--- <path/to>/easyconfigs/s/Spark/Spark-2.0.2.eb 2017-06-12 22:16:14.353929000 +0200\n+++ Spark-2.0.2.custom.eb 2017-06-12 22:39:59.155061000 +0200\n@@ -15,7 +15,7 @@\n     'http://www.us.apache.org/dist/%(namelower)s/%(namelower)s-%(version)s/',\n ]\n\n-dependencies = [('Java', '1.7.0_80')]\n+dependencies = [('Java', '1.8.0_121')]\n\n sanity_check_paths = {\n     'files': ['bin/spark-shell'],\n\n\n\n\nBuild it and load the module\n\n\n$> eb ./Spark-2.0.2.custom.eb\n$> module spider Spark\n$> module load devel/Spark\n\n\n\n\nInteractive usage\n\n\nPySpark is the Spark Python API and exposes Spark Contexts to the Python programming environment. Use \n--exclusive\n to allocate an exclusive node, load the spark module, then run the Python Spark shell:\n\n\n$> si --exclusive --ntasks=1 --cpus-per-task=28\n$> module use $LOCAL_MODULE\n$> module load devel/Spark\n$> pyspark\n\n\n\n\nAfter some initialization output, you will see the following:\n\n\n$> pyspark\nPython 2.7.5 (default, Nov  6 2016, 00:28:07)\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel).\n17/06/12 23:59:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.2\n      /_/\n\nUsing Python version 2.7.5 (default, Nov  6 2016 00:28:07)\nSparkSession available as 'spark'.\n>>>\n\n\n\n\nSee \ntutorial\n for playing with pyspark.\n\n\nScala Spark Shell\n\n\nSpark includes a modified version of the Scala shell that can be used interactively. Instead of running \npyspark\n above, run the \nspark-shell\n command:\n\n\n$> spark-shell\n\n\n\n\nAfter some initialization output, a scala shell prompt with the Spark context will be available:\n\n\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel).\n17/06/13 00:06:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/06/13 00:06:43 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.\nSpark context Web UI available at http://172.17.7.30:4040\nSpark context available as 'sc' (master = local[*], app id = local-1497305203669).\nSpark session available as 'spark'.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.0.2\n      /_/\n\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_121)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala>\n\n\n\n\nR Spark Shell\n\n\nThe Spark R API is still experimental. Only a subset of the R API is available -- See the \nSparkR Documentation\n.\n\n\nLoad one of the R modules and then run the SparkR shell:\n\n\n$> module load lang/R\n$> sparkR\n\nR version 3.4.0 (2017-04-21) -- \"You Stupid Darkness\"\nCopyright (C) 2017 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\nLaunching java with spark-submit command /home/users/svarrette/.local/easybuild/software/devel/Spark/2.0.2/bin/spark-submit   \"sparkr-shell\" /tmp/Rtmphb0s8J/backend_port180ad365cc6b5\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel).\n17/06/13 00:08:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n Welcome to\n    ____              __\n   / __/__  ___ _____/ /__\n  _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version  2.0.2\n    /_/\n\n\n SparkSession available as 'spark'.\n>\n\n\n\n\nRunning Spark standalone cluster\n\n\n\n\nReference Documentation\n\n\n\n\nSpark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).\n\n\nSpecifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark\u2019s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.\n\n\n\n\nThere are several useful things to note about this architecture:\n\n\n\n\nEach application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system.\n\n\nSpark is agnostic to the underlying cluster manager. As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN).\n\n\nThe driver program must listen for and accept incoming connections from its executors throughout its lifetime (e.g., see spark.driver.port in the network config section). As such, the driver program must be network addressable from the worker nodes.\n\n\nBecause the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. If you'd like to send requests to the cluster remotely, it's better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes.\n\n\n\n\nCluster Manager\n\n\nSpark currently supports three cluster managers:\n\n\n\n\nStandalone\n \u2013 a simple cluster manager included with Spark that makes it easy to set up a cluster.\n\n\nApache Mesos\n \u2013 a general cluster manager that can also run Hadoop MapReduce and service applications.\n\n\nHadoop YARN\n \u2013 the resource manager in Hadoop 2.\n\n\n\n\nIn this session, we will deploy a \nstandalone cluster\n.\n\n\nWe will prepare a launcher script that will:\n\n\n\n\ncreate a master and the workers\n\n\nsubmit a spark application to the cluster using the \nspark-submit\n script\n\n\nLet the application run and collect the result\n\n\nstop the cluster at the end.\n\n\n\n\nTo facilitate these steps, Spark comes with a couple of scripts you can use to launch or stop your cluster, based on Hadoop's deploy scripts, and available in \n$EBROOTSPARK/sbin\n:\n\n\n\n\n\n\n\n\nScript\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsbin/start-master.sh\n\n\nStarts a master instance on the machine the script is executed on.\n\n\n\n\n\n\nsbin/start-slaves.sh\n\n\nStarts a slave instance on each machine specified in the conf/slaves file.\n\n\n\n\n\n\nsbin/start-slave.sh\n\n\nStarts a slave instance on the machine the script is executed on.\n\n\n\n\n\n\nsbin/start-all.sh\n\n\nStarts both a master and a number of slaves as described above.\n\n\n\n\n\n\nsbin/stop-master.sh\n\n\nStops the master that was started via the bin/start-master.sh script.\n\n\n\n\n\n\nsbin/stop-slaves.sh\n\n\nStops all slave instances on the machines specified in the conf/slaves file.\n\n\n\n\n\n\nsbin/stop-all.sh\n\n\nStops both the master and the slaves as described above.\n\n\n\n\n\n\n\n\nWe are now going to prepare a launcher scripts to permit passive runs (typically in the \n{default | batch}\n queue).\nWe will place them in a separate directory as it will host the outcomes of the executions on the UL HPC platform .\nCopy and adapt the \ndefault SLURM launcher\n you should have a copy in \n~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh\n\n\n$> cd ~/tutorials/Spark/\n$> cp ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh launcher-spark-pi.sh\n\n\n\n\nThe launcher will be organized as follows.\n\n\nWe will first exclusively allocate 2 nodes\n\n\n#!/bin/bash -l\n# Time-stamp: <Sun 2017-06-11 22:13 svarrette>\n##################################################################\n#SBATCH -N 1\n# Exclusive mode is recommended for all spark jobs\n#SBATCH --exclusive\n#SBATCH --ntasks-per-node 1\n### -c, --cpus-per-task=<ncpus>\n###     (multithreading) Request that ncpus be allocated per process\n#SBATCH -c 28\n#SBATCH --time=0-01:00:00   # 1 hour\n#\n#          Set the name of the job\n#SBATCH -J SparkMaster\n#          Passive jobs specifications\n#SBATCH --partition=batch\n#SBATCH --qos qos-batch\n\n\n\n\nThen we will load the custom module\n\n\n# Use the RESIF build modules\nif [ -f  /etc/profile ]; then\n   .  /etc/profile\nfi\n\n# Load the {intel | foss} toolchain and whatever module(s) you need\nmodule purge\nmodule use $HOME/.local/easybuild/modules/all\nmodule load devel/Spark\n\nexport SPARK_HOME=$EBROOTSPARK\n\n\n\n\nThen start the Spark master and worker daemons using the Spark scripts\n\n\n# sbin/start-master.sh - Starts a master instance on the machine the script is executed on.\n$SPARK_HOME/sbin/start-all.sh\n\nexport MASTER=spark://$HOSTNAME:7077\n\necho\necho \"========= Spark Master ========\"\necho $MASTER\necho \"===============================\"\necho\n\n\n\n\nNow we can submit an example python Pi estimation script to the Spark cluster with 100 partitions\n\n\nNote\n: partitions in this context refers of course to Spark's Resilient Distributed Dataset (RDD) and how the dataset is distributed across the nodes in the Spark cluster.\n\n\nspark-submit --master $MASTER  $SPARK_HOME/examples/src/main/python/pi.py 100\n\n\n\n\nFinally, at the end, clean your environment and\n\n\n# sbin/stop-master.sh - Stops the master that was started via the bin/start-master.sh script.\n$SPARK_HOME/sbin/stop-all.sh\n\n\n\n\nWhen the job completes, you can find the Pi estimation result in the slurm output file:\n\n\n$> grep Pi slurm-2853.out\nPi is roughly 3.147861",
            "title": "Big Data Application Over Apache Spark"
        },
        {
            "location": "/advanced/Spark/#running-big-data-application-using-apache-spark-on-ul-hpc-platform",
            "text": "The objective of this tutorial is to compile and run on  Apache Spark   on top of the  UL HPC  platform.  Advanced users only : rely on  screen  (see   tutorial  or the  UL HPC tutorial  on the  frontend prior to running any  oarsub  or  srun/sbatch  command to be more resilient to disconnection.  The latest version of this tutorial is available on Github",
            "title": "Running Big Data Application using Apache Spark on UL HPC platform"
        },
        {
            "location": "/advanced/Spark/#objectives",
            "text": "Apache Spark  is a large-scale data processing engine that performs in-memory computing. Spark offers bindings in Java, Scala, Python and R for building parallel applications.\nhigh-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.  In this tutorial, we are going to build Spark using Easybuild and perform some basic examples.",
            "title": "Objectives"
        },
        {
            "location": "/advanced/Spark/#building-spark",
            "text": "",
            "title": "Building Spark"
        },
        {
            "location": "/advanced/Spark/#pre-requisite-installing-easybuild",
            "text": "See also  PS3 .  First we are going to install Easybuild following  the official instructions .  Add the following entries to your  ~/.bashrc :  export EASYBUILD_PREFIX=$HOME/.local/easybuild\nexport EASYBUILD_MODULES_TOOL=Lmod\nexport EASYBUILD_MODULE_NAMING_SCHEME=CategorizedModuleNamingScheme\n# Use the below variable to run:\n#    module use $LOCAL_MODULES\n#    module load tools/EasyBuild\nexport LOCAL_MODULES=${EASYBUILD_PREFIX}/modules/all  Then source this file to expose the environment variables:  $> source ~/.bashrc\n$> echo $EASYBUILD_PREFIX\n/home/users/svarrette/.local/easybuild  Now let's install Easybuild following the  boostrapping procedure  $> cd /tmp/\n# download script\ncurl -o /tmp/bootstrap_eb.py  https://raw.githubusercontent.com/hpcugent/easybuild-framework/develop/easybuild/scripts/bootstrap_eb.py\n\n# install Easybuild\n$> python /tmp/bootstrap_eb.py $EASYBUILD_PREFIX\n\n# Load it\n$> echo $MODULEPATH\n$> module use $LOCAL_MODULES\n$> echo $MODULEPATH\n$> module spider Easybuild\n$> module load tools/EasyBuild",
            "title": "Pre-requisite: Installing Easybuild"
        },
        {
            "location": "/advanced/Spark/#search-for-an-easybuild-recipe-for-spark",
            "text": "$> eb -S Spark\n# Try to install the most recent version\n$> eb Spark-2.0.2.eb -Dr    # Dry-run\n$> eb Spark-2.0.2.eb -r  This is going to fail because of the Java dependency which is unable to build.\nSo we are going to create a custom easyconfig file  $> mkdir -p ~/tutorials/Spark\n$> cd ~/tutorials/Spark\n# Check the source easyconfig file\n$> eb -S Spark\n$> cp <path/to>/easyconfigs/s/Spark/Spark-2.0.2.eb  Spark-2.0.2.custom.eb  Modify it to ensure a successful build.  --- <path/to>/easyconfigs/s/Spark/Spark-2.0.2.eb 2017-06-12 22:16:14.353929000 +0200\n+++ Spark-2.0.2.custom.eb 2017-06-12 22:39:59.155061000 +0200\n@@ -15,7 +15,7 @@\n     'http://www.us.apache.org/dist/%(namelower)s/%(namelower)s-%(version)s/',\n ]\n\n-dependencies = [('Java', '1.7.0_80')]\n+dependencies = [('Java', '1.8.0_121')]\n\n sanity_check_paths = {\n     'files': ['bin/spark-shell'],  Build it and load the module  $> eb ./Spark-2.0.2.custom.eb\n$> module spider Spark\n$> module load devel/Spark",
            "title": "Search for an Easybuild Recipe for Spark"
        },
        {
            "location": "/advanced/Spark/#interactive-usage",
            "text": "PySpark is the Spark Python API and exposes Spark Contexts to the Python programming environment. Use  --exclusive  to allocate an exclusive node, load the spark module, then run the Python Spark shell:  $> si --exclusive --ntasks=1 --cpus-per-task=28\n$> module use $LOCAL_MODULE\n$> module load devel/Spark\n$> pyspark  After some initialization output, you will see the following:  $> pyspark\nPython 2.7.5 (default, Nov  6 2016, 00:28:07)\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel).\n17/06/12 23:59:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.2\n      /_/\n\nUsing Python version 2.7.5 (default, Nov  6 2016 00:28:07)\nSparkSession available as 'spark'.\n>>>  See  tutorial  for playing with pyspark.",
            "title": "Interactive usage"
        },
        {
            "location": "/advanced/Spark/#scala-spark-shell",
            "text": "Spark includes a modified version of the Scala shell that can be used interactively. Instead of running  pyspark  above, run the  spark-shell  command:  $> spark-shell  After some initialization output, a scala shell prompt with the Spark context will be available:  Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel).\n17/06/13 00:06:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/06/13 00:06:43 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.\nSpark context Web UI available at http://172.17.7.30:4040\nSpark context available as 'sc' (master = local[*], app id = local-1497305203669).\nSpark session available as 'spark'.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.0.2\n      /_/\n\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_121)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala>",
            "title": "Scala Spark Shell"
        },
        {
            "location": "/advanced/Spark/#r-spark-shell",
            "text": "The Spark R API is still experimental. Only a subset of the R API is available -- See the  SparkR Documentation .  Load one of the R modules and then run the SparkR shell:  $> module load lang/R\n$> sparkR\n\nR version 3.4.0 (2017-04-21) -- \"You Stupid Darkness\"\nCopyright (C) 2017 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\nLaunching java with spark-submit command /home/users/svarrette/.local/easybuild/software/devel/Spark/2.0.2/bin/spark-submit   \"sparkr-shell\" /tmp/Rtmphb0s8J/backend_port180ad365cc6b5\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel).\n17/06/13 00:08:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n Welcome to\n    ____              __\n   / __/__  ___ _____/ /__\n  _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version  2.0.2\n    /_/\n\n\n SparkSession available as 'spark'.\n>",
            "title": "R Spark Shell"
        },
        {
            "location": "/advanced/Spark/#running-spark-standalone-cluster",
            "text": "Reference Documentation   Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).  Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark\u2019s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.   There are several useful things to note about this architecture:   Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system.  Spark is agnostic to the underlying cluster manager. As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN).  The driver program must listen for and accept incoming connections from its executors throughout its lifetime (e.g., see spark.driver.port in the network config section). As such, the driver program must be network addressable from the worker nodes.  Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. If you'd like to send requests to the cluster remotely, it's better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes.   Cluster Manager  Spark currently supports three cluster managers:   Standalone  \u2013 a simple cluster manager included with Spark that makes it easy to set up a cluster.  Apache Mesos  \u2013 a general cluster manager that can also run Hadoop MapReduce and service applications.  Hadoop YARN  \u2013 the resource manager in Hadoop 2.   In this session, we will deploy a  standalone cluster .  We will prepare a launcher script that will:   create a master and the workers  submit a spark application to the cluster using the  spark-submit  script  Let the application run and collect the result  stop the cluster at the end.   To facilitate these steps, Spark comes with a couple of scripts you can use to launch or stop your cluster, based on Hadoop's deploy scripts, and available in  $EBROOTSPARK/sbin :     Script  Description      sbin/start-master.sh  Starts a master instance on the machine the script is executed on.    sbin/start-slaves.sh  Starts a slave instance on each machine specified in the conf/slaves file.    sbin/start-slave.sh  Starts a slave instance on the machine the script is executed on.    sbin/start-all.sh  Starts both a master and a number of slaves as described above.    sbin/stop-master.sh  Stops the master that was started via the bin/start-master.sh script.    sbin/stop-slaves.sh  Stops all slave instances on the machines specified in the conf/slaves file.    sbin/stop-all.sh  Stops both the master and the slaves as described above.     We are now going to prepare a launcher scripts to permit passive runs (typically in the  {default | batch}  queue).\nWe will place them in a separate directory as it will host the outcomes of the executions on the UL HPC platform .\nCopy and adapt the  default SLURM launcher  you should have a copy in  ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh  $> cd ~/tutorials/Spark/\n$> cp ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh launcher-spark-pi.sh  The launcher will be organized as follows.  We will first exclusively allocate 2 nodes  #!/bin/bash -l\n# Time-stamp: <Sun 2017-06-11 22:13 svarrette>\n##################################################################\n#SBATCH -N 1\n# Exclusive mode is recommended for all spark jobs\n#SBATCH --exclusive\n#SBATCH --ntasks-per-node 1\n### -c, --cpus-per-task=<ncpus>\n###     (multithreading) Request that ncpus be allocated per process\n#SBATCH -c 28\n#SBATCH --time=0-01:00:00   # 1 hour\n#\n#          Set the name of the job\n#SBATCH -J SparkMaster\n#          Passive jobs specifications\n#SBATCH --partition=batch\n#SBATCH --qos qos-batch  Then we will load the custom module  # Use the RESIF build modules\nif [ -f  /etc/profile ]; then\n   .  /etc/profile\nfi\n\n# Load the {intel | foss} toolchain and whatever module(s) you need\nmodule purge\nmodule use $HOME/.local/easybuild/modules/all\nmodule load devel/Spark\n\nexport SPARK_HOME=$EBROOTSPARK  Then start the Spark master and worker daemons using the Spark scripts  # sbin/start-master.sh - Starts a master instance on the machine the script is executed on.\n$SPARK_HOME/sbin/start-all.sh\n\nexport MASTER=spark://$HOSTNAME:7077\n\necho\necho \"========= Spark Master ========\"\necho $MASTER\necho \"===============================\"\necho  Now we can submit an example python Pi estimation script to the Spark cluster with 100 partitions  Note : partitions in this context refers of course to Spark's Resilient Distributed Dataset (RDD) and how the dataset is distributed across the nodes in the Spark cluster.  spark-submit --master $MASTER  $SPARK_HOME/examples/src/main/python/pi.py 100  Finally, at the end, clean your environment and  # sbin/stop-master.sh - Stops the master that was started via the bin/start-master.sh script.\n$SPARK_HOME/sbin/stop-all.sh  When the job completes, you can find the Pi estimation result in the slurm output file:  $> grep Pi slurm-2853.out\nPi is roughly 3.147861",
            "title": "Running Spark standalone cluster"
        },
        {
            "location": "/advanced/advanced_workflows/README/",
            "text": "Advanced workflows on sequential jobs management\n\n\nPrerequisites\n\n\nMake sure you have followed the tutorial \"Getting started\" and \"HPC workflow with sequential jobs\".\n\n\nIntro\n\n\nDuring this session, we will review various advanced features of OAR:\n\n\n\n\nExercise 1: Advanced OAR features: container, array jobs\n\n\nExercise 2: Best effort jobs\n\n\nExercise 3: Checkpoint restart\n\n\n\n\nWe will use the following github repositories:\n\n\n\n\nULHPC/launcher-scripts\n\n\nULHPC/tutorials\n\n\n\n\nContainer\n\n\nConnect the the cluster access node, and set-up the environment for this tutorial\n\n\n(yourmachine)$> ssh chaos-cluster\n\n\n\nIf your network connection is unstable, use \nscreen\n:\n\n\n(access)$> screen\n\n\n\nWe will use 2 directory:\n\n\n\n\n$HOME\n: default home directory, \nbacked up\n, maximum 50GB, for important files\n\n\n$WORK\n: work directory, \nnon backed up\n, maximum 500GB\n\n\n\n\nCreate a sub directory $WORK/PS2, and work inside it\n\n\n(access)$> mkdir $WORK/PS6\n(access)$> cd $WORK/PS6\n\n\n\nIn the following parts, we will assume that you are working in this directory.\n\n\nClone the repositories \nULHPC/tutorials\n and \nULHPC/launcher-scripts.git\n\n\n(access)$> git clone https://github.com/ULHPC/launcher-scripts.git\n(access)$> git clone https://github.com/ULHPC/tutorials.git\n\n\n\nExercise 1: useful oar features\n\n\nContainer\n\n\nWith OAR, it is possible to execute jobs within another one.\nThis functionality is called \ncontainer jobs\n.\n\n\nIt can be used to ensure you will have a pool of nodes in advance.\nIn example, if you want to create a container on the 29th of June, with 4 nodes, available during 10 hours\n\n\noarsub -t container -r \"2015-06-29 09:00:00\" -l nodes=4,walltime=10:00:00\n\n\n\nFor testing purposes, you can create a small and short container with a passive job\n\n\noarsub -t container -l nodes=2,walltime=0:30:00 \"sleep 1800\"\n[ADMISSION RULE] Modify resource description with type constraints\nOAR_JOB_ID=1428304\n\n\n\nOAR gives you the job id of your container, you can submit jobs inside this container with the parameter \n-t inner=<container id>\n\n\noarsub -I -t inner=1428304\n\n\n\nNote that an inner job can not be a reservation (ie. it cannot overlap the container reservation).\n\n\nArray job\n\n\nLet's create an array of 5 jobs. \nWith an array, in one oarsub command, you can submit N sub jobs.\n\n\noarsub --array 5 -n array_job_test -l /core=1 $WORK/PS6/tutorials/advanced/advanced_workflows/scripts/array_job.sh\n\nOAR_JOB_ID=1430652\nOAR_JOB_ID=1430653\nOAR_JOB_ID=1430654\nOAR_JOB_ID=1430655\nOAR_JOB_ID=1430656\nOAR_ARRAY_ID=1430652U\n\n\n\nThe command is started with the environment variable \"OAR_ARRAY_INDEX\".\n\n\nUsing this index, you can split your work load in N batches.\n\n\nIf you look at the output in the OAR stdout files, you should read something like that:\n\n\ncat OAR.array_job_test.*\n\nhostname :  s-cluster1-13\nOAR_JOB_NAME    : array_job_test\nOAR_JOB_ID      : 1430652\nOAR_ARRAY_ID    : 1430652\nOAR_ARRAY_INDEX : 1\nhostname :  s-cluster1-13\nOAR_JOB_NAME    : array_job_test\nOAR_JOB_ID      : 1430653\nOAR_ARRAY_ID    : 1430652\nOAR_ARRAY_INDEX : 2\nhostname :  s-cluster1-13\nOAR_JOB_NAME    : array_job_test\nOAR_JOB_ID      : 1430654\nOAR_ARRAY_ID    : 1430652\nOAR_ARRAY_INDEX : 3\nhostname :  s-cluster1-13\nOAR_JOB_NAME    : array_job_test\nOAR_JOB_ID      : 1430655\nOAR_ARRAY_ID    : 1430652\nOAR_ARRAY_INDEX : 4\nhostname :  s-cluster1-13\nOAR_JOB_NAME    : array_job_test\nOAR_JOB_ID      : 1430656\nOAR_ARRAY_ID    : 1430652\nOAR_ARRAY_INDEX : 5\n\n\n\nNote: array jobs can neither be Interactive (-I) nor a reservation (-r).\n\n\nExercise 2: Best effort job\n\n\nBy default, your jobs end in the default queue meaning they have all equivalent priority. \nYou can also decide to create so called best-effort jobs which are scheduled in the besteffort queue.\nTheir particularity is that they are deleted if another not besteffort job wants resources where they are running.\n\n\nThe main advantage is that besteffort jobs allows you to bypass the limit on the default queue (number of jobs and walltime).\n\n\nIn order to submit a besteffort job, you must append the parameter \"-t besteffort\" to your oarsub command.\nHere is an example:\n\n\noarsub -t besteffort /path/to/prog\n\n\n\nThe probability of your best effort job being killed increases with the walltime of the job.\nWe strongly advise to send best effort job with small walltimes (in example 2 hours), \nand just resubmit your job if it is killed.\n\n\nYou can do that automatically with the \nidempotent\n type.\n\n\noarsub -t besteffort -t idempotent /path/to/prog\n\n\n\nIf your idempotent job is killed by the OAR scheduler, then another job is automatically \ncreated and scheduled with same configuration.\nAdditionally, your job is also resubmitted if the exit code of your program is 99.\n\n\nOur best effort launcher script implements this mechanism.\n\n\nWe will now submit a besteffort job using the script \nlauncher_besteffort.sh\n.\nThis job will execute a R script computing 130 Fibonacci numbers, \nnote that the script sleep 2 seconds in each iteration in order to simulate a long running job.\n\n\nEdit the file \n$WORK/PS6/launcher-scripts/bash/besteffort/launcher_besteffort.sh\n\n\nIn the \"Job settings\" section, load R by adding this line:\n\n\nmodule load lang/R\n\n\n\nChange the \n$TASK\n variable:\n\n\nTASK=\"Rscript $WORK/PS6/tutorials/advanced/advanced_workflows/scripts/test.R\"\n\n\n\nNow, submit the script on a specific node (choose a random free node)\n\n\noarsub -t besteffort -t idempotent -l nodes=1 -p \"network_address='h-cluster1-6'\" $WORK/PS6/launcher-scripts/bash/besteffort/launcher_besteffort.sh\n\noarstat -u\n\n\n\nOnce the script is started, submit a normal interactive job on the same node, in order to force OAR to kill the besteffort job\n\n\noarsub -I -l nodes=1 -p \"network_address='h-cluster1-6'\"\n\noarstat -u\n\n\n\nQ: What do you observe ?\n\n\nExercise 3: Checkpointing\n\n\nCheckpointing\n is a technique which consists of storing a snapshot of the current application state,\nin order to re-use it later for restarting the execution.\nCheckpointing your jobs brings the following features:\n\n\n\n\nthe job can overcome the limits of the default queue, especially the maximum walltime\n\n\nfault tolerance, your job can survive even if the node crash\n\n\n\n\nYou can implement a checkpointing mechanism yourself in your applications, or you can try with BLCR.\nIn all cases, your checkpointing mechanism must be chosen on a case by case basis, BLCR is not the only solution. \nNote that BLCR has some limitations, especially with sockets and network communication.\n\n\nIn this exercise, we will use BLCR with MATLAB.\n\n\nBLCR can be used with 3 commands:\n\n\n\n\ncr_run\n is a wrapper for your application\n\n\ncr_checkpoint\n will stop the application and save its state in a context file\n\n\ncr_restart\n will restart the application based on a context file\n\n\n\n\nInteractive example with Matlab\n\n\nCreate an interactive job, start a matlab program \ncr_run\n\n\n(access) oarsub -I\n\n(node) screen\n\n(node) module load base/MATLAB\n(node) export LIBCR_DISABLE_NSCD=YES\n(node) cr_run matlab -nojvm -nodisplay -nosplash <  $WORK/PS6/tutorials/advanced/advanced_workflows/scripts/test.m &\n\n\n\nGet the process id of matlab\n\n\n(node) echo $!\n\n\n\nCreate a new window inside your screen with \"Ctrl-a c\".\n\n\nLook at the process table on the node\n\n\n(node) ps aux | grep MATLAB\n\n\n\nStop the matlab process and register its state in the file \n/tmp/test.context\n\n\n(node) cr_checkpoint -f /tmp/test.context --kill -T <Matlab process id>\n\n\n\nMATLAB should not be in the process table now\n\n\n(node) ps aux | grep MATLAB\n\n\n\nRestart the process from the context file\n\n\n(node) cr_restart --no-restore-pid /tmp/test.context\n\n\n\nThe MATLAB process has been restarted\n\n\n(node) ps aux | grep MATLAB\n\n\n\nOAR integration\n\n\nCheckpointing is supported by OAR, by combining several features:\n\n\n\n\nbesteffort\n jobs: described in the previous exercise\n\n\nidempotent\n jobs: if your processus returns an exit code equal to 99, your job will be resubmitted with the same parameters ;\n\n\ncheckpoint parameter\n: enable the checkpointing mechanism, specifies the time in seconds before sending a signal to the first processus of the job ;\n\n\nsignal parameter\n: specify which signal to use when checkpointing (default is SIGUSR2).\n\n\n\n\nLet's start a long Matlab program with the launcher \nlauncher_checkpoint_restart.sh\n which uses all these features.\n\n\nFirst in \n$WORK/PS6/launcher-scripts/bash/besteffort/launcher_checkpoint_restart.sh\n, modify the variable \nTASK\n with your program:\n\n\nTASK=\"matlab -nojvm -nodisplay -nosplash -r run('$WORK/PS6/tutorials/advanced/advanced_workflows/scripts/test.m');exit;\"\n\n\n\nIn the \"Job settings\" section, load the Matlab module by adding this line:\n\n\nmodule load base/MATLAB\n\n\n\nSubmit your job with this command\n\n\noarsub --checkpoint 30 --signal 12 -l walltime=00:02:00 -t besteffort -t idempotent $WORK/PS6/launcher-scripts/bash/besteffort/launcher_checkpoint_restart.sh\n\n\n\nLook at the OAR output in the stdout file and monitor the running jobs with oarstat.\n\n\nNote that BLCR restore the full context of the job, including the file descriptors.\nTherefore, the output of MATLAB is always sent to the stdout file of the initial job.\n\n\nPlease, clean your files\n\n\ncd $WORK\nrm -rf PS6",
            "title": "Advanced workflows on sequential jobs management"
        },
        {
            "location": "/advanced/advanced_workflows/README/#advanced-workflows-on-sequential-jobs-management",
            "text": "",
            "title": "Advanced workflows on sequential jobs management"
        },
        {
            "location": "/advanced/advanced_workflows/README/#prerequisites",
            "text": "Make sure you have followed the tutorial \"Getting started\" and \"HPC workflow with sequential jobs\".",
            "title": "Prerequisites"
        },
        {
            "location": "/advanced/advanced_workflows/README/#intro",
            "text": "During this session, we will review various advanced features of OAR:   Exercise 1: Advanced OAR features: container, array jobs  Exercise 2: Best effort jobs  Exercise 3: Checkpoint restart   We will use the following github repositories:   ULHPC/launcher-scripts  ULHPC/tutorials",
            "title": "Intro"
        },
        {
            "location": "/advanced/advanced_workflows/README/#container",
            "text": "",
            "title": "Container"
        },
        {
            "location": "/advanced/advanced_workflows/README/#connect-the-the-cluster-access-node-and-set-up-the-environment-for-this-tutorial",
            "text": "(yourmachine)$> ssh chaos-cluster  If your network connection is unstable, use  screen :  (access)$> screen  We will use 2 directory:   $HOME : default home directory,  backed up , maximum 50GB, for important files  $WORK : work directory,  non backed up , maximum 500GB   Create a sub directory $WORK/PS2, and work inside it  (access)$> mkdir $WORK/PS6\n(access)$> cd $WORK/PS6  In the following parts, we will assume that you are working in this directory.  Clone the repositories  ULHPC/tutorials  and  ULHPC/launcher-scripts.git  (access)$> git clone https://github.com/ULHPC/launcher-scripts.git\n(access)$> git clone https://github.com/ULHPC/tutorials.git",
            "title": "Connect the the cluster access node, and set-up the environment for this tutorial"
        },
        {
            "location": "/advanced/advanced_workflows/README/#exercise-1-useful-oar-features",
            "text": "",
            "title": "Exercise 1: useful oar features"
        },
        {
            "location": "/advanced/advanced_workflows/README/#container_1",
            "text": "With OAR, it is possible to execute jobs within another one.\nThis functionality is called  container jobs .  It can be used to ensure you will have a pool of nodes in advance.\nIn example, if you want to create a container on the 29th of June, with 4 nodes, available during 10 hours  oarsub -t container -r \"2015-06-29 09:00:00\" -l nodes=4,walltime=10:00:00  For testing purposes, you can create a small and short container with a passive job  oarsub -t container -l nodes=2,walltime=0:30:00 \"sleep 1800\"\n[ADMISSION RULE] Modify resource description with type constraints\nOAR_JOB_ID=1428304  OAR gives you the job id of your container, you can submit jobs inside this container with the parameter  -t inner=<container id>  oarsub -I -t inner=1428304  Note that an inner job can not be a reservation (ie. it cannot overlap the container reservation).",
            "title": "Container"
        },
        {
            "location": "/advanced/advanced_workflows/README/#array-job",
            "text": "Let's create an array of 5 jobs. \nWith an array, in one oarsub command, you can submit N sub jobs.  oarsub --array 5 -n array_job_test -l /core=1 $WORK/PS6/tutorials/advanced/advanced_workflows/scripts/array_job.sh\n\nOAR_JOB_ID=1430652\nOAR_JOB_ID=1430653\nOAR_JOB_ID=1430654\nOAR_JOB_ID=1430655\nOAR_JOB_ID=1430656\nOAR_ARRAY_ID=1430652U  The command is started with the environment variable \"OAR_ARRAY_INDEX\".  Using this index, you can split your work load in N batches.  If you look at the output in the OAR stdout files, you should read something like that:  cat OAR.array_job_test.*\n\nhostname :  s-cluster1-13\nOAR_JOB_NAME    : array_job_test\nOAR_JOB_ID      : 1430652\nOAR_ARRAY_ID    : 1430652\nOAR_ARRAY_INDEX : 1\nhostname :  s-cluster1-13\nOAR_JOB_NAME    : array_job_test\nOAR_JOB_ID      : 1430653\nOAR_ARRAY_ID    : 1430652\nOAR_ARRAY_INDEX : 2\nhostname :  s-cluster1-13\nOAR_JOB_NAME    : array_job_test\nOAR_JOB_ID      : 1430654\nOAR_ARRAY_ID    : 1430652\nOAR_ARRAY_INDEX : 3\nhostname :  s-cluster1-13\nOAR_JOB_NAME    : array_job_test\nOAR_JOB_ID      : 1430655\nOAR_ARRAY_ID    : 1430652\nOAR_ARRAY_INDEX : 4\nhostname :  s-cluster1-13\nOAR_JOB_NAME    : array_job_test\nOAR_JOB_ID      : 1430656\nOAR_ARRAY_ID    : 1430652\nOAR_ARRAY_INDEX : 5  Note: array jobs can neither be Interactive (-I) nor a reservation (-r).",
            "title": "Array job"
        },
        {
            "location": "/advanced/advanced_workflows/README/#exercise-2-best-effort-job",
            "text": "By default, your jobs end in the default queue meaning they have all equivalent priority. \nYou can also decide to create so called best-effort jobs which are scheduled in the besteffort queue.\nTheir particularity is that they are deleted if another not besteffort job wants resources where they are running.  The main advantage is that besteffort jobs allows you to bypass the limit on the default queue (number of jobs and walltime).  In order to submit a besteffort job, you must append the parameter \"-t besteffort\" to your oarsub command.\nHere is an example:  oarsub -t besteffort /path/to/prog  The probability of your best effort job being killed increases with the walltime of the job.\nWe strongly advise to send best effort job with small walltimes (in example 2 hours), \nand just resubmit your job if it is killed.  You can do that automatically with the  idempotent  type.  oarsub -t besteffort -t idempotent /path/to/prog  If your idempotent job is killed by the OAR scheduler, then another job is automatically \ncreated and scheduled with same configuration.\nAdditionally, your job is also resubmitted if the exit code of your program is 99.  Our best effort launcher script implements this mechanism.  We will now submit a besteffort job using the script  launcher_besteffort.sh .\nThis job will execute a R script computing 130 Fibonacci numbers, \nnote that the script sleep 2 seconds in each iteration in order to simulate a long running job.  Edit the file  $WORK/PS6/launcher-scripts/bash/besteffort/launcher_besteffort.sh  In the \"Job settings\" section, load R by adding this line:  module load lang/R  Change the  $TASK  variable:  TASK=\"Rscript $WORK/PS6/tutorials/advanced/advanced_workflows/scripts/test.R\"  Now, submit the script on a specific node (choose a random free node)  oarsub -t besteffort -t idempotent -l nodes=1 -p \"network_address='h-cluster1-6'\" $WORK/PS6/launcher-scripts/bash/besteffort/launcher_besteffort.sh\n\noarstat -u  Once the script is started, submit a normal interactive job on the same node, in order to force OAR to kill the besteffort job  oarsub -I -l nodes=1 -p \"network_address='h-cluster1-6'\"\n\noarstat -u  Q: What do you observe ?",
            "title": "Exercise 2: Best effort job"
        },
        {
            "location": "/advanced/advanced_workflows/README/#exercise-3-checkpointing",
            "text": "Checkpointing  is a technique which consists of storing a snapshot of the current application state,\nin order to re-use it later for restarting the execution.\nCheckpointing your jobs brings the following features:   the job can overcome the limits of the default queue, especially the maximum walltime  fault tolerance, your job can survive even if the node crash   You can implement a checkpointing mechanism yourself in your applications, or you can try with BLCR.\nIn all cases, your checkpointing mechanism must be chosen on a case by case basis, BLCR is not the only solution. \nNote that BLCR has some limitations, especially with sockets and network communication.  In this exercise, we will use BLCR with MATLAB.  BLCR can be used with 3 commands:   cr_run  is a wrapper for your application  cr_checkpoint  will stop the application and save its state in a context file  cr_restart  will restart the application based on a context file",
            "title": "Exercise 3: Checkpointing"
        },
        {
            "location": "/advanced/advanced_workflows/README/#interactive-example-with-matlab",
            "text": "Create an interactive job, start a matlab program  cr_run  (access) oarsub -I\n\n(node) screen\n\n(node) module load base/MATLAB\n(node) export LIBCR_DISABLE_NSCD=YES\n(node) cr_run matlab -nojvm -nodisplay -nosplash <  $WORK/PS6/tutorials/advanced/advanced_workflows/scripts/test.m &  Get the process id of matlab  (node) echo $!  Create a new window inside your screen with \"Ctrl-a c\".  Look at the process table on the node  (node) ps aux | grep MATLAB  Stop the matlab process and register its state in the file  /tmp/test.context  (node) cr_checkpoint -f /tmp/test.context --kill -T <Matlab process id>  MATLAB should not be in the process table now  (node) ps aux | grep MATLAB  Restart the process from the context file  (node) cr_restart --no-restore-pid /tmp/test.context  The MATLAB process has been restarted  (node) ps aux | grep MATLAB",
            "title": "Interactive example with Matlab"
        },
        {
            "location": "/advanced/advanced_workflows/README/#oar-integration",
            "text": "Checkpointing is supported by OAR, by combining several features:   besteffort  jobs: described in the previous exercise  idempotent  jobs: if your processus returns an exit code equal to 99, your job will be resubmitted with the same parameters ;  checkpoint parameter : enable the checkpointing mechanism, specifies the time in seconds before sending a signal to the first processus of the job ;  signal parameter : specify which signal to use when checkpointing (default is SIGUSR2).   Let's start a long Matlab program with the launcher  launcher_checkpoint_restart.sh  which uses all these features.  First in  $WORK/PS6/launcher-scripts/bash/besteffort/launcher_checkpoint_restart.sh , modify the variable  TASK  with your program:  TASK=\"matlab -nojvm -nodisplay -nosplash -r run('$WORK/PS6/tutorials/advanced/advanced_workflows/scripts/test.m');exit;\"  In the \"Job settings\" section, load the Matlab module by adding this line:  module load base/MATLAB  Submit your job with this command  oarsub --checkpoint 30 --signal 12 -l walltime=00:02:00 -t besteffort -t idempotent $WORK/PS6/launcher-scripts/bash/besteffort/launcher_checkpoint_restart.sh  Look at the OAR output in the stdout file and monitor the running jobs with oarstat.  Note that BLCR restore the full context of the job, including the file descriptors.\nTherefore, the output of MATLAB is always sent to the stdout file of the initial job.",
            "title": "OAR integration"
        },
        {
            "location": "/advanced/advanced_workflows/README/#please-clean-your-files",
            "text": "cd $WORK\nrm -rf PS6",
            "title": "Please, clean your files"
        },
        {
            "location": "/advanced/advanced_scheduling/",
            "text": "-\n- mode: markdown;mode:visual-line;  fill-column: 80 -\n-\n\n\nCopyright (c) 2017 \nULHPC management team\n \nwww\n\n\n\n\nUL HPC Tutorial: Advanced scheduling with SLURM\n\n\n \n \n \n \n \n \n\n\n\n\nThe objective of this tutorial is to practice using the SLURM cluster\nworkload manager in use on the UL HPC \niris cluster\n.\n\n\nIt's important that you read the \nslides\n first.\n\n\nThey review, for iris:\n\n\n\n\nthe way SLURM was configured, accounting and permissions\n\n\ncommon and advanced SLURM tools and commands\n\n\nSLURM job types\n\n\nSLURM generic launchers you can use as a base for your own jobs\n\n\na comparison of SLURM (iris cluster) and OAR (gaia and chaos)\n\n\n\n\nPart one\n\n\nYou will now get familiar, if not already, with the main tools part of SLURM (otherwise skip down to \nPart two\n).\n\n\nYou must first connect to the iris cluster frontend, e.g. with \nssh yourlogin@access-iris.uni.lu -p 8022\n.\n\n\nNotes\n:\n\n\n\n\nAll the commands used have detailed manuals (\nman $toolname\n) that you can always refer to.\n\n\nTo make interactive jobs easier to launch, a function \nsi\n exists that starts an interactive job with your parameters and the qos-interactive QOS\n\n\nYou can override it in your own \n~/.bashrc\n with aliases that customizes it for your particular needs, e.g.\n\n\nalias si='srun -p interactive --qos qos-interactive --time=0:30:0 --pty bash -i'\n\n\nalias si='srun -p interactive --qos qos-interactive-001 --pty bash -i'\n\n\nalias six='srun -p interactive --qos qos-interactive --x11 --pty bash -i'\n\n\n\n\n\n\nUsers that are part of groups with access to dedicated QOS should use their specific QOS when launching jobs below (e.g. \nqos-interactive-001\n)\n\n\n\n\nPartition (queue) and node status\n\n\n\n\nShow queued jobs, show more details ('long' view that includes the job time limit):\n\n\n\n\nsqueue\nsqueue -l\n\n\n\n\nQuestion\n: are all jobs visible by default? what does \nsqueue -a\n do?\n\n\n\n\nShow only the queued jobs of your user (\n$USER\n is an environment variable in your shell), then for another specific user:\n\n\n\n\nsqueue -u $USER\nsqueue -u vplugaru\n\n\n\n\n\n\nShow queued jobs in a specific partition:\n\n\n\n\nsqueue -p $partition\n\n\n\n\n\n\nShow queued jobs that are in a specific state (pending / running / failed / preempted, see \nman squeue\n for all available states):\n\n\n\n\nsqueue -t PD\nsqueue -t R\nsqueue -t F\nsqueue -t PR\n\n\n\n\nQuestion\n: what other job states exist?\n\n\n\n\nShow partition status, summarized status (without node state), and node-oriented partition status:\n\n\n\n\nsinfo\nsinfo -s\nsinfo -N\n\n\n\n\nQuestions\n: What does the 'mix' state in the output of \nsinfo\n signify?  What will happen to your jobs if the nodes are 'down' or 'drain'? What will you see when looking at a job with \nsqueue\n or \nscontrol show job\n ?\n\n\n\n\nShow node reservations that have been created by the administrators for specific users or accounts:\n\n\n\n\nsinfo -T\n\n\n\n\n\n\nShow node details (all nodes, specific node):\n\n\n\n\nscontrol show nodes\nscontrol show nodes $nodename\n\n\n\n\n\n\nCheck the default account your jobs will use:\n\n\n\n\nsacctmgr show user $USER format=user%20s,defaultaccount%30s\n\n\n\n\n\n\nSee all account associations for your user and the QOS they grant access to:\n\n\n\n\nsacctmgr list association where users=$USER format=account%30s,user%20s,qos%120s\n\n\n\n\nJob submission and management\n\n\nStarting interactive jobs\n\n\n\n\nStart an interactive job with the default number of cores and walltime:\n\n\n\n\nsrun -p interactive --qos qos-interactive --pty bash -i\n\n\n\n\nQuestion\n: now before exiting the job, what does \nenv | grep SLURM\n give out? What is the walltime for this job?\n\n\n\n\nStart an interactive job for 3 minutes, with 2 nodes and 4 tasks per node:\n\n\n\n\nsrun -p interactive --qos qos-interactive --time=0:03:0 -N 2 --ntasks-per-node=4 --pty bash -i\n\n\n\n\nQuestion\n: can you ssh between the nodes part of this job? what happens if you try to ssh to a different node (not from your job/jobs)? if you are still connected to the job after 3 minutes, what happens?\n\n\n\n\nStart an interactive job with \nX11 forwarding\n such that GUI applications (running in the cluster) will be shown on your workstation:\n\n\nnote that your initial connection to the iris cluster needs to have X11 Forwarding enabled, e.g. \nssh -X iris-cluster\n\n\n\n\n\n\n\n\nsrun -p interactive --qos qos-interactive --pty -x11 bash -i\n\n\n\n\nQuestion\n: what happens if you launch a graphical application (e.g. \nxterm\n)? did it appear on your own machine? if not, what went wrong?\n\n\n\n\nStart a best-effort interactive job (can be interrupted by regular jobs if other users submit them):\n\n\n\n\nsrun -p interactive --qos qos-besteffort --pty bash -i\n\n\n\n\nQuestion\n: can you make this job be preempted? try to allocate other nodes in the interactive partition and see what happens when the besteffort job is marked for preemption.\n\n\nCollecting job information\n\n\nNow start a job with one of the previous commands, and you will check its details (runtime metrics, status, after execution statistics, etc.).\n\n\n\n\nShow the details of a job:\n\n\n\n\nscontrol show job $jobid\n\n\n\n\nQuestion\n: what happens if you try to take a look at a job which is not in the queue (waiting/running) anymore (e.g. \nscontrol show job 2\n)?\n\n\n\n\nCheck waiting job priority (detailed view):\n\n\n\n\nsprio -l\n\n\n\n\n\n\nCheck expected job start time:\n\n\n\n\nsqueue --start -u $USER\n\n\n\n\n\n\nShow running job (and steps) system-level utilization (memory, I/O, energy):\n\n\nnote that \nsstat\n information is limited to your own jobs\n\n\n\n\n\n\n\n\nsstat -j $jobid\n\n\n\n\n\n\nShow specific statistics from a running job (and steps) or multiple jobs:\n\n\n\n\nsstat -j $jobid --format=AveCPU,AveRSS,AveVMSize,MaxRSS,MaxVMSize\nsstat -j $jobid1,$jobid2 --format=AveCPU,AveRSS,AveVMSize,MaxRSS,MaxVMSize\n\n\n\n\n\n\nOutput the statistics in a parseable format, delimited by \n|\n (with, then without trailing \n|\n):\n\n\n\n\nsstat -p -j $jobid --format=AveCPU,AveRSS,AveVMSize,MaxRSS,MaxVMSize\nsstat -P -j $jobid --format=AveCPU,AveRSS,AveVMSize,MaxRSS,MaxVMSize\n\n\n\n\n\n\nShow running or completed job (and steps) system-level utilization from the accounting information, and with full details:\n\n\n\n\nsacct -j $jobid\nsacct -j $jobid -l\n\n\n\n\nQuestion\n: remember that job id #2? can we see its information with \nsacct -j 2 --format=user\n and \nsacct -j 2 -l\n ?\n\n\n\n\nShow statistics relevant to the job allocation itself not taking steps into consideration, and with more details:\n\n\n\n\nsacct -X -j $jobid\nsacct -X -j $jobid -l\n\n\n\n\n\n\nShow a subset of interesting statistics from a completed job and its steps, including:\n\n\nelapsed time in both human readable and total # of seconds\n\n\nmaximum resident set size of all tasks in job (you may want to add also \nmaxrssnode\n and \nmaxrsstask\n for a better understanding of which process consumed memory)\n\n\nmaximum virtual memory size (idem for \nmaxvmsizenode\n and \nmaxvmsizetask\n)\n\n\nconsumed energy (in Joules), be aware there are many caveats!\n\n\nyour job needs to be the only one running on the corresponding compute nodes\n\n\nthe \nRAPL mechanism\n will not take into account all possible hardware elements which consume power (CPUs, GPUs and DRAM are included)\n\n\n\n\n\n\n\n\n\n\n\n\nsacct -j $jobid --format=account,user,jobid,jobname,partition,state,elapsed,elapsedraw,start,end,maxrss,maxvmsize,consumedenergy,consumedenergyraw,nnodes,ncpus,nodelist\n\n\n\n\n\n\nOutput the same statistics in the parseable \n|\n-delimited format, for a single and multiple jobs:\n\n\n\n\nsacct -p -j $jobid --format=account,user,jobid,jobname,partition,state,elapsed,elapsedraw,start,end,maxrss,maxvmsize,consumedenergy,consumedenergyraw,nnodes,ncpus,nodelist\nsacct -p -j $jobid1,$jobid2 --format=account,user,jobid,jobname,partition,state,elapsed,elapsedraw,start,end,maxrss,maxvmsize,consumedenergy,consumedenergyraw,nnodes,ncpus,nodelist\n\n\n\n\n\n\nShow statistics for all personal jobs started since a particular date, then without job steps:\n\n\n\n\nsacct --starttime 2017-06-12 -u $USER\nsacct -X --starttime 2017-06-12 -u $USER\n\n\n\n\nPausing, resuming and cancelling jobs\n\n\n\n\nTo stop a waiting job from being scheduled and later to allow it to be scheduled:\n\n\n\n\nscontrol hold $jobid\nscontrol release $jobid\n\n\n\n\nQuestion\n: what do you see as \"Reason\" in \nsqueue\n output on a large (say, 10 nodes interactive) job that you submitted and then ran \nscontrol hold $jobid\n on?\n\n\n\n\nTo pause a running job and then resume it:\n\n\n\n\nscontrol suspend $jobid\nscontrol resume $jobid\n\n\n\n\nQuestion\n: what happens when you try to suspend an interactive job?\n\n\n\n\nTo remove a job from the queue (stopping it if already started):\n\n\n\n\nscancel $jobid\n\n\n\n\nQuestion\n: what happens when you cancel a running job, does it get killed immediately?\n\n\n\n\nTo remove a job by name:\n\n\n\n\nscancel --name=$jobname\nscancel -n $jobname\n\n\n\n\nQuestion\n: what happens if you start two jobs with the same name and cancel by name?\n\n\n\n\nTo remove all user jobs:\n\n\n\n\nscancel --user=$USER\nscancel -u $USER\n\n\n\n\n\n\nTo remove all waiting jobs (\npending\n state) for a given user:\n\n\n\n\nscancel --user=$USER --state=pending\nscancel -u $USER -t pending\n\n\n\n\n\n\nTo remove all waiting jobs in a given partition (e.g. \nbatch\n):\n\n\n\n\nscancel -u $USER --partition=batch\nscancel -u $USER -p batch\n\n\n\n\n\n\nTo stop and restart a given job:\n\n\n\n\nscontrol requeue $jobid\n\n\n\n\nQuestion\n: what message do you get when you try to requeue an interactive job?\n\n\nPart two: the harder stuff\n\n\nWe have made available a set of template \nbatch launcher scripts for SLURM\n.\n\n\nYou should now:\n\n\n\n\nadapt the most appropriate one (sequential, parallel, etc.) for your most commonly used HPC application\n\n\nlaunch your own (short execution time) test case, on a single core for sequential code or two distributed cores for parallel code\n\n\ntake note of your fair-share and usage values (e.g. with \nsshare -A $(sacctmgr -n show user $USER format=defaultaccount%30s)\n)\n\n\n\n\n\n\nmonitor job progression:\n\n\nwith \nsprio\n / \nsprio -l\n to see its priority in the queue\n\n\nusing \nsstat\n once it starts, to get running metrics\n\n\nby connecting to the job (\n--jobid\n parameter of \nsrun\n) and using \nhtop\n\n\n\n\n\n\nfinally, once the job finished or you have stopped it:\n\n\ncheck the information visible with \nsacct\n\n\nhow did your fair-share and usage values change?",
            "title": "Advanced scheduling with SLURM"
        },
        {
            "location": "/advanced/advanced_scheduling/#ul-hpc-tutorial-advanced-scheduling-with-slurm",
            "text": "The objective of this tutorial is to practice using the SLURM cluster\nworkload manager in use on the UL HPC  iris cluster .  It's important that you read the  slides  first.  They review, for iris:   the way SLURM was configured, accounting and permissions  common and advanced SLURM tools and commands  SLURM job types  SLURM generic launchers you can use as a base for your own jobs  a comparison of SLURM (iris cluster) and OAR (gaia and chaos)",
            "title": "UL HPC Tutorial: Advanced scheduling with SLURM"
        },
        {
            "location": "/advanced/advanced_scheduling/#part-one",
            "text": "You will now get familiar, if not already, with the main tools part of SLURM (otherwise skip down to  Part two ).  You must first connect to the iris cluster frontend, e.g. with  ssh yourlogin@access-iris.uni.lu -p 8022 .  Notes :   All the commands used have detailed manuals ( man $toolname ) that you can always refer to.  To make interactive jobs easier to launch, a function  si  exists that starts an interactive job with your parameters and the qos-interactive QOS  You can override it in your own  ~/.bashrc  with aliases that customizes it for your particular needs, e.g.  alias si='srun -p interactive --qos qos-interactive --time=0:30:0 --pty bash -i'  alias si='srun -p interactive --qos qos-interactive-001 --pty bash -i'  alias six='srun -p interactive --qos qos-interactive --x11 --pty bash -i'    Users that are part of groups with access to dedicated QOS should use their specific QOS when launching jobs below (e.g.  qos-interactive-001 )",
            "title": "Part one"
        },
        {
            "location": "/advanced/advanced_scheduling/#partition-queue-and-node-status",
            "text": "Show queued jobs, show more details ('long' view that includes the job time limit):   squeue\nsqueue -l  Question : are all jobs visible by default? what does  squeue -a  do?   Show only the queued jobs of your user ( $USER  is an environment variable in your shell), then for another specific user:   squeue -u $USER\nsqueue -u vplugaru   Show queued jobs in a specific partition:   squeue -p $partition   Show queued jobs that are in a specific state (pending / running / failed / preempted, see  man squeue  for all available states):   squeue -t PD\nsqueue -t R\nsqueue -t F\nsqueue -t PR  Question : what other job states exist?   Show partition status, summarized status (without node state), and node-oriented partition status:   sinfo\nsinfo -s\nsinfo -N  Questions : What does the 'mix' state in the output of  sinfo  signify?  What will happen to your jobs if the nodes are 'down' or 'drain'? What will you see when looking at a job with  squeue  or  scontrol show job  ?   Show node reservations that have been created by the administrators for specific users or accounts:   sinfo -T   Show node details (all nodes, specific node):   scontrol show nodes\nscontrol show nodes $nodename   Check the default account your jobs will use:   sacctmgr show user $USER format=user%20s,defaultaccount%30s   See all account associations for your user and the QOS they grant access to:   sacctmgr list association where users=$USER format=account%30s,user%20s,qos%120s",
            "title": "Partition (queue) and node status"
        },
        {
            "location": "/advanced/advanced_scheduling/#job-submission-and-management",
            "text": "",
            "title": "Job submission and management"
        },
        {
            "location": "/advanced/advanced_scheduling/#starting-interactive-jobs",
            "text": "Start an interactive job with the default number of cores and walltime:   srun -p interactive --qos qos-interactive --pty bash -i  Question : now before exiting the job, what does  env | grep SLURM  give out? What is the walltime for this job?   Start an interactive job for 3 minutes, with 2 nodes and 4 tasks per node:   srun -p interactive --qos qos-interactive --time=0:03:0 -N 2 --ntasks-per-node=4 --pty bash -i  Question : can you ssh between the nodes part of this job? what happens if you try to ssh to a different node (not from your job/jobs)? if you are still connected to the job after 3 minutes, what happens?   Start an interactive job with  X11 forwarding  such that GUI applications (running in the cluster) will be shown on your workstation:  note that your initial connection to the iris cluster needs to have X11 Forwarding enabled, e.g.  ssh -X iris-cluster     srun -p interactive --qos qos-interactive --pty -x11 bash -i  Question : what happens if you launch a graphical application (e.g.  xterm )? did it appear on your own machine? if not, what went wrong?   Start a best-effort interactive job (can be interrupted by regular jobs if other users submit them):   srun -p interactive --qos qos-besteffort --pty bash -i  Question : can you make this job be preempted? try to allocate other nodes in the interactive partition and see what happens when the besteffort job is marked for preemption.",
            "title": "Starting interactive jobs"
        },
        {
            "location": "/advanced/advanced_scheduling/#collecting-job-information",
            "text": "Now start a job with one of the previous commands, and you will check its details (runtime metrics, status, after execution statistics, etc.).   Show the details of a job:   scontrol show job $jobid  Question : what happens if you try to take a look at a job which is not in the queue (waiting/running) anymore (e.g.  scontrol show job 2 )?   Check waiting job priority (detailed view):   sprio -l   Check expected job start time:   squeue --start -u $USER   Show running job (and steps) system-level utilization (memory, I/O, energy):  note that  sstat  information is limited to your own jobs     sstat -j $jobid   Show specific statistics from a running job (and steps) or multiple jobs:   sstat -j $jobid --format=AveCPU,AveRSS,AveVMSize,MaxRSS,MaxVMSize\nsstat -j $jobid1,$jobid2 --format=AveCPU,AveRSS,AveVMSize,MaxRSS,MaxVMSize   Output the statistics in a parseable format, delimited by  |  (with, then without trailing  | ):   sstat -p -j $jobid --format=AveCPU,AveRSS,AveVMSize,MaxRSS,MaxVMSize\nsstat -P -j $jobid --format=AveCPU,AveRSS,AveVMSize,MaxRSS,MaxVMSize   Show running or completed job (and steps) system-level utilization from the accounting information, and with full details:   sacct -j $jobid\nsacct -j $jobid -l  Question : remember that job id #2? can we see its information with  sacct -j 2 --format=user  and  sacct -j 2 -l  ?   Show statistics relevant to the job allocation itself not taking steps into consideration, and with more details:   sacct -X -j $jobid\nsacct -X -j $jobid -l   Show a subset of interesting statistics from a completed job and its steps, including:  elapsed time in both human readable and total # of seconds  maximum resident set size of all tasks in job (you may want to add also  maxrssnode  and  maxrsstask  for a better understanding of which process consumed memory)  maximum virtual memory size (idem for  maxvmsizenode  and  maxvmsizetask )  consumed energy (in Joules), be aware there are many caveats!  your job needs to be the only one running on the corresponding compute nodes  the  RAPL mechanism  will not take into account all possible hardware elements which consume power (CPUs, GPUs and DRAM are included)       sacct -j $jobid --format=account,user,jobid,jobname,partition,state,elapsed,elapsedraw,start,end,maxrss,maxvmsize,consumedenergy,consumedenergyraw,nnodes,ncpus,nodelist   Output the same statistics in the parseable  | -delimited format, for a single and multiple jobs:   sacct -p -j $jobid --format=account,user,jobid,jobname,partition,state,elapsed,elapsedraw,start,end,maxrss,maxvmsize,consumedenergy,consumedenergyraw,nnodes,ncpus,nodelist\nsacct -p -j $jobid1,$jobid2 --format=account,user,jobid,jobname,partition,state,elapsed,elapsedraw,start,end,maxrss,maxvmsize,consumedenergy,consumedenergyraw,nnodes,ncpus,nodelist   Show statistics for all personal jobs started since a particular date, then without job steps:   sacct --starttime 2017-06-12 -u $USER\nsacct -X --starttime 2017-06-12 -u $USER",
            "title": "Collecting job information"
        },
        {
            "location": "/advanced/advanced_scheduling/#pausing-resuming-and-cancelling-jobs",
            "text": "To stop a waiting job from being scheduled and later to allow it to be scheduled:   scontrol hold $jobid\nscontrol release $jobid  Question : what do you see as \"Reason\" in  squeue  output on a large (say, 10 nodes interactive) job that you submitted and then ran  scontrol hold $jobid  on?   To pause a running job and then resume it:   scontrol suspend $jobid\nscontrol resume $jobid  Question : what happens when you try to suspend an interactive job?   To remove a job from the queue (stopping it if already started):   scancel $jobid  Question : what happens when you cancel a running job, does it get killed immediately?   To remove a job by name:   scancel --name=$jobname\nscancel -n $jobname  Question : what happens if you start two jobs with the same name and cancel by name?   To remove all user jobs:   scancel --user=$USER\nscancel -u $USER   To remove all waiting jobs ( pending  state) for a given user:   scancel --user=$USER --state=pending\nscancel -u $USER -t pending   To remove all waiting jobs in a given partition (e.g.  batch ):   scancel -u $USER --partition=batch\nscancel -u $USER -p batch   To stop and restart a given job:   scontrol requeue $jobid  Question : what message do you get when you try to requeue an interactive job?",
            "title": "Pausing, resuming and cancelling jobs"
        },
        {
            "location": "/advanced/advanced_scheduling/#part-two-the-harder-stuff",
            "text": "We have made available a set of template  batch launcher scripts for SLURM .  You should now:   adapt the most appropriate one (sequential, parallel, etc.) for your most commonly used HPC application  launch your own (short execution time) test case, on a single core for sequential code or two distributed cores for parallel code  take note of your fair-share and usage values (e.g. with  sshare -A $(sacctmgr -n show user $USER format=defaultaccount%30s) )    monitor job progression:  with  sprio  /  sprio -l  to see its priority in the queue  using  sstat  once it starts, to get running metrics  by connecting to the job ( --jobid  parameter of  srun ) and using  htop    finally, once the job finished or you have stopped it:  check the information visible with  sacct  how did your fair-share and usage values change?",
            "title": "Part two: the harder stuff"
        },
        {
            "location": "/advanced/Python/",
            "text": "-\n- mode: markdown;mode:visual-line;  fill-column: 80 -\n-\n\n\nAuthors: Cl\u00e9ment Parisot, Sarah Peter\n\n\nCopyright (c) 2017 UL HPC Team  -- see \nhttp://hpc.uni.lu\n\n\n\n\nUL HPC Tutorial: [Advanced] Prototyping with Python\n\n\n \n \n \n \n \n \n\n\n\n\nPython is a high-level interpreted language widely used in research. It lets you work quickly and comes with a lot of available packages which give more useful functionalities.\n\n\nIn this tutorial, we are going to explain the steps to run a Python script on the cluster and install a Python package as a user. We will also create a virtual environment and switch from one to the other. We will show how to use different versions of Python on a node. Finally, we will parallelize your code with Scoop and compile it in C to fasten its execution.\n\n\nExamples used in this tutorial\n\n\nExample 1\n\n\nThe first example used in this tutorial is fully inspired from \nPythonCExtensions\n. This code simply computes the mean value of an array of random numbers. The na\u00efve code used to compute the mean of an array is:\n\n\ndef mean(lst):\n    return sum(lst) / len(lst)\n\n\ndef standard_deviation(lst):\n    m = mean(lst)\n    variance = sum([(value - m) ** 2 for value in lst])\n    return math.sqrt(variance / len(lst))\n\n\n\n\nThe variable will be the size of the array on which we want to compute the mean. The idea is to reduce the time used to compute this value by using libraries (\nnumpy\n) or compile the code in C.\n\n\nExample 2\n\n\nThe second example used in this tutorial comes from \nScoop example computation of pi\n. We will use a \nMonte-Carlo method\n to compute the value of pi. As written in the Scoop documentation, it spawns two pseudo-random numbers that are fed to the hypot function which calculates the hypotenuse of its parameters. This step computes the Pythagorean equation (\\sqrt{x^2+y^2}) of the given parameters to find the distance from the origin (0,0) to the randomly placed point (which X and Y values were generated from the two pseudo-random values). Then, the result is compared to one to evaluate if this point is inside or outside the unit disk. If it is inside (have a distance from the origin lesser than one), a value of one is produced (red dots in the figure), otherwise the value is zero (blue dots in the figure). The experiment is repeated \ntries\n number of times with new random values.\n\n\n\n\nThe variable here will be the number of workers (cores on which the script runs) compared to the time of execution.\n\n\nPython usage\n\n\nIn this part we will simply run our Python script on the UL HPC platform, on a single node.\n\n\nGet all the scripts\n\n\nClone the UL HPC python tutorial under your home directory on the \nIris\n cluster. If you have cloned it before, simply run \ngit pull\n to update it to the latest version.\n\n\n(laptop)$> ssh iris-cluster\n(access)$> git clone https://github.com/ULHPC/tutorials.git\n(access)$> cd tutorials/\n(access)$> git stash && git pull -r && git stash pop\n\n\n\n\nAll the scripts used in this tutorial can be found under \ntutorials/advanced/Python/\n.\n\n\nExecute your first python script on the cluster (Example 1)\n\n\nFirst, connect to \niris-cluster\n and go to example 1:\n\n\n(laptop)$> ssh iris-cluster\n(access)$> cd tutorials/advanced/Python/example1/\n\n\n\n\nTo run your script \ninteractively\n on the cluster, you should do:\n\n\n(access)>$ si\n(iris-001)$> python example1.py\n\n\n\n\nYou should see the output of your script directly written in your terminal. It prints the length of the array and the number of seconds it took to compute the standard deviation 10,000 times.\n\n\nTo run your script in a \npassive\n way, you should create a batch script to run your python script.\n\n\n\n\nCreate a \nexample1.sh\n file under \ntutorials/advanced/Python/example1/\n.\n\n\nEdit it by using your favorite editor (\nvim\n, \nnano\n, \nemacs\n...)\n\n\nAdd a shebang at the beginning (\n#!/bin/bash\n)\n\n\nAdd \n#SBATCH\n parameters (see \nSlurm documentation\n)\n\n\n1\n core\n\n\nexample1\n name\n\n\nmaximum \n10m\n walltime\n\n\nlogfile under \nexample1.out\n\n\n\n\nNow run the script using\n\n\n(access)$> sbatch example1.sh\n\n\n\n\nNow, check that the content of \nexample1.out\n corresponds to the expected output (in interactive mode).\n\n\nHINT:\n You can find the answer under \ntutorials/advanced/Python/example1/example1.sh.answer\n.\n\n\nCompare version of Python\n\n\nYou can switch between several version of Python that are already install on UL HPC iris cluster. To list the versions available, you should use this command on a compute node:\n\n\n(iris-001)$> module spider Python\n\n\n\n\nQUESTIONS:\n\n\n\n\nWhat are the versions of Python available on Iris cluster ? On Gaia cluster ?\n\n\nWhich toolchains have been used to build them ?\n\n\n\n\nHere we will compare the performance of Python 2.7 and Python 3.\n\n\nHere are the steps to compare 2 codes:\n\n\n\n\nGo to \ntutorials/advanced/Python/example2\n\n\nCreate a batch script named \nexample2.sh\n\n\nEdit it with your favorite editor (\nvim\n, \nnano\n, \nemacs\n...)\n\n\nAdd a shebang at the beginning (\n#!/bin/bash\n)\n\n\nAdd \n#SBATCH\n parameters\n\n\n1\n core\n\n\nexample2\n name\n\n\nmaximum \n10m\n walltime\n\n\nlogfile under \nexample2.out\n\n\nLoad Python version 2.7\n\n\nExecute the script a first time with this version of Python\n\n\nLoad Python version 3\n\n\nExecute the script a second time with this Python version.\n\n\nCheck the content of the file \nexample2.out\n and identify the 2 executions and the module load.\n\n\n\n\nQUESTIONS\n\n\n\n\nWhat is the fastest version of Python ?\n\n\nThere are both \nfoss\n and \nintel\n compiled versions of Python available on the Iris cluster. Modify your script to compare their execution time. Which is the fastest one ?\n\n\n\n\nHINT\n\n\n\n\nYou can use \nmodule load\n command to load a specific version of Python.\n\n\nAn example of a BATCH script can be found under \ntutorials/advanced/Python/example2/example2.sh.answer\n\n\n\n\nUse a library to optimize your code\n\n\nIn this part we will try to use \nNumpy\n, a Python library, to optimize our code.\n\n\nIn \ntutorials/advanced/Python/example3/example3.py\n you should see a version of the previous script using Numpy.\n\n\nTry to execute the script on iris cluster in \ninteractive\n mode.\n\n\n(access)$> si\n(iris-001)$> python example3.py\n\n\n\n\nQUESTIONS\n\n\n\n\nWhy did the execution fail ? What is the problem ?\n\n\n\n\nWe need to install the numpy library. Hopefully, numpy is available as a module on iris cluster. Use the commands from \nexample2\n to:\n\n\n\n\nLoad \nnumpy\n module\n\n\nExecute your script on the node\n\n\n\n\n(access)$> si\n(iris-001)$> module spider numpy\n(iris-001)$> module load math/numpy\n(iris-001)$> python example3.py\n\n\n\n\nAs you can see, there is only one version of numpy available with module. If we want to use a more recent or older version of numpy, we can install it ourselves in our home directory. For that we will use the \npip\n tool.\n\n\npip\n is a package manager for Python. With this tool you can manage Python packages easily: install, uninstall, list, search packages or upgrade them. If you specify the \n--user\n parameter, the package will be installed under \nyour home directory\n and will be available on all the compute nodes. Let's install numpy using \npip\n.\n\n\n(access)$> si\n(iris-001)$> pip install --user numpy==1.13.0\n(iris-001)$> python -c \"import numpy as np; print np.__version__\"\n\n\n\n\nQUESTIONS\n\n\n\n\nWhich execution is faster between \nnumpy\n code (example3.py) and \nna\u00efve\n code (example1.py) ?\n\n\nWhy do you think that numpy is not as powerful as intended ? Which parameter can we change to compare the performances ?\n\n\n\n\nCreate virtual environment to switch between several versions of a package\n\n\nHere comes a very specific case. Sometimes you have to use tools which depend on a specific version of a package. You probably don't want to uninstall and reinstall the package with \npip\n each time you want to use one tool or the other.\n\n\nVirtualenv allows you to create several environments which will contain their own list of Python packages. The basic usage is to \ncreate one virtual environment per project\n.\n\n\nIn this tutorial we will create a new virtual environment for the previous code in order to install a different version of numpy and check the performances of our code with it.\n\n\nFirst of all, install \nvirtualenv\n package using pip:\n\n\n(access)$> si\n(iris-001)$> pip install --user virtualenv\n\n\n\n\nNow you can create your environment for this project. Name it \nnumpy12\n.\n\n\n(iris-001)$> cd ~/tutorials/advanced/Python/example3/\n(iris-001)$> virtualenv numpy12\n\n\n\n\nSo now you should be able to active this environment with this \nsource\n command. Please notice the \n(numpy12)\n present in your prompt that indicates that the \nnumpy12\n environment is active.\n\n\n(iris-001)$> source numpy12/bin/activate\n(numpy12)(iris-001)$>\n\n\n\n\nQUESTIONS\n\n\n\n\nUsing \npip freeze\n, what are the modules available before the activation of your virtual environment ?\n\n\nWhat are the module available after ?\n\n\nWhat version of python is used inside the virtual environment ? Where is it located ? (You can use \nwhich\n command.)\n\n\n\n\nTo exit a virtual environment run the \ndeactivate\n command.\n\n\nSo now, we can install a different numpy version inside our virtual environment. Check that the version installed corresponds to numpy 1.12.\n\n\n(iris-001)$> source numpy12/bin/activate\n(numpy12)(iris-001)$> pip install numpy==1.12\n(numpy12)(iris-001)$> python -c \"import numpy as np; print np.__version__\"\n(numpy12)(iris-001)$> deactivate\n(iris-001) python -c \"import numpy as np; print np.__version__\"\n\n\n\n\nNow you can adapt your script to load the right virtualenv and compare the performance of different versions of numpy.\n\n\nQUESTIONS\n\n\n\n\nCheck the size of numpy12 folder. Why is it so big ? What does it contain ?\n\n\n\n\nCompile your code in C language\n\n\nC language is known to be very powerful and to execute faster. It has to be compiled (typically using GCC compiler) to be executed. There exist many tools that can convert your Python code to C code to benefit from its performances (\nCython\n, \nPythran\n, ...).\n\n\nThe goal of this part is to adapt our na\u00efve code and use the \nPythran\n tool to convert it to C code. This code will then be imported as a standard Python module and executed.\n\n\nThe code can be found under \ntutorials/advanced/Python/example4/example4.py\n.\n\n\n\n\nOpen the \nexample4.py\n file\n\n\nReferring to \nPythran documentation\n, add a comment before the \nstandard_deviation\n function to help pythran to convert your python function into a C one.\n\n\nParameter should be a list of float\n\n\nFunction name should be \nstandard_dev\n\n\n\n\n#code to insert in example4.py\n\n#pythran export standard_dev(float list)\ndef standard_dev(lst):\n\n\n\n\n\n\nBe sure to have \npythran\n installed! If not, use \npip install --user pythran\n command (within a job) to install it in your home directory.\n\n\nCompile your code using pythran:\n\n\n\n\n(iris-001)$> pythran example4.py -o std.so # NEVER COMPILE ON ACCESS\n(iris-001)$> python -c \"import std\" # this imports the newly generated module with C implementation\n\n\n\n\n\n\nHave a look at \nc_compare.py\n that contains the code to \n\n\nimport your module\n\n\nand execute the mean function from this module on a random array\n\n\nExecute your code on a node and compare the execution time to the other one.\n\n\n\n\nQUESTIONS\n\n\n\n\nWhat is the fastest execution ? Why ?\n\n\nWhere can I find the code that has been generated from my Python script ?\n\n\n\n\nHINT:\n If you run \npythran example4.py -e -o std.c\n it will generate the C code. Have a look at the \n*.c\n files in your directory.\n\n\nUse Scoop to parallelize execution of your Python code with Slurm\n\n\nIn this part, we will use Scoop library to parallelize our Python code and execute it on iris cluster. This example uses the Monte-Carlo algorithm to compute the value of pi. Please have a look at the top of this page to check how it works.\n\n\nWe will first have to install the scoop library using \npip\n:\n\n\n(access)$> si\n(iris-001)$> pip install --user filelock\n(iris-001)$> pip install --user https://github.com/soravux/scoop/archive/master.zip\n\n\n\n\nScoop comes with direct Slurm bindings. If you run your code on a single node, it will try to use the most cores that it can. If you have reserved several nodes, it will use all the nodes of your reservation and distribute work on it.\n\n\nYou can specify the number of cores to use with the \n-n\n option in scoop.\n\n\nWe will write a batch script to execute our python script. We want to compare time of execution to the number of workers used in scoop. We want to go from 1 worker (single core) to 50 workers, increasing the worker number 1 by 1. As you can see, our script takes 1 parameter \nx\n in input which corresponds to the number of workers.\n\n\nThe batch script should contain:\n\n\n\n\n50 tasks reserved\n\n\nmaximum execution time of 35m\n\n\nname of the job should be \nscoop\n\n\na job array which goes from 1 to 50\n\n\na minimum number of 2 nodes reserved\n\n\na call to \npython -m scoop [...]\n to call the script with increasing number of cores reserved (\n$SLURM_ARRAY_TASK_ID\n)\n\n\na command to disable concurrent run of this job\n\n\noutput file should go to \nscoop.log\n\n\n\n\nHINT\n Have a look at \ntutorials/advanced/Python/example5/scoop.sh\n for the batch script example\n\n\nRun this script with \nsbatch\n command. Check the content of \nscoop.log\n using \ntail scoop.log\n to see if everything is going well.\n\n\nWhen your job is over, you can use \nmake graph\n command to generate the graph.\n\n\nQUESTIONS\n\n\n\n\nWhat is the correlation between number of workers and execution time ?\n\n\nUse what you learned in the previous part to optimize your code!",
            "title": "Prototyping with Python"
        },
        {
            "location": "/advanced/Python/#ul-hpc-tutorial-advanced-prototyping-with-python",
            "text": "Python is a high-level interpreted language widely used in research. It lets you work quickly and comes with a lot of available packages which give more useful functionalities.  In this tutorial, we are going to explain the steps to run a Python script on the cluster and install a Python package as a user. We will also create a virtual environment and switch from one to the other. We will show how to use different versions of Python on a node. Finally, we will parallelize your code with Scoop and compile it in C to fasten its execution.",
            "title": "UL HPC Tutorial: [Advanced] Prototyping with Python"
        },
        {
            "location": "/advanced/Python/#examples-used-in-this-tutorial",
            "text": "",
            "title": "Examples used in this tutorial"
        },
        {
            "location": "/advanced/Python/#example-1",
            "text": "The first example used in this tutorial is fully inspired from  PythonCExtensions . This code simply computes the mean value of an array of random numbers. The na\u00efve code used to compute the mean of an array is:  def mean(lst):\n    return sum(lst) / len(lst)\n\n\ndef standard_deviation(lst):\n    m = mean(lst)\n    variance = sum([(value - m) ** 2 for value in lst])\n    return math.sqrt(variance / len(lst))  The variable will be the size of the array on which we want to compute the mean. The idea is to reduce the time used to compute this value by using libraries ( numpy ) or compile the code in C.",
            "title": "Example 1"
        },
        {
            "location": "/advanced/Python/#example-2",
            "text": "The second example used in this tutorial comes from  Scoop example computation of pi . We will use a  Monte-Carlo method  to compute the value of pi. As written in the Scoop documentation, it spawns two pseudo-random numbers that are fed to the hypot function which calculates the hypotenuse of its parameters. This step computes the Pythagorean equation (\\sqrt{x^2+y^2}) of the given parameters to find the distance from the origin (0,0) to the randomly placed point (which X and Y values were generated from the two pseudo-random values). Then, the result is compared to one to evaluate if this point is inside or outside the unit disk. If it is inside (have a distance from the origin lesser than one), a value of one is produced (red dots in the figure), otherwise the value is zero (blue dots in the figure). The experiment is repeated  tries  number of times with new random values.   The variable here will be the number of workers (cores on which the script runs) compared to the time of execution.",
            "title": "Example 2"
        },
        {
            "location": "/advanced/Python/#python-usage",
            "text": "In this part we will simply run our Python script on the UL HPC platform, on a single node.",
            "title": "Python usage"
        },
        {
            "location": "/advanced/Python/#get-all-the-scripts",
            "text": "Clone the UL HPC python tutorial under your home directory on the  Iris  cluster. If you have cloned it before, simply run  git pull  to update it to the latest version.  (laptop)$> ssh iris-cluster\n(access)$> git clone https://github.com/ULHPC/tutorials.git\n(access)$> cd tutorials/\n(access)$> git stash && git pull -r && git stash pop  All the scripts used in this tutorial can be found under  tutorials/advanced/Python/ .",
            "title": "Get all the scripts"
        },
        {
            "location": "/advanced/Python/#execute-your-first-python-script-on-the-cluster-example-1",
            "text": "First, connect to  iris-cluster  and go to example 1:  (laptop)$> ssh iris-cluster\n(access)$> cd tutorials/advanced/Python/example1/  To run your script  interactively  on the cluster, you should do:  (access)>$ si\n(iris-001)$> python example1.py  You should see the output of your script directly written in your terminal. It prints the length of the array and the number of seconds it took to compute the standard deviation 10,000 times.  To run your script in a  passive  way, you should create a batch script to run your python script.   Create a  example1.sh  file under  tutorials/advanced/Python/example1/ .  Edit it by using your favorite editor ( vim ,  nano ,  emacs ...)  Add a shebang at the beginning ( #!/bin/bash )  Add  #SBATCH  parameters (see  Slurm documentation )  1  core  example1  name  maximum  10m  walltime  logfile under  example1.out   Now run the script using  (access)$> sbatch example1.sh  Now, check that the content of  example1.out  corresponds to the expected output (in interactive mode).  HINT:  You can find the answer under  tutorials/advanced/Python/example1/example1.sh.answer .",
            "title": "Execute your first python script on the cluster (Example 1)"
        },
        {
            "location": "/advanced/Python/#compare-version-of-python",
            "text": "You can switch between several version of Python that are already install on UL HPC iris cluster. To list the versions available, you should use this command on a compute node:  (iris-001)$> module spider Python  QUESTIONS:   What are the versions of Python available on Iris cluster ? On Gaia cluster ?  Which toolchains have been used to build them ?   Here we will compare the performance of Python 2.7 and Python 3.  Here are the steps to compare 2 codes:   Go to  tutorials/advanced/Python/example2  Create a batch script named  example2.sh  Edit it with your favorite editor ( vim ,  nano ,  emacs ...)  Add a shebang at the beginning ( #!/bin/bash )  Add  #SBATCH  parameters  1  core  example2  name  maximum  10m  walltime  logfile under  example2.out  Load Python version 2.7  Execute the script a first time with this version of Python  Load Python version 3  Execute the script a second time with this Python version.  Check the content of the file  example2.out  and identify the 2 executions and the module load.   QUESTIONS   What is the fastest version of Python ?  There are both  foss  and  intel  compiled versions of Python available on the Iris cluster. Modify your script to compare their execution time. Which is the fastest one ?   HINT   You can use  module load  command to load a specific version of Python.  An example of a BATCH script can be found under  tutorials/advanced/Python/example2/example2.sh.answer",
            "title": "Compare version of Python"
        },
        {
            "location": "/advanced/Python/#use-a-library-to-optimize-your-code",
            "text": "In this part we will try to use  Numpy , a Python library, to optimize our code.  In  tutorials/advanced/Python/example3/example3.py  you should see a version of the previous script using Numpy.  Try to execute the script on iris cluster in  interactive  mode.  (access)$> si\n(iris-001)$> python example3.py  QUESTIONS   Why did the execution fail ? What is the problem ?   We need to install the numpy library. Hopefully, numpy is available as a module on iris cluster. Use the commands from  example2  to:   Load  numpy  module  Execute your script on the node   (access)$> si\n(iris-001)$> module spider numpy\n(iris-001)$> module load math/numpy\n(iris-001)$> python example3.py  As you can see, there is only one version of numpy available with module. If we want to use a more recent or older version of numpy, we can install it ourselves in our home directory. For that we will use the  pip  tool.  pip  is a package manager for Python. With this tool you can manage Python packages easily: install, uninstall, list, search packages or upgrade them. If you specify the  --user  parameter, the package will be installed under  your home directory  and will be available on all the compute nodes. Let's install numpy using  pip .  (access)$> si\n(iris-001)$> pip install --user numpy==1.13.0\n(iris-001)$> python -c \"import numpy as np; print np.__version__\"  QUESTIONS   Which execution is faster between  numpy  code (example3.py) and  na\u00efve  code (example1.py) ?  Why do you think that numpy is not as powerful as intended ? Which parameter can we change to compare the performances ?",
            "title": "Use a library to optimize your code"
        },
        {
            "location": "/advanced/Python/#create-virtual-environment-to-switch-between-several-versions-of-a-package",
            "text": "Here comes a very specific case. Sometimes you have to use tools which depend on a specific version of a package. You probably don't want to uninstall and reinstall the package with  pip  each time you want to use one tool or the other.  Virtualenv allows you to create several environments which will contain their own list of Python packages. The basic usage is to  create one virtual environment per project .  In this tutorial we will create a new virtual environment for the previous code in order to install a different version of numpy and check the performances of our code with it.  First of all, install  virtualenv  package using pip:  (access)$> si\n(iris-001)$> pip install --user virtualenv  Now you can create your environment for this project. Name it  numpy12 .  (iris-001)$> cd ~/tutorials/advanced/Python/example3/\n(iris-001)$> virtualenv numpy12  So now you should be able to active this environment with this  source  command. Please notice the  (numpy12)  present in your prompt that indicates that the  numpy12  environment is active.  (iris-001)$> source numpy12/bin/activate\n(numpy12)(iris-001)$>  QUESTIONS   Using  pip freeze , what are the modules available before the activation of your virtual environment ?  What are the module available after ?  What version of python is used inside the virtual environment ? Where is it located ? (You can use  which  command.)   To exit a virtual environment run the  deactivate  command.  So now, we can install a different numpy version inside our virtual environment. Check that the version installed corresponds to numpy 1.12.  (iris-001)$> source numpy12/bin/activate\n(numpy12)(iris-001)$> pip install numpy==1.12\n(numpy12)(iris-001)$> python -c \"import numpy as np; print np.__version__\"\n(numpy12)(iris-001)$> deactivate\n(iris-001) python -c \"import numpy as np; print np.__version__\"  Now you can adapt your script to load the right virtualenv and compare the performance of different versions of numpy.  QUESTIONS   Check the size of numpy12 folder. Why is it so big ? What does it contain ?",
            "title": "Create virtual environment to switch between several versions of a package"
        },
        {
            "location": "/advanced/Python/#compile-your-code-in-c-language",
            "text": "C language is known to be very powerful and to execute faster. It has to be compiled (typically using GCC compiler) to be executed. There exist many tools that can convert your Python code to C code to benefit from its performances ( Cython ,  Pythran , ...).  The goal of this part is to adapt our na\u00efve code and use the  Pythran  tool to convert it to C code. This code will then be imported as a standard Python module and executed.  The code can be found under  tutorials/advanced/Python/example4/example4.py .   Open the  example4.py  file  Referring to  Pythran documentation , add a comment before the  standard_deviation  function to help pythran to convert your python function into a C one.  Parameter should be a list of float  Function name should be  standard_dev   #code to insert in example4.py\n\n#pythran export standard_dev(float list)\ndef standard_dev(lst):   Be sure to have  pythran  installed! If not, use  pip install --user pythran  command (within a job) to install it in your home directory.  Compile your code using pythran:   (iris-001)$> pythran example4.py -o std.so # NEVER COMPILE ON ACCESS\n(iris-001)$> python -c \"import std\" # this imports the newly generated module with C implementation   Have a look at  c_compare.py  that contains the code to   import your module  and execute the mean function from this module on a random array  Execute your code on a node and compare the execution time to the other one.   QUESTIONS   What is the fastest execution ? Why ?  Where can I find the code that has been generated from my Python script ?   HINT:  If you run  pythran example4.py -e -o std.c  it will generate the C code. Have a look at the  *.c  files in your directory.",
            "title": "Compile your code in C language"
        },
        {
            "location": "/advanced/Python/#use-scoop-to-parallelize-execution-of-your-python-code-with-slurm",
            "text": "In this part, we will use Scoop library to parallelize our Python code and execute it on iris cluster. This example uses the Monte-Carlo algorithm to compute the value of pi. Please have a look at the top of this page to check how it works.  We will first have to install the scoop library using  pip :  (access)$> si\n(iris-001)$> pip install --user filelock\n(iris-001)$> pip install --user https://github.com/soravux/scoop/archive/master.zip  Scoop comes with direct Slurm bindings. If you run your code on a single node, it will try to use the most cores that it can. If you have reserved several nodes, it will use all the nodes of your reservation and distribute work on it.  You can specify the number of cores to use with the  -n  option in scoop.  We will write a batch script to execute our python script. We want to compare time of execution to the number of workers used in scoop. We want to go from 1 worker (single core) to 50 workers, increasing the worker number 1 by 1. As you can see, our script takes 1 parameter  x  in input which corresponds to the number of workers.  The batch script should contain:   50 tasks reserved  maximum execution time of 35m  name of the job should be  scoop  a job array which goes from 1 to 50  a minimum number of 2 nodes reserved  a call to  python -m scoop [...]  to call the script with increasing number of cores reserved ( $SLURM_ARRAY_TASK_ID )  a command to disable concurrent run of this job  output file should go to  scoop.log   HINT  Have a look at  tutorials/advanced/Python/example5/scoop.sh  for the batch script example  Run this script with  sbatch  command. Check the content of  scoop.log  using  tail scoop.log  to see if everything is going well.  When your job is over, you can use  make graph  command to generate the graph.  QUESTIONS   What is the correlation between number of workers and execution time ?  Use what you learned in the previous part to optimize your code!",
            "title": "Use Scoop to parallelize execution of your Python code with Slurm"
        },
        {
            "location": "/advanced/ReproducibleResearch/",
            "text": "-\n- mode: markdown; mode: visual-line; -\n-\n\n\nTutorial \"Reproducible Research at the Cloud Era\"\n\n\n \n \n \n \n \n\n\nThis tutorial is actually not part of the UL HPC Tutorial but it was prepared by S. Varrette\nand given during the \nIEEE CloudCom 2016\n conference.\n\n\n\n\nFor more information: \nhttp://rr-tutorials.readthedocs.io/",
            "title": "Reproducible Research"
        },
        {
            "location": "/advanced/ReproducibleResearch/#tutorial-reproducible-research-at-the-cloud-era",
            "text": "This tutorial is actually not part of the UL HPC Tutorial but it was prepared by S. Varrette\nand given during the  IEEE CloudCom 2016  conference.   For more information:  http://rr-tutorials.readthedocs.io/",
            "title": "Tutorial \"Reproducible Research at the Cloud Era\""
        },
        {
            "location": "/rtfd/",
            "text": "The documentation for these tutorials is handled by \nRead the Docs\n, a web service dedicated to documentation management for the open source community.\n\n\n\n\nReference documentation\n\n\n\n\nBy default, the \nULHPC/tutorials\n repository is bound to the \nulhpc-tutorials\n project on Read the Docs. \n\n\nYou might wish to generate locally the docs:\n\n\n\n\nInstall \nmkdocs\n\n\nPreview your documentation from the project root by running \nmkdocs serve\n and visite with your favorite browser the URL \nhttp://localhost:8000\n\n\nbuild the full documentation locally by running \nmkdocs build\n to create the \nsite/\n directory.",
            "title": "Documentation (RTFD)"
        },
        {
            "location": "/contributing/",
            "text": "Proposing a new tutorial / Contributing to this repository\n\n\nYou're using a specific software on the UL HPC platform not listed in the above list? Then most probably you\n\n\n\n\ndeveloped a set of script to effectively run that software\n\n\nused to face issues such that you're aware (eventually unconsciously) of tricks and tips for that specific usage.\n\n\n\n\nThen your inputs are valuable for the other users and we would appreciate your help to complete this repository with new topics/entries.\n\n\nTo do that, the general approach is similar to the one proposed by \nGithub via the Forking procedure\n.\nSince we use \ngit-flow\n, your workflow for contributing to this repository should typically involve the following steps:\n\n\n\n\nFork it\n\n\nInitialize your local copy of the repository (including git submodules etc.): \nmake setup\n\n\nCreate your feature branch: \ngit flow feature start <feature_name>\n\n\nCommit your changes: \ngit commit -am 'Added some feature'\n\n\nPublish your feature branch: \ngit flow feature publish <feature_name>\n\n\nCreate new \nPull Request\n\n\n\n\nMore details are provided below.\n\n\ngit-flow\n\n\nThe Git branching model for this repository follows the guidelines of \ngitflow\n.\nIn particular, the central repo (on \ngithub.com\n) holds two main branches with an infinite lifetime:\n\n\n\n\nproduction\n: the \nproduction-ready\n tutorials\n\n\ndevel\n: the main branch where the latest developments interviene. This is the \ndefault\n branch you get when you clone the repo.\n\n\n\n\nNew tutorial layout\n\n\nSo assuming you have \nforked this repository\n to work freely on your own copy of it, you can now feed a new tutorial, assuming you follow the below guidelines.\n\n\nDirectory Layout\n\n\n{advanced | basic}/<name>  # Select the appropriate root directory\n\u251c\u2500\u2500 README.md              # Main tutorial file, in Markdown\n\u251c\u2500\u2500 index.md -> README.md  # Symlink (for mkdocs)\n\u251c\u2500\u2500 slides.pdf             # Slides proposing an overview of the tutorial\n\u251c\u2500\u2500 cover_slides.png       # Picture of the cover of the slide\n\u251c\u2500\u2500 Makefile               # GNU Makefile offering the targets 'fetch', 'compile', 'run' and 'run_interactive'\n\u251c\u2500\u2500 plots                  # Directory hosting the Gnuplots / R plots data\n\u251c\u2500\u2500 runs/                  # Directory hosting the logs of the runs\n\u251c\u2500\u2500 scripts/               # Eventually, a directory hosting some specific scripts\n\u2514\u2500\u2500 launcher-<name>.{slurm|oar}.sh # launcher script to be used in the tutorial\n\n# Prepare the appropriate link for ReadtheDocs\ndocs/{advanced | basic}/<name>/ -> ../../../{advanced | basic}/<name>\n\n\n\n\nYou shall stick to a single \nREADME.md\n file, (using the \nmarkdown\n format) if possible.\nRemember that they shall be understandable for users having no or very few knowledge on your topic!\n\n\nOne \nproposal\n to organize the workflow of your tutorial:\n\n\n\n\nSelect a typical sample example that will be used throughout all the tutorial, that is easy to fetch from the official page of the software. Adapt the \nmake fetch\n directive in your root \nMakefile\n to perform the corresponding actions.\n\n\n(eventually) detail how to build the sources (using \nEasyBuild\n. Adapt the \nmake build\n accordingly.\n\n\ndedicate a section to the running of this example in an \ninteractive\n job such that the reader has a better understanding of:\n\n\nthe involved modules to load\n\n\nthe classical way to execute the software\n\n\netc.\n   Adapt also the \nmake run_interactive\n accordingly\n\n\ndedicate a second section to the running of the example in a \npassive\n job, typically providing a generic launcher script adapted to your software. You might adapt / extend the \nUL HPC launcher scripts\n the same way to extend these tutorials. Adapt also the \nmake run\n accordingly.\n\n\na last section would typically involves hints / elements to benchmark the execution, add tips/tricks to improve the performances (and see the effects of those improvements) and have a way to plot the results.  Adapt the \nmake plot\n accordingly\n\n\n\n\nSemantic Versionning\n\n\nThe operation consisting of releasing a new version of this repository is automated by a set of tasks within the \nMakefile\n at the root of this repository.\n\n\nIn this context, a version number have the following format:\n\n\n  <major>.<minor>.<patch>\n\n\n\nwhere:\n\n\n\n\n< major >\n corresponds to the major version number\n\n\n< minor >\n corresponds to the minor version number\n\n\n< patch >\n corresponds to the patching version number\n\n\n\n\nExample: \n1.2.0\n\n\nThe current version number is stored in the file \nVERSION\n. \nDO NOT EDIT THIS FILE\n, use the below primitives to affect the number it contains.\nFor more information on the version, run:\n\n\n $> make versioninfo\n\n\n\nIf a new  version number such be bumped, you simply have to run:\n\n\n $> make start_bump_{major,minor,patch}\n\n\n\nThis will start the release process for you using \ngit-flow\n.\nThen, to make the release effective, just run:\n\n\n $> make release\n\n\n\nThis will finalize the release using \ngit-flow\n, create the appropriate tag and merge all things the way they should be.",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#proposing-a-new-tutorial-contributing-to-this-repository",
            "text": "You're using a specific software on the UL HPC platform not listed in the above list? Then most probably you   developed a set of script to effectively run that software  used to face issues such that you're aware (eventually unconsciously) of tricks and tips for that specific usage.   Then your inputs are valuable for the other users and we would appreciate your help to complete this repository with new topics/entries.  To do that, the general approach is similar to the one proposed by  Github via the Forking procedure .\nSince we use  git-flow , your workflow for contributing to this repository should typically involve the following steps:   Fork it  Initialize your local copy of the repository (including git submodules etc.):  make setup  Create your feature branch:  git flow feature start <feature_name>  Commit your changes:  git commit -am 'Added some feature'  Publish your feature branch:  git flow feature publish <feature_name>  Create new  Pull Request   More details are provided below.",
            "title": "Proposing a new tutorial / Contributing to this repository"
        },
        {
            "location": "/contributing/#git-flow",
            "text": "The Git branching model for this repository follows the guidelines of  gitflow .\nIn particular, the central repo (on  github.com ) holds two main branches with an infinite lifetime:   production : the  production-ready  tutorials  devel : the main branch where the latest developments interviene. This is the  default  branch you get when you clone the repo.",
            "title": "git-flow"
        },
        {
            "location": "/contributing/#new-tutorial-layout",
            "text": "So assuming you have  forked this repository  to work freely on your own copy of it, you can now feed a new tutorial, assuming you follow the below guidelines.",
            "title": "New tutorial layout"
        },
        {
            "location": "/contributing/#directory-layout",
            "text": "{advanced | basic}/<name>  # Select the appropriate root directory\n\u251c\u2500\u2500 README.md              # Main tutorial file, in Markdown\n\u251c\u2500\u2500 index.md -> README.md  # Symlink (for mkdocs)\n\u251c\u2500\u2500 slides.pdf             # Slides proposing an overview of the tutorial\n\u251c\u2500\u2500 cover_slides.png       # Picture of the cover of the slide\n\u251c\u2500\u2500 Makefile               # GNU Makefile offering the targets 'fetch', 'compile', 'run' and 'run_interactive'\n\u251c\u2500\u2500 plots                  # Directory hosting the Gnuplots / R plots data\n\u251c\u2500\u2500 runs/                  # Directory hosting the logs of the runs\n\u251c\u2500\u2500 scripts/               # Eventually, a directory hosting some specific scripts\n\u2514\u2500\u2500 launcher-<name>.{slurm|oar}.sh # launcher script to be used in the tutorial\n\n# Prepare the appropriate link for ReadtheDocs\ndocs/{advanced | basic}/<name>/ -> ../../../{advanced | basic}/<name>  You shall stick to a single  README.md  file, (using the  markdown  format) if possible.\nRemember that they shall be understandable for users having no or very few knowledge on your topic!  One  proposal  to organize the workflow of your tutorial:   Select a typical sample example that will be used throughout all the tutorial, that is easy to fetch from the official page of the software. Adapt the  make fetch  directive in your root  Makefile  to perform the corresponding actions.  (eventually) detail how to build the sources (using  EasyBuild . Adapt the  make build  accordingly.  dedicate a section to the running of this example in an  interactive  job such that the reader has a better understanding of:  the involved modules to load  the classical way to execute the software  etc.\n   Adapt also the  make run_interactive  accordingly  dedicate a second section to the running of the example in a  passive  job, typically providing a generic launcher script adapted to your software. You might adapt / extend the  UL HPC launcher scripts  the same way to extend these tutorials. Adapt also the  make run  accordingly.  a last section would typically involves hints / elements to benchmark the execution, add tips/tricks to improve the performances (and see the effects of those improvements) and have a way to plot the results.  Adapt the  make plot  accordingly",
            "title": "Directory Layout"
        },
        {
            "location": "/contributing/#semantic-versionning",
            "text": "The operation consisting of releasing a new version of this repository is automated by a set of tasks within the  Makefile  at the root of this repository.  In this context, a version number have the following format:    <major>.<minor>.<patch>  where:   < major >  corresponds to the major version number  < minor >  corresponds to the minor version number  < patch >  corresponds to the patching version number   Example:  1.2.0  The current version number is stored in the file  VERSION .  DO NOT EDIT THIS FILE , use the below primitives to affect the number it contains.\nFor more information on the version, run:   $> make versioninfo  If a new  version number such be bumped, you simply have to run:   $> make start_bump_{major,minor,patch}  This will start the release process for you using  git-flow .\nThen, to make the release effective, just run:   $> make release  This will finalize the release using  git-flow , create the appropriate tag and merge all things the way they should be.",
            "title": "Semantic Versionning"
        },
        {
            "location": "/contacts/",
            "text": "These tutorials has been implemented in the context of the \nUL HPC\n Platform of the \nUniversity of Luxembourg\n by the \nUL HPC Management Team\n.\n\n\nYet we are grateful to the following users of the platform who helped us to consolidate or complete these tutorials (in alphabetical order):\n\n\n\n\nXavier Besseron\n\n\nJoseph Emeras\n\n\nMaxime Schmitt\n\n\nP. Wohlschlegel\n\n\n\n\nYou can submit bug / issues / feature request using the \nULHPC/tutorials Tracker\n. \nAlternatively, you can contact the \nUL HPC Management Team\n responsible for these tutorials using the following email address: \nhpc-sysadmins@uni.lu",
            "title": "Contacts"
        }
    ]
}